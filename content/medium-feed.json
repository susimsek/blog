{
  "items": [
    {
      "creator": "Şuayb Şimşek",
      "title": "Spring Boot GraalVM Native Image",
      "link": "https://medium.com/yapi-kredi-teknoloji/spring-boot-graalvm-native-image-3703a3b9fd29?source=rss-bda589f2335a------2",
      "pubDate": "Tue, 22 Apr 2025 08:02:29 GMT",
      "content:encoded": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*07CXmp711t--05EF398hwQ.jpeg\" /></figure><h3>📖 Introduction</h3><p>Java applications often suffer from long startup times and high memory consumption, making modern software development more challenging. However, <strong>GraalVM Native Image</strong> offers a groundbreaking solution by significantly improving the performance of Spring Boot applications.</p><p>In this article, we will explore <a href=\"https://github.com/susimsek/spring-boot-graalvm-native-example\">my GitHub example project</a> and demonstrate how to develop <strong>high-performance and lightweight applications</strong> using <strong>Java 21, Spring Boot 3.4, and GraalVM Native Image</strong>. 🚀</p><h3>📌 What is GraalVM and Why Should You Use It?</h3><p>GraalVM is a <strong>high-performance JIT (Just-In-Time) and AOT (Ahead-of-Time) compiler</strong> that enables Java applications to be compiled into native code.</p><h3>Key Advantages of GraalVM:</h3><p>✅ <strong>Instant startup time</strong> — Runs natively without requiring a JVM!<br>✅ <strong>Lower memory consumption</strong> — Uses significantly less RAM compared to the traditional JVM.<br>✅ <strong>Smaller Docker containers</strong> — Ideal for cloud-native architectures.<br>✅ <strong>Enhanced security</strong> — Eliminates unnecessary runtime components, reducing the attack surface.</p><h3>📌 JVM vs. GraalVM Native Image Comparison</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*o3bhQC_W4wL4IADTLfWb6w.png\" /></figure><h3>⚙️ Getting Started with Spring Boot and GraalVM Native Image</h3><h3>1️⃣ Set Up Your Development Environment</h3><p>Before you begin, ensure you have the following installed:</p><p>🔹 <strong>Java 21</strong><br>🔹 <strong>GraalVM 22.3+</strong> (with Native Image support)<br>🔹 <strong>UPX</strong> (optional, for compressing the executable)</p><h3>2️⃣ Running the Project</h3><p>First, clone the repository from GitHub:</p><pre>git clone https://github.com/susimsek/spring-boot-graalvm-native-example.git<br> cd spring-boot-graalvm-native-example<br> ./mvnw spring-boot:run</pre><p>The application will be available at <a href=\"http://localhost:8080/\"><strong>http://localhost:8080</strong></a>.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*MbuoYAEuIaXN2aBb8a9oig.png\" /></figure><blockquote><strong><em>Test the API:</em></strong></blockquote><blockquote><em>curl -X GET </em><a href=\"http://localhost:8080/api/v1/hello\"><em>http://localhost:8080/api/v1/hello</em></a></blockquote><blockquote><strong><em>Response:</em></strong></blockquote><blockquote><em>{ &quot;message&quot;: &quot;Hello, GraalVM Native Image!&quot; }</em></blockquote><h3>🏗 Building Your Spring Boot Application as a GraalVM Native Image</h3><p>To compile your Spring Boot project into a native binary, follow these steps:</p><h3>Compile a Native Image</h3><pre>./mvnw native:compile -B -ntp -Pnative,prod -DskipTests</pre><p>This will generate a <strong>target/native-executable</strong> binary.</p><p><strong>Optionally, compress the executable using UPX:</strong></p><pre>upx --ultra-brute --lzma target/native-executable</pre><p>UPX (Ultimate Packer for eXecutables) is a powerful tool that compresses executables to save disk space and reduce deployment size. <strong>Compressed binaries</strong> function identically to their uncompressed counterparts but occupy less storage.</p><p><strong>Advantages of UPX:</strong><br>✅ <strong>Smaller file size</strong> — Reduces executable size by 50–80%.<br>✅ <strong>No performance loss</strong> — Compressed binaries execute as normal.<br>✅ <strong>Efficient execution</strong> — Maintains CPU performance while conserving disk space.</p><blockquote><strong><em>Example:</em></strong><em> If </em><em>target/native-executable is 140MB, using UPX can shrink it to </em><strong><em>30-35MB</em></strong><em>.</em></blockquote><h3>🐳 Deploying with Docker</h3><p>A GraalVM-generated <strong>native executable</strong> can be easily containerized using Docker.</p><h3>Build the Docker Image</h3><pre>docker build -t spring-boot-graalvm-samples .</pre><h3>Run the Docker Container</h3><pre>docker run -d -p 8080:8080 spring-boot-graalvm-samples</pre><h3>🚀 Deploying to Kubernetes</h3><p>You can deploy your Spring Boot GraalVM application to Kubernetes using a Helm Chart:</p><pre>helm install graalvm-native-app deploy/helm/graalvm-native-app</pre><p>To uninstall:</p><pre>helm uninstall graalvm-native-app</pre><h3>🛠 Conclusion</h3><p>By leveraging Spring Boot and GraalVM Native Image, you can develop <strong>fast-starting, lightweight, and cloud-native</strong> applications.</p><p>This approach provides <strong>reduced memory consumption, improved startup performance, and enhanced security</strong>. If you’re looking to build high-performance Spring Boot applications, try <a href=\"https://github.com/susimsek/spring-boot-graalvm-native-example\">cloning my GitHub repository</a> and get started today! 🚀</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=3703a3b9fd29\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/yapi-kredi-teknoloji/spring-boot-graalvm-native-image-3703a3b9fd29\">Spring Boot GraalVM Native Image</a> was originally published in <a href=\"https://medium.com/yapi-kredi-teknoloji\">Yapı Kredi Teknoloji</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
      "content:encodedSnippet": "📖 Introduction\nJava applications often suffer from long startup times and high memory consumption, making modern software development more challenging. However, GraalVM Native Image offers a groundbreaking solution by significantly improving the performance of Spring Boot applications.\nIn this article, we will explore my GitHub example project and demonstrate how to develop high-performance and lightweight applications using Java 21, Spring Boot 3.4, and GraalVM Native Image. 🚀\n📌 What is GraalVM and Why Should You Use It?\nGraalVM is a high-performance JIT (Just-In-Time) and AOT (Ahead-of-Time) compiler that enables Java applications to be compiled into native code.\nKey Advantages of GraalVM:\n✅ Instant startup time — Runs natively without requiring a JVM!\n✅ Lower memory consumption — Uses significantly less RAM compared to the traditional JVM.\n✅ Smaller Docker containers — Ideal for cloud-native architectures.\n✅ Enhanced security — Eliminates unnecessary runtime components, reducing the attack surface.\n📌 JVM vs. GraalVM Native Image Comparison\n\n⚙️ Getting Started with Spring Boot and GraalVM Native Image\n1️⃣ Set Up Your Development Environment\nBefore you begin, ensure you have the following installed:\n🔹 Java 21\n🔹 GraalVM 22.3+ (with Native Image support)\n🔹 UPX (optional, for compressing the executable)\n2️⃣ Running the Project\nFirst, clone the repository from GitHub:\ngit clone https://github.com/susimsek/spring-boot-graalvm-native-example.git\n cd spring-boot-graalvm-native-example\n ./mvnw spring-boot:run\nThe application will be available at http://localhost:8080.\n\nTest the API:\ncurl -X GET http://localhost:8080/api/v1/hello\nResponse:\n{ \"message\": \"Hello, GraalVM Native Image!\" }\n🏗 Building Your Spring Boot Application as a GraalVM Native Image\nTo compile your Spring Boot project into a native binary, follow these steps:\nCompile a Native Image\n./mvnw native:compile -B -ntp -Pnative,prod -DskipTests\nThis will generate a target/native-executable binary.\nOptionally, compress the executable using UPX:\nupx --ultra-brute --lzma target/native-executable\nUPX (Ultimate Packer for eXecutables) is a powerful tool that compresses executables to save disk space and reduce deployment size. Compressed binaries function identically to their uncompressed counterparts but occupy less storage.\nAdvantages of UPX:\n✅ Smaller file size — Reduces executable size by 50–80%.\n✅ No performance loss — Compressed binaries execute as normal.\n✅ Efficient execution — Maintains CPU performance while conserving disk space.\nExample: If target/native-executable is 140MB, using UPX can shrink it to 30-35MB.\n🐳 Deploying with Docker\nA GraalVM-generated native executable can be easily containerized using Docker.\nBuild the Docker Image\ndocker build -t spring-boot-graalvm-samples .\nRun the Docker Container\ndocker run -d -p 8080:8080 spring-boot-graalvm-samples\n🚀 Deploying to Kubernetes\nYou can deploy your Spring Boot GraalVM application to Kubernetes using a Helm Chart:\nhelm install graalvm-native-app deploy/helm/graalvm-native-app\nTo uninstall:\nhelm uninstall graalvm-native-app\n🛠 Conclusion\nBy leveraging Spring Boot and GraalVM Native Image, you can develop fast-starting, lightweight, and cloud-native applications.\nThis approach provides reduced memory consumption, improved startup performance, and enhanced security. If you’re looking to build high-performance Spring Boot applications, try cloning my GitHub repository and get started today! 🚀\n\nSpring Boot GraalVM Native Image was originally published in Yapı Kredi Teknoloji on Medium, where people are continuing the conversation by highlighting and responding to this story.",
      "dc:creator": "Şuayb Şimşek",
      "guid": "https://medium.com/p/3703a3b9fd29",
      "categories": ["upx", "kubernetes", "spring-boot", "native-image", "graalvm"],
      "isoDate": "2025-04-22T08:02:29.000Z"
    },
    {
      "creator": "Şuayb Şimşek",
      "title": "React & Spring Boot Graphql Fullstack Microservice Application on Kubernetes",
      "link": "https://suaybsimsek58.medium.com/react-spring-boot-graphql-fullstack-microservice-application-on-kubernetes-eb227e1a748b?source=rss-bda589f2335a------2",
      "pubDate": "Sun, 28 Aug 2022 16:29:12 GMT",
      "content:encoded": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*PGprMpt9l9D_eUGYW1tzSA.png\" /></figure><p>Hello everyone, In this article, we will develop a fullstack graphql microservice application using React and Spring Boot and deploy this application on Kubernetes.</p><h3>GraphQL</h3><p>GraphQL is a query language and server-side runtime for application programming interfaces (APIs) that prioritizes giving clients exactly the data they request and no more.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/240/1*kNO-rVTtHXdRaHZCTtpBLg.png\" /></figure><p>GraphQL is designed to make APIs fast, flexible, and developer-friendly. It can even be deployed within an integrated development environment (IDE) known as GraphiQL. As an alternative to REST, GraphQL lets developers construct requests that pull data from multiple data sources in a single API call.</p><h3>Apollo Federation</h3><p>Although Apollo Federation is a complex structure, we can briefly say that it is a technology that enables multiple graphql schemes to be gathered under a single gateway and accessed as a single scheme.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/216/1*uf7NRPwIF3HsxM8GikI_tQ.png\" /></figure><p>Apollo federation is an elegant approach to apply microservice pattern with GraphQL. Using federation you can easily split your schema into multiple subschemas and implement each subschema logic in it’s own service.</p><p>In this article we gonna implement a simple product-review graph, so we will have 4 services:</p><ul><li>product service</li><li>review service</li><li>apollo gateway</li><li>auth service</li></ul><h3>Auth service</h3><p>We are using oauth2 authorization server that implemented using spring security authorization server framework to secure the application.we will not focus on this service in this article. OAuth authorization server is responsible for authenticating the users and issuing access tokens containing the user data and proper access policies.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*kgBXec8dhZgKxdCBsBo7SQ.png\" /></figure><p>The OAuth2 Authorization Server can be accessed from this link via kubernetes.</p><p><a href=\"http://auth.susimsek.github.io\">http://auth.susimsek.github.io</a></p><p>The OAuth2 Authorization Server can be accessed from this link via heroku. <a href=\"https://graphql-fullstack-auth-service.herokuapp.com\">https://graphql-fullstack-auth-service.herokuapp.com</a></p><h3>Apollo Gateway</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*LD3elemQEyFm114w2me_Kg.png\" /></figure><p>Let’s define our Apollo Federation gateway.</p><pre>npm install @apollo/gateway apollo-server graphql --save</pre><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/cdd7e923df55696e20a3bc95c170ea3a/href\">https://medium.com/media/cdd7e923df55696e20a3bc95c170ea3a/href</a></iframe><p>We added cors configuration, Vault and Consul configuration to apollo gateway.And also added authorization header value to context to send authorization header to subgraph.We used manuel composition to compose a supergraph schema from a collection of subgraph schemas.manuel composition recommended for production environment by the apollo federation team.</p><h3>Product service</h3><p>First, we use spring graphql starter.Spring for GraphQL provides support for Spring applications built on <a href=\"https://www.graphql-java.com/\">GraphQL Java</a>.The project reached version 1.0 in May 2022.</p><pre>&lt;dependency&gt;<br>   &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;<br>   &lt;artifactId&gt;spring-boot-starter-graphql&lt;/artifactId&gt;<br>&lt;/dependency&gt;</pre><p>Then we will user spring webflux starter to provide reactive programming support.Reactive systems better utilize modern processors. Also, the inclusion of back-pressure in reactive programming ensures better resilience between decoupled components.</p><pre>&lt;dependency&gt;<br>   &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;<br>   &lt;artifactId&gt;spring-boot-starter-webflux&lt;/artifactId&gt;<br>&lt;/dependency&gt;</pre><p>Then let’s define our product schema</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/d06079dbc030bc0cd676de6b995a5750/href\">https://medium.com/media/d06079dbc030bc0cd676de6b995a5750/href</a></iframe><p>Take a note of that @key directive in <strong>Product </strong>type. It indicates that we defined a GraphQL <strong>entity</strong>.</p><p>Let’s create our controller to serve our <strong>product</strong> subgraph</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/02114e90722a69db30c19fb517345bd7/href\">https://medium.com/media/02114e90722a69db30c19fb517345bd7/href</a></iframe><h3>Review service</h3><p>Once we installed same dependencies as for product service we can define the schema.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/29c9d0eea06f44efc24b991b3204469f/href\">https://medium.com/media/29c9d0eea06f44efc24b991b3204469f/href</a></iframe><p>As you can see, for now review subgraph doesn’t have <strong>Query</strong> section at all — we defined the <strong>Review </strong>type and what the most interesting — extended the <strong>Product</strong> type with a new reviews property that returns an array of product’s reviews.</p><p>Let’s create our controller to serve our <strong>review</strong> subgraph</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/8b0a57920343158c3fdb41b35e463c94/href\">https://medium.com/media/8b0a57920343158c3fdb41b35e463c94/href</a></iframe><h3>Getting everything together</h3><p>Now, let’s launch and query. First start auth service and then our subgraphs. finally start the gateway. Apollo studio should be available on <a href=\"http://localhost:4000/\">http://localhost:4000/</a>.</p><p>Get access token from oauth2 server and assign it to Authorization header. Then, run the following query.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/e598349f411ef5ec672836987be175e9/href\">https://medium.com/media/e598349f411ef5ec672836987be175e9/href</a></iframe><p>The result should be</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/d9e37d845c9bb35bcd41e3a509263e56/href\">https://medium.com/media/d9e37d845c9bb35bcd41e3a509263e56/href</a></iframe><h3>Frontend</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/982/1*hKmu4RbuCltBRBYNqcMjVg.png\" /></figure><p>We use GraphQL Code Generator is a CLI tool that generates code out of our GraphQL schema.</p><p>Install the following npm packages required to run GraphQL Code Generator</p><pre>npm install @graphql-codegen/cli @graphql-codegen/introspection @graphql-codegen/typescript @graphql-codegen/typescript-operations @graphql-codegen/typescript-react-apollo --save-dev</pre><p>And also install the following npm packages generate types for TypeScript,Graphql and Apollo.</p><pre>npm install graphql @apollo/client typescript --save</pre><p>Then add the codegen.yml file to your project. This file is a config for <strong>GraphQL-Codegen</strong>.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/c8e71b0de06219bb1578d0ca91ba84a5/href\">https://medium.com/media/c8e71b0de06219bb1578d0ca91ba84a5/href</a></iframe><p>The next step is to add scripts to package.json</p><pre>&quot;scripts&quot;: {<br>  &quot;generate&quot;: &quot;graphql-codegen --config codegen.yml&quot;,<br>  &quot;sonar&quot;: &quot;node sonarqube-scanner.js&quot;<br>}</pre><p>You’ll run this script (npm run codegen) every time you&#39;ve changed anything in your GraphQL API or in your GraphQL files to get the most up-to-date types generated.</p><p>Let’s create the following GraphQL query to fetch product reviews</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/3649ddb37ccdcebfdcf66e0dccc516eb/href\">https://medium.com/media/3649ddb37ccdcebfdcf66e0dccc516eb/href</a></iframe><p>Run the codegen to generate types and hooks:</p><pre>npm run generate</pre><p>Use the generated hook in react.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/044ee8961f47a563fb4abf51823748e8/href\">https://medium.com/media/044ee8961f47a563fb4abf51823748e8/href</a></iframe><p>That’s it. Graphql codegen is amazing tool to generate reusable hooks in your GraphQL files.By using this approach we are able to;</p><ul><li>Avoid generating dynamic queries at runtime.</li><li>Use graphql query language rather than a language-specific syntax like tagged template literals</li><li>Improve on the DX for we auto-generate reusable hooks with type-safety and IDE IntelliSense</li><li>Validate queries against our schema</li><li>Rebuild our code when we change our queries</li></ul><h3>Heroku</h3><p>I just deployed oauth2 authorization server fullstack application to heroku.You can access the oauth2 authorization server via the links below.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/148/0*yZKl06FmC9ALP_0-.png\" /></figure><p>The Oauth2 Authorization server can be accessed from this link.</p><p><a href=\"https://graphql-fullstack-auth-service.herokuapp.com\">http://graphql-fullstack-auth-service.herokuapp.com</a></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/990/1*0gMilg2yMdrvLWv255adjA.png\" /></figure><p>You can log in to the authorization server using that credential information or login to the authorization server with gmail.</p><pre><em>username:</em> admin<br><em>password:</em> password</pre><h3>Deployment with Docker Compose</h3><p>Docker Compose is a tool for defining and running multi-container Docker applications.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/367/1*ZFW8FyGjTuNkwHQgAZ76iA.png\" /></figure><p>You can deploy the app by running the following bash command</p><pre><em>sudo</em> chmod +x deploy.sh<br><em>./deploy.sh</em> -d</pre><p>You can uninstall app the following bash command</p><pre><em>./deploy.sh</em> -d -r</pre><p>The Fullstack GraphQL App be accessed with nginx from the link below.</p><p><a href=\"http://127.0.0.1\">http://127.0.0.1</a></p><p>The OAuth2 Authorization Server be accessed from the link below. (default nginx ingress)</p><p><a href=\"http://127.0.0.1:9000\">http://127.0.0.1:9000</a></p><h3>Kubernetes Deployment with Helm</h3><p><a href=\"https://www.helm.sh/\">Helm</a> is a package manager for Kubernetes that allows developers and operators to more easily package, configure, and deploy applications and services onto Kubernetes clusters.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/367/0*NBbRuV1NjGNvgARG.png\" /></figure><p>You can deploy the app by running the following bash command</p><pre><em>sudo</em> chmod +x deploy.sh<br><em>./deploy.sh</em> -k</pre><p>You can uninstall app the following bash command</p><pre><em>./deploy.sh</em> -k -r</pre><p>You can upgrade the App (if you have made any changes to the generated manifests) by running the following bash command</p><pre><em>./deploy.sh</em> -u</pre><p>The Fullstack GraphQL App be accessed with ingress from the link below.(default nginx ingress)</p><p><a href=\"https://gqlmsweb.susimsek.github.io/\">http://gqlmsweb.susimsek.github.io/</a></p><p>The OAuth2 Authorization Server be accessed with ingress from the link below. (default nginx ingress)</p><p><a href=\"https://auth.susimsek.github.io/\">http://auth.susimsek.github.io/</a></p><h3>Summary</h3><p>In this article, we have focused on how to develop a fullstack microservice application graphql based on Kubernetes.</p><p>GraphQL is a very exciting new technology that can potentially revolutionize the way we develop Web APIs.With Apollo Federation, implementing federated GraphQL in the context of microservices becomes easier than ever.</p><p>Finally, the full implementation of this article can be found in <a href=\"https://github.com/susimsek/GraphqlMicroserviceFullstack\">the GitHub project</a>.</p><p>My article ends here. See you in the next articles.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=eb227e1a748b\" width=\"1\" height=\"1\" alt=\"\">",
      "content:encodedSnippet": "Hello everyone, In this article, we will develop a fullstack graphql microservice application using React and Spring Boot and deploy this application on Kubernetes.\nGraphQL\nGraphQL is a query language and server-side runtime for application programming interfaces (APIs) that prioritizes giving clients exactly the data they request and no more.\n\nGraphQL is designed to make APIs fast, flexible, and developer-friendly. It can even be deployed within an integrated development environment (IDE) known as GraphiQL. As an alternative to REST, GraphQL lets developers construct requests that pull data from multiple data sources in a single API call.\nApollo Federation\nAlthough Apollo Federation is a complex structure, we can briefly say that it is a technology that enables multiple graphql schemes to be gathered under a single gateway and accessed as a single scheme.\n\nApollo federation is an elegant approach to apply microservice pattern with GraphQL. Using federation you can easily split your schema into multiple subschemas and implement each subschema logic in it’s own service.\nIn this article we gonna implement a simple product-review graph, so we will have 4 services:\n\nproduct service\nreview service\napollo gateway\nauth service\n\nAuth service\nWe are using oauth2 authorization server that implemented using spring security authorization server framework to secure the application.we will not focus on this service in this article. OAuth authorization server is responsible for authenticating the users and issuing access tokens containing the user data and proper access policies.\n\nThe OAuth2 Authorization Server can be accessed from this link via kubernetes.\nhttp://auth.susimsek.github.io\nThe OAuth2 Authorization Server can be accessed from this link via heroku. https://graphql-fullstack-auth-service.herokuapp.com\nApollo Gateway\n\nLet’s define our Apollo Federation gateway.\nnpm install @apollo/gateway apollo-server graphql --save\nhttps://medium.com/media/cdd7e923df55696e20a3bc95c170ea3a/href\nWe added cors configuration, Vault and Consul configuration to apollo gateway.And also added authorization header value to context to send authorization header to subgraph.We used manuel composition to compose a supergraph schema from a collection of subgraph schemas.manuel composition recommended for production environment by the apollo federation team.\nProduct service\nFirst, we use spring graphql starter.Spring for GraphQL provides support for Spring applications built on GraphQL Java.The project reached version 1.0 in May 2022.\n<dependency>\n   <groupId>org.springframework.boot</groupId>\n   <artifactId>spring-boot-starter-graphql</artifactId>\n</dependency>\nThen we will user spring webflux starter to provide reactive programming support.Reactive systems better utilize modern processors. Also, the inclusion of back-pressure in reactive programming ensures better resilience between decoupled components.\n<dependency>\n   <groupId>org.springframework.boot</groupId>\n   <artifactId>spring-boot-starter-webflux</artifactId>\n</dependency>\nThen let’s define our product schema\nhttps://medium.com/media/d06079dbc030bc0cd676de6b995a5750/href\nTake a note of that @key directive in Product type. It indicates that we defined a GraphQL entity.\nLet’s create our controller to serve our product subgraph\nhttps://medium.com/media/02114e90722a69db30c19fb517345bd7/href\nReview service\nOnce we installed same dependencies as for product service we can define the schema.\nhttps://medium.com/media/29c9d0eea06f44efc24b991b3204469f/href\nAs you can see, for now review subgraph doesn’t have Query section at all — we defined the Review type and what the most interesting — extended the Product type with a new reviews property that returns an array of product’s reviews.\nLet’s create our controller to serve our review subgraph\nhttps://medium.com/media/8b0a57920343158c3fdb41b35e463c94/href\nGetting everything together\nNow, let’s launch and query. First start auth service and then our subgraphs. finally start the gateway. Apollo studio should be available on http://localhost:4000/.\nGet access token from oauth2 server and assign it to Authorization header. Then, run the following query.\nhttps://medium.com/media/e598349f411ef5ec672836987be175e9/href\nThe result should be\nhttps://medium.com/media/d9e37d845c9bb35bcd41e3a509263e56/href\nFrontend\n\nWe use GraphQL Code Generator is a CLI tool that generates code out of our GraphQL schema.\nInstall the following npm packages required to run GraphQL Code Generator\nnpm install @graphql-codegen/cli @graphql-codegen/introspection @graphql-codegen/typescript @graphql-codegen/typescript-operations @graphql-codegen/typescript-react-apollo --save-dev\nAnd also install the following npm packages generate types for TypeScript,Graphql and Apollo.\nnpm install graphql @apollo/client typescript --save\nThen add the codegen.yml file to your project. This file is a config for GraphQL-Codegen.\nhttps://medium.com/media/c8e71b0de06219bb1578d0ca91ba84a5/href\nThe next step is to add scripts to package.json\n\"scripts\": {\n  \"generate\": \"graphql-codegen --config codegen.yml\",\n  \"sonar\": \"node sonarqube-scanner.js\"\n}\nYou’ll run this script (npm run codegen) every time you've changed anything in your GraphQL API or in your GraphQL files to get the most up-to-date types generated.\nLet’s create the following GraphQL query to fetch product reviews\nhttps://medium.com/media/3649ddb37ccdcebfdcf66e0dccc516eb/href\nRun the codegen to generate types and hooks:\nnpm run generate\nUse the generated hook in react.\nhttps://medium.com/media/044ee8961f47a563fb4abf51823748e8/href\nThat’s it. Graphql codegen is amazing tool to generate reusable hooks in your GraphQL files.By using this approach we are able to;\n\nAvoid generating dynamic queries at runtime.\nUse graphql query language rather than a language-specific syntax like tagged template literals\nImprove on the DX for we auto-generate reusable hooks with type-safety and IDE IntelliSense\nValidate queries against our schema\nRebuild our code when we change our queries\n\nHeroku\nI just deployed oauth2 authorization server fullstack application to heroku.You can access the oauth2 authorization server via the links below.\n\nThe Oauth2 Authorization server can be accessed from this link.\nhttp://graphql-fullstack-auth-service.herokuapp.com\n\nYou can log in to the authorization server using that credential information or login to the authorization server with gmail.\nusername: admin\npassword: password\nDeployment with Docker Compose\nDocker Compose is a tool for defining and running multi-container Docker applications.\n\nYou can deploy the app by running the following bash command\nsudo chmod +x deploy.sh\n./deploy.sh -d\nYou can uninstall app the following bash command\n./deploy.sh -d -r\nThe Fullstack GraphQL App be accessed with nginx from the link below.\nhttp://127.0.0.1\nThe OAuth2 Authorization Server be accessed from the link below. (default nginx ingress)\nhttp://127.0.0.1:9000\nKubernetes Deployment with Helm\nHelm is a package manager for Kubernetes that allows developers and operators to more easily package, configure, and deploy applications and services onto Kubernetes clusters.\n\nYou can deploy the app by running the following bash command\nsudo chmod +x deploy.sh\n./deploy.sh -k\nYou can uninstall app the following bash command\n./deploy.sh -k -r\nYou can upgrade the App (if you have made any changes to the generated manifests) by running the following bash command\n./deploy.sh -u\nThe Fullstack GraphQL App be accessed with ingress from the link below.(default nginx ingress)\nhttp://gqlmsweb.susimsek.github.io/\nThe OAuth2 Authorization Server be accessed with ingress from the link below. (default nginx ingress)\nhttp://auth.susimsek.github.io/\nSummary\nIn this article, we have focused on how to develop a fullstack microservice application graphql based on Kubernetes.\nGraphQL is a very exciting new technology that can potentially revolutionize the way we develop Web APIs.With Apollo Federation, implementing federated GraphQL in the context of microservices becomes easier than ever.\nFinally, the full implementation of this article can be found in the GitHub project.\nMy article ends here. See you in the next articles.",
      "dc:creator": "Şuayb Şimşek",
      "guid": "https://medium.com/p/eb227e1a748b",
      "categories": ["microservices", "spring-boot", "graphql", "kubernetes", "react"],
      "isoDate": "2022-08-28T16:29:12.000Z"
    },
    {
      "creator": "Şuayb Şimşek",
      "title": "React & Spring Boot Hateoas Driven Fullstack Application on Kubernetes",
      "link": "https://suaybsimsek58.medium.com/react-spring-boot-hateoas-driven-fullstack-application-on-kubernetes-7ea33894d12b?source=rss-bda589f2335a------2",
      "pubDate": "Tue, 15 Mar 2022 19:22:26 GMT",
      "content:encoded": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*mbUH1UeaKybgUMKTP1ou5w.png\" /></figure><p>Hello everyone, In this article, we will develop a hateoas driven fullstack application using React and Spring Boot and deploy this application on Kubernetes.</p><h3>Richardson Maturity Model</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/535/1*dK6PpMxmdt-HH7EXO20Sew.png\" /></figure><p>Leonard Richardson analyzed a hundred different web service designs and divided these designs into four categories. These categories are based on how much the web services are <a href=\"https://restfulapi.net/rest-architectural-constraints/\">REST compliant</a>.</p><p>This model of division of REST services to identify their maturity level — is called Richardson Maturity Model.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/266/1*4xcJomcTOcGeLWg4J7ghqA.png\" /></figure><p>Richardson used three main factors to decide the maturity of a service. These factors are</p><ol><li><a href=\"https://restfulapi.net/resource-naming/\">URI</a>,</li><li><a href=\"https://restfulapi.net/http-methods/\">HTTP Methods</a>,</li><li><a href=\"https://restfulapi.net/hateoas/\">HATEOAS (Hypermedia)</a></li></ol><p>The more a service employs these factors — the more mature it shall be considered.</p><h3>Maturity Levels</h3><p>In his analysis, Richardson described total 4 maturity levels as given below:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/468/1*enHtcgVqThY90MpbMFbc3Q.png\" /></figure><ul><li>Level Zero</li><li>Level One</li><li>Level Two</li><li>Level Three</li></ul><p>Note that Roy Fielding has already made it very clear that <a href=\"http://roy.gbiv.com/untangled/2008/rest-apis-must-be-hypertext-driven\">level 3 RMM is a pre-condition of REST</a>.</p><h3>Level Three</h3><p>Level three of maturity makes use of all three, i.e. URIs and HTTP, and HATEOAS.</p><p>Level three is the most mature level of Richardson’s model, which encourages easy discoverability. This level makes it easy for the responses to be self-descriptive by using HATEOAS.</p><p>Level three services lead the service consumers through a trail of resources, causing application state transitions as a result.</p><h3>Hateoas</h3><p>The term <strong>HATEOAS</strong> stands for the phrase <strong>H</strong>ypermedia <strong>A</strong>s <strong>T</strong>he <strong>E</strong>ngine <strong>O</strong>f <strong>A</strong>pplication <strong>S</strong>tate. To understand this further, we first need to understand the meaning of <strong>Hypermedia</strong>.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/153/1*1qTT497DQ-ie1B7LmkdI8Q.png\" /></figure><p>The single most important reason for HATEOAS is <strong>loose coupling</strong>. If a consumer of a REST service needs to hard-code all the resource URLs, then it is tightly coupled with your service implementation. Instead, if you return the URLs, it could use for the actions, then it is loosely coupled. There is no tight dependency on the URI structure, as it is specified and used from the response.</p><h3>HAL — Hypertext Application Language</h3><p>When you design a RESTful service, there is a need to specify how to return data and links corresponding to a request. HAL is a simple format that gives an easy, consistent way to hyperlink between resources in your REST API. Here is an example</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*7_IzGR9cvywMJ4NHbuQBaA.png\" /></figure><p>With HAL, you have a few categories of representations:</p><ul><li><strong>Links:</strong> Specified as a combination of</li><li>Target — Given as a URI</li><li>Relation — A name</li><li><strong>Embedded Resources:</strong> Other resources contained within a given REST resource</li><li><strong>State:</strong> The actual resource data</li></ul><p>If you happen to use the Spring Framework to develop your REST service, then Spring HATEOAS is a good engine to use for your service.</p><h3>Spring Boot HATEOAS</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/363/1*OrB6plZLWwdaapcgQeNKSw.png\" /></figure><p>First, let’s add the Spring HATEOAS dependency to our <em>pom.xml</em></p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/1ee00ddc418508cf9c7e10d0fae339c4/href\">https://medium.com/media/1ee00ddc418508cf9c7e10d0fae339c4/href</a></iframe><p>We will use SpringDoc for api documentation. Springdoc supports hateoas.we simply add the <em>springdoc-openapi-ui and springdoc-openapi-hateoas ui </em>dependency to our <em>pom.xml</em>:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/331/1*dEdt4WAg0T8wJOSrhMidZg.png\" /></figure><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/407ef64f3c894855c9b06bc4c976321f/href\">https://medium.com/media/407ef64f3c894855c9b06bc4c976321f/href</a></iframe><p>Native images provide various advantages like an instant startup and reduced memory consumption.We will use Spring Native to compile and build native images using Buildpacks and GraalVM’s native build tools.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/414/1*7nArJMkG1Q5NmiPOuSfY2w.png\" /></figure><p>Let’s add the <a href=\"https://repo.spring.io/artifactory/release/org/springframework/experimental/spring-native/\"><em>spring-native</em></a> Maven dependency and required plugins,repos.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/c9771fcc2f34225ec626c2b3aa808767/href\">https://medium.com/media/c9771fcc2f34225ec626c2b3aa808767/href</a></iframe><p>Here, we’ll use the tiny builder out of the various available builders like base and full to build a native image.Also, we enabled the upx by providing the <em>UPX</em> value to the BP_BINARY_COMPRESSION_METHOD environment variable.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/444/1*PcFg4nqcTsO14-6IKEx9hQ.png\" /></figure><p>UPX is an advanced file compressor that compresses executable files.When spring native or golang projects are compiled, since they produce executable programs directly, we can reduce the size of these programs by 50–70% with upx.</p><p>we’ll add property to use Java 17 for compilation</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/ef85335f9696a7cc3891288107d091f4/href\">https://medium.com/media/ef85335f9696a7cc3891288107d091f4/href</a></iframe><p><strong>Spring HATEOAS offers three abstractions for creating the URI — <em>RepresentationModel, Link, and WebMvcLinkBuilder</em></strong>. We can use these to create the metadata and associate it to the resource representation.</p><h3>Adding Hypermedia Support to a Resource</h3><p>The CapabilityDto resource extends from the RepresentationModel class to inherit the add() method. So once we create a link, we can easily set that value to the resource representation without adding any new fields to it.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/3c65184ea0664ed5c9d997738e105cc8/href\">https://medium.com/media/3c65184ea0664ed5c9d997738e105cc8/href</a></iframe><h3>Creating Links</h3><p>Spring Hateoas provides <strong>the <em>WebMvcLinkBuilder</em> — which simplifies building URIs</strong> by avoiding hard-coded the links.</p><p>The following snippet shows building the customer self-link using the <em>WebMvcLinkBuilder</em> class:</p><pre>linkTo(methodOn(CapabilityController.class).getCapability(resource.getContent().getId())).withSelfRel()</pre><p>Let’s have a look:</p><ul><li>the <em>linkTo()</em> method inspects the controller class and obtains its root mapping</li><li>the <em>slash()</em> method adds the <em>customerId</em> value as the path variable of the link</li><li>finally, the <em>withSelfMethod()</em> qualifies the relation as a self-link</li></ul><h3>Assemblers</h3><p>it’s not about assembly language, but about a special kind of class that converts our resource to RepresentationModel.</p><p>One of such assemblers is SimpleRepresentationModelAssembler. Its implementation goes as follows:</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/805ddf1563097696c072a07e12c36749/href\">https://medium.com/media/805ddf1563097696c072a07e12c36749/href</a></iframe><p>In this case, our entity will be wrapped in an EntityModel (this class extends RepresentationModel) to which the links specified by us in the addLinks() will be added. Here we overwrite two addLinks() methods – one for entire data collections and the other for single resources. Then, as part of the controller, it is enough to call the toModel() or toCollectionModel()<strong> </strong>method (addLinks() are template methods here), depending on whether we return a collection or a single representation.</p><p>Spring hateoas provides automatic pagination links, we must use <a href=\"https://docs.spring.io/spring-hateoas/docs/current/api/org/springframework/hateoas/PagedModel.html\">PagedModel</a> provided by spring hateoas module which helps in creating representations of pageable collections.</p><p>PagedResourcesAssembler accepts the JPA entities list or Dtos List, and convert it to PagedModel.</p><h3>Spring HATEOAS in Action</h3><ul><li>Finally, PagedModel,EntityModel,CollectionModel is returned as API response from web controller.</li></ul><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/180021334c6e728aefdc76bf50c890db/href\">https://medium.com/media/180021334c6e728aefdc76bf50c890db/href</a></iframe><h3>Verify links</h3><p>Next, let’s invoke the getAllCapabilitiesWithPagination<em>()</em> method</p><p>curl <a href=\"http://localhost:8080/api/paged-capabilities\">http://localhost:8080/api/paged-capabilities</a></p><p>And examine the result:</p><pre>{<br>  &quot;_embedded&quot;: {<br>    &quot;capabilities&quot;: [<br>      {<br>        &quot;id&quot;: 1,<br>        &quot;techStack&quot;: &quot;ReactJS&quot;,<br>        &quot;numOfDevelopers&quot;: 70,<br>        &quot;numOfAvailableDevelopers&quot;: 20,<br>        &quot;_links&quot;: {<br>          &quot;self&quot;: {<br>            &quot;href&quot;: &quot;http://localhost:8080/api/capabilities/1&quot;<br>          },<br>          &quot;capabilities&quot;: {<br>            &quot;href&quot;: &quot;http://localhost:8080/api/capabilities&quot;<br>          }<br>        }<br>      }<br>    ]<br>  },<br>  &quot;_links&quot;: {<br>    &quot;first&quot;: {<br>      &quot;href&quot;: &quot;http://localhost:8080/api/paged-capabilities?page=0&amp;size=1&quot;<br>    },<br>    &quot;self&quot;: {<br>      &quot;href&quot;: &quot;http://localhost:8080/api/paged-capabilities?page=0&amp;size=1&quot;<br>    },<br>    &quot;next&quot;: {<br>      &quot;href&quot;: &quot;http://localhost:8080/api/paged-capabilities?page=1&amp;size=1&quot;<br>    },<br>    &quot;last&quot;: {<br>      &quot;href&quot;: &quot;http://localhost:8080/api/paged-capabilities?page=12&amp;size=1&quot;<br>    },<br>    &quot;capabilities&quot;: {<br>      &quot;href&quot;: &quot;http://localhost:8080/api/capabilities&quot;<br>    }<br>  },<br>  &quot;page&quot;: {<br>    &quot;size&quot;: 1,<br>    &quot;totalElements&quot;: 13,<br>    &quot;totalPages&quot;: 13,<br>    &quot;number&quot;: 0<br>  }<br>}</pre><h3>Spring Hateoas Conclusion</h3><p>We saw hateoas driven rest api that adding HATEOAS links using spring boot hateoas module is very much easy and requires very less time and effort. In return, it increases the discoverability and usefulness of APIs by many folds.</p><h3>React Hateoas</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/395/1*qhOfS0A0lU8GcFAD456Avg.png\" /></figure><p>/api/paged-capabilities api returns the links associated with the capability resource.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*2k-Oj7CVjEsw8DbMrGTYug.png\" /></figure><p>Without HATEOAS, the client needs to know the URIs beforehand to call the correct endpoints. Instead of hardcoding the URIs in our component, we can refer to the links contained within our capability resource</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/5de3ff8de2584da5982b901b3dcb3d1c/href\">https://medium.com/media/5de3ff8de2584da5982b901b3dcb3d1c/href</a></iframe><p>We’ll start by writing a thunk function that makes an api call to our /api/paged-accounts endpoint to request an page of capality object, and then dispatch an action containing that array and links.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/e7604c41be65117e6e0da73715be1acb/href\">https://medium.com/media/e7604c41be65117e6e0da73715be1acb/href</a></iframe><p>In the code below, we created a capabilityReducer.GET_CAPABILITIES action will return the page, payload and links.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/592510203994c4acffeaff1332aee752/href\">https://medium.com/media/592510203994c4acffeaff1332aee752/href</a></iframe><p>AddCapability component gets the post Link from the redux store and passes this link as a parameter to the addCapabilityHandler.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/664b463d6d8569e31b97031ae7ecae69/href\">https://medium.com/media/664b463d6d8569e31b97031ae7ecae69/href</a></iframe><p>addCapabilityHandler executes <strong>addCapability()</strong> asynchronous HTTP request .Then, It dispatches ADD_CAPABILITY action to update the redux store.</p><h3>Paging</h3><p>Spring hateoas provides PagedModel to enable automatic pagination links.</p><p>We can retrieve a list of capabilities from GET /api/paged-capabilities, which features basic pagination:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*m8dI61JhYEHcJNikd_M96Q.png\" /></figure><p>The outer links array in the response payload includes references to the first, previous, current (via &quot;rel&quot; : &quot;self&quot;), next, and last resources pages</p><h3>React Hateoas Conclusion</h3><p>100% HATEOAS IS compatible with React &amp; Flux, HATEOAS is compatible with Angular, HATEOAS is compatible with JQuery and even vanilla JS.</p><p>HATEOAS doesn’t not impose any technical or implementation requirements on a consuming client.</p><p>HATEOAS is in fact simply a concept to which you can design your API (you can use one of several standards though like <a href=\"http://stateless.co/hal_specification.html\">HAL</a>)</p><p>Basically if you can call an API then you can implement a HATEOAS client.</p><p>So how to get there:</p><ul><li>Step 1, how would you normally do an API call in React? Do it the same way.</li><li>Step 2, interrogate response.</li><li>Step 3, based on response, respond in the UI appropriately.</li></ul><h3>Heroku</h3><p>I deployed to heroku the hateoas fullstack application.You can access the web ui of the application and the swagger documentation via the links below.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/148/1*VEUaD79frAwYs4GYpDRskQ.png\" /></figure><p>The Hateoas Fullstack application can be accessed from this link.</p><p><a href=\"https://hateoas-fullstack-ui.herokuapp.com/\">https://hateoas-fullstack-ui.herokuapp.com</a></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*a2-2OYYPhZ1TvjlS.png\" /></figure><p>The swagger ui can be accessed from this link.<br><a href=\"https://hateoas-fullstack-api.herokuapp.com/swagger-ui.html\">https://hateoas-fullstack-api.herokuapp.com/swagger-ui.html</a></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*vTkE65V0Kq1hyI6z.png\" /></figure><h3>Kubernetes Deployment with Helm</h3><p><a href=\"https://www.helm.sh/\">Helm</a> is a package manager for Kubernetes that allows developers and operators to more easily package, configure, and deploy applications and services onto Kubernetes clusters.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/367/1*17xVcNLzYsJABpViO9_SGg.png\" /></figure><p>You can deploy hateoas fullstack app by running the following bash command</p><pre>./helm-apply.sh</pre><p>You can upgrade hateoas fullstack apps (if you have made any changes to the generated manifests) by running the following bash command</p><pre>./helm-upgrade.sh</pre><p>The Hateoas Fullstack application can be accessed with ingress from the link below.</p><p><a href=\"http://hateoas-fullstack-ui.github.io\">http://hateoas-fullstack-ui.github.io</a></p><p>Finally, the full implementation of this article can be found in <a href=\"https://github.com/susimsek/HateoasFullstackApp\">the GitHub project</a>.</p><p>My article ends here. See you in the next articles.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=7ea33894d12b\" width=\"1\" height=\"1\" alt=\"\">",
      "content:encodedSnippet": "Hello everyone, In this article, we will develop a hateoas driven fullstack application using React and Spring Boot and deploy this application on Kubernetes.\nRichardson Maturity Model\n\nLeonard Richardson analyzed a hundred different web service designs and divided these designs into four categories. These categories are based on how much the web services are REST compliant.\nThis model of division of REST services to identify their maturity level — is called Richardson Maturity Model.\n\nRichardson used three main factors to decide the maturity of a service. These factors are\n\nURI,\nHTTP Methods,\nHATEOAS (Hypermedia)\n\nThe more a service employs these factors — the more mature it shall be considered.\nMaturity Levels\nIn his analysis, Richardson described total 4 maturity levels as given below:\n\nLevel Zero\nLevel One\nLevel Two\nLevel Three\n\nNote that Roy Fielding has already made it very clear that level 3 RMM is a pre-condition of REST.\nLevel Three\nLevel three of maturity makes use of all three, i.e. URIs and HTTP, and HATEOAS.\nLevel three is the most mature level of Richardson’s model, which encourages easy discoverability. This level makes it easy for the responses to be self-descriptive by using HATEOAS.\nLevel three services lead the service consumers through a trail of resources, causing application state transitions as a result.\nHateoas\nThe term HATEOAS stands for the phrase Hypermedia As The Engine Of Application State. To understand this further, we first need to understand the meaning of Hypermedia.\n\nThe single most important reason for HATEOAS is loose coupling. If a consumer of a REST service needs to hard-code all the resource URLs, then it is tightly coupled with your service implementation. Instead, if you return the URLs, it could use for the actions, then it is loosely coupled. There is no tight dependency on the URI structure, as it is specified and used from the response.\nHAL — Hypertext Application Language\nWhen you design a RESTful service, there is a need to specify how to return data and links corresponding to a request. HAL is a simple format that gives an easy, consistent way to hyperlink between resources in your REST API. Here is an example\n\nWith HAL, you have a few categories of representations:\n\nLinks: Specified as a combination of\nTarget — Given as a URI\nRelation — A name\nEmbedded Resources: Other resources contained within a given REST resource\nState: The actual resource data\n\nIf you happen to use the Spring Framework to develop your REST service, then Spring HATEOAS is a good engine to use for your service.\nSpring Boot HATEOAS\n\nFirst, let’s add the Spring HATEOAS dependency to our pom.xml\nhttps://medium.com/media/1ee00ddc418508cf9c7e10d0fae339c4/href\nWe will use SpringDoc for api documentation. Springdoc supports hateoas.we simply add the springdoc-openapi-ui and springdoc-openapi-hateoas ui dependency to our pom.xml:\nhttps://medium.com/media/407ef64f3c894855c9b06bc4c976321f/href\nNative images provide various advantages like an instant startup and reduced memory consumption.We will use Spring Native to compile and build native images using Buildpacks and GraalVM’s native build tools.\n\nLet’s add the spring-native Maven dependency and required plugins,repos.\nhttps://medium.com/media/c9771fcc2f34225ec626c2b3aa808767/href\nHere, we’ll use the tiny builder out of the various available builders like base and full to build a native image.Also, we enabled the upx by providing the UPX value to the BP_BINARY_COMPRESSION_METHOD environment variable.\n\nUPX is an advanced file compressor that compresses executable files.When spring native or golang projects are compiled, since they produce executable programs directly, we can reduce the size of these programs by 50–70% with upx.\nwe’ll add property to use Java 17 for compilation\nhttps://medium.com/media/ef85335f9696a7cc3891288107d091f4/href\nSpring HATEOAS offers three abstractions for creating the URI — RepresentationModel, Link, and WebMvcLinkBuilder. We can use these to create the metadata and associate it to the resource representation.\nAdding Hypermedia Support to a Resource\nThe CapabilityDto resource extends from the RepresentationModel class to inherit the add() method. So once we create a link, we can easily set that value to the resource representation without adding any new fields to it.\nhttps://medium.com/media/3c65184ea0664ed5c9d997738e105cc8/href\nCreating Links\nSpring Hateoas provides the WebMvcLinkBuilder — which simplifies building URIs by avoiding hard-coded the links.\nThe following snippet shows building the customer self-link using the WebMvcLinkBuilder class:\nlinkTo(methodOn(CapabilityController.class).getCapability(resource.getContent().getId())).withSelfRel()\nLet’s have a look:\n\nthe linkTo() method inspects the controller class and obtains its root mapping\nthe slash() method adds the customerId value as the path variable of the link\nfinally, the withSelfMethod() qualifies the relation as a self-link\n\nAssemblers\nit’s not about assembly language, but about a special kind of class that converts our resource to RepresentationModel.\nOne of such assemblers is SimpleRepresentationModelAssembler. Its implementation goes as follows:\nhttps://medium.com/media/805ddf1563097696c072a07e12c36749/href\nIn this case, our entity will be wrapped in an EntityModel (this class extends RepresentationModel) to which the links specified by us in the addLinks() will be added. Here we overwrite two addLinks() methods – one for entire data collections and the other for single resources. Then, as part of the controller, it is enough to call the toModel() or toCollectionModel() method (addLinks() are template methods here), depending on whether we return a collection or a single representation.\nSpring hateoas provides automatic pagination links, we must use PagedModel provided by spring hateoas module which helps in creating representations of pageable collections.\nPagedResourcesAssembler accepts the JPA entities list or Dtos List, and convert it to PagedModel.\nSpring HATEOAS in Action\n\nFinally, PagedModel,EntityModel,CollectionModel is returned as API response from web controller.\nhttps://medium.com/media/180021334c6e728aefdc76bf50c890db/href\nVerify links\nNext, let’s invoke the getAllCapabilitiesWithPagination() method\ncurl http://localhost:8080/api/paged-capabilities\nAnd examine the result:\n{\n  \"_embedded\": {\n    \"capabilities\": [\n      {\n        \"id\": 1,\n        \"techStack\": \"ReactJS\",\n        \"numOfDevelopers\": 70,\n        \"numOfAvailableDevelopers\": 20,\n        \"_links\": {\n          \"self\": {\n            \"href\": \"http://localhost:8080/api/capabilities/1\"\n          },\n          \"capabilities\": {\n            \"href\": \"http://localhost:8080/api/capabilities\"\n          }\n        }\n      }\n    ]\n  },\n  \"_links\": {\n    \"first\": {\n      \"href\": \"http://localhost:8080/api/paged-capabilities?page=0&size=1\"\n    },\n    \"self\": {\n      \"href\": \"http://localhost:8080/api/paged-capabilities?page=0&size=1\"\n    },\n    \"next\": {\n      \"href\": \"http://localhost:8080/api/paged-capabilities?page=1&size=1\"\n    },\n    \"last\": {\n      \"href\": \"http://localhost:8080/api/paged-capabilities?page=12&size=1\"\n    },\n    \"capabilities\": {\n      \"href\": \"http://localhost:8080/api/capabilities\"\n    }\n  },\n  \"page\": {\n    \"size\": 1,\n    \"totalElements\": 13,\n    \"totalPages\": 13,\n    \"number\": 0\n  }\n}\nSpring Hateoas Conclusion\nWe saw hateoas driven rest api that adding HATEOAS links using spring boot hateoas module is very much easy and requires very less time and effort. In return, it increases the discoverability and usefulness of APIs by many folds.\nReact Hateoas\n\n/api/paged-capabilities api returns the links associated with the capability resource.\n\nWithout HATEOAS, the client needs to know the URIs beforehand to call the correct endpoints. Instead of hardcoding the URIs in our component, we can refer to the links contained within our capability resource\nhttps://medium.com/media/5de3ff8de2584da5982b901b3dcb3d1c/href\nWe’ll start by writing a thunk function that makes an api call to our /api/paged-accounts endpoint to request an page of capality object, and then dispatch an action containing that array and links.\nhttps://medium.com/media/e7604c41be65117e6e0da73715be1acb/href\nIn the code below, we created a capabilityReducer.GET_CAPABILITIES action will return the page, payload and links.\nhttps://medium.com/media/592510203994c4acffeaff1332aee752/href\nAddCapability component gets the post Link from the redux store and passes this link as a parameter to the addCapabilityHandler.\nhttps://medium.com/media/664b463d6d8569e31b97031ae7ecae69/href\naddCapabilityHandler executes addCapability() asynchronous HTTP request .Then, It dispatches ADD_CAPABILITY action to update the redux store.\nPaging\nSpring hateoas provides PagedModel to enable automatic pagination links.\nWe can retrieve a list of capabilities from GET /api/paged-capabilities, which features basic pagination:\n\nThe outer links array in the response payload includes references to the first, previous, current (via \"rel\" : \"self\"), next, and last resources pages\nReact Hateoas Conclusion\n100% HATEOAS IS compatible with React & Flux, HATEOAS is compatible with Angular, HATEOAS is compatible with JQuery and even vanilla JS.\nHATEOAS doesn’t not impose any technical or implementation requirements on a consuming client.\nHATEOAS is in fact simply a concept to which you can design your API (you can use one of several standards though like HAL)\nBasically if you can call an API then you can implement a HATEOAS client.\nSo how to get there:\n\nStep 1, how would you normally do an API call in React? Do it the same way.\nStep 2, interrogate response.\nStep 3, based on response, respond in the UI appropriately.\n\nHeroku\nI deployed to heroku the hateoas fullstack application.You can access the web ui of the application and the swagger documentation via the links below.\n\nThe Hateoas Fullstack application can be accessed from this link.\nhttps://hateoas-fullstack-ui.herokuapp.com\n\nThe swagger ui can be accessed from this link.\nhttps://hateoas-fullstack-api.herokuapp.com/swagger-ui.html\n\nKubernetes Deployment with Helm\nHelm is a package manager for Kubernetes that allows developers and operators to more easily package, configure, and deploy applications and services onto Kubernetes clusters.\n\nYou can deploy hateoas fullstack app by running the following bash command\n./helm-apply.sh\nYou can upgrade hateoas fullstack apps (if you have made any changes to the generated manifests) by running the following bash command\n./helm-upgrade.sh\nThe Hateoas Fullstack application can be accessed with ingress from the link below.\nhttp://hateoas-fullstack-ui.github.io\nFinally, the full implementation of this article can be found in the GitHub project.\nMy article ends here. See you in the next articles.",
      "dc:creator": "Şuayb Şimşek",
      "guid": "https://medium.com/p/7ea33894d12b",
      "categories": ["kubernetes", "spring-boot", "hateoas", "react", "spring-native"],
      "isoDate": "2022-03-15T19:22:26.000Z"
    },
    {
      "creator": "Şuayb Şimşek",
      "title": "Spring Boot Fullstack Blockchain Application With Hyperledger Fabric running on Kubernetes (Part 6)",
      "link": "https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-6-b82662ada0a4?source=rss-bda589f2335a------2",
      "pubDate": "Mon, 10 Jan 2022 20:01:06 GMT",
      "content:encoded": "<h3>Spring Boot Fullstack Blockchain Application With Hyperledger Fabric running on Kubernetes (Part 6) — Orderer</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*268MMjpMXhKeCJKmo74BWA.png\" /></figure><p>Hello everyone, through this article series we will the Hyperledger Fabric integration with Spring Boot.In this article, we will look into orderer.Also,I will also explain how to deploy orderer service on Kubernetes.</p><p>Other articles on Hyperledger Fabric integration with Spring Boot can be accessed from the links below.</p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-1-c8f7c87d60eb\">Part 1 — Introduction</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-ddce8d25858d\">Part 2 — Kubernetes Cluster Setup</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-3-7eb2fc75ba60\">Part 3 — Fabric CA Server</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-4-d6583a57153c\">Part 4 — Generating Certificates and Artifacts</a></p><p><a href=\"https://suaybsimsek58.medium.com/1723502770e7\">Part 5 — Kafka</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-6-b82662ada0a4\">Part 6— Orderer</a></p><h3>Orderer</h3><p>The Orderer is responsible for packaging transactions into Blocks, and distribute them to Anchor Peers across the network.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*V9-6AwxNQQFzGDs8.png\" /></figure><p>The transaction flow of Fabric have the steps Proposal, Packaging and Validation. The orderer is responsible for Packaging and involved in the Validation step for distribution of new blocks on the network.</p><p>Ordering service provides a shared communication channel to clients and peers, offering a broadcast service for messages containing transactions. Clients connect to the channel and may broadcast messages on the channel which are then delivered to all peers. The channel supports atomic delivery of all messages, that is, message communication with total-order delivery and (implementation specific) reliability. In other words, the channel outputs the same messages to all connected peers and outputs them to all peers in the same logical order.</p><p>Ordering service is not capable of transaction validations, it’s primary goal to provide total order for transactions published, cut blocks with ordered transactions.</p><h3>Ordering Service Implementations</h3><p>While every ordering service currently available handles transactions and configuration updates the same way, there are nevertheless several different implementations for achieving consensus on the strict ordering of transactions between ordering service nodes</p><h3>Raft</h3><p>New as of v1.4.1, Raft is a crash fault tolerant (CFT) ordering service based on an implementation of <a href=\"https://raft.github.io/raft.pdf\">Raft protocol</a> in <a href=\"https://coreos.com/etcd/\">etcd</a></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*hD4gWyOV642eUcMH.png\" /></figure><p>Raft follows a “leader and follower” model, where a leader node is elected (per channel) and its decisions are replicated by the followers.</p><p>Raft ordering services should be easier to set up and manage than Kafka-based ordering services, and their design allows different organizations to contribute nodes to a distributed ordering service.</p><h3>Kafka</h3><p>Similar to Raft-based ordering, Apache Kafka is a CFT implementation that uses a “leader and follower” node configuration.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*lwPYdYLUhGgofsc6.png\" /></figure><p>Kafka utilizes a ZooKeeper ensemble for management purposes.Asset transfer projesinde kafka kullanıyoruz.We will use kafka.</p><h3><strong>Solo</strong> (deprecated in v2.x)</h3><p>The Solo implementation of the ordering service is intended for test only and consists only of a single ordering node.It has been deprecated and may be removed entirely in a future release.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/680/0*FhYVUBpcoh46CoKv.png\" /></figure><p>Existing users of Solo should move to a single node Raft network for equivalent function.</p><h3>Installation of Orderer on Kubernetes</h3><p>For the asset transfer project, we will set up kafka and zookeeper to run 5 pods on kubernetes.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/629/1*FJ7d03WfEvpA-jpNz_bbwg.png\" /></figure><p>Let’s open the project we downloaded from <a href=\"https://github.com/susimsek/spring-boot-hlf-k8s-fullstack-blockchain-app\">this link</a> and go to the directory where the k8s is located.</p><pre>$ cd deploy/k8s</pre><h3>Kafka and Zookeeper Deployment</h3><p>Let’s create a deployment for each orderer.</p><p>The yaml files that create it are in the following directory in the project below.</p><p>deploy/k8s/orderer/orderer.yaml</p><p>deploy/k8s/orderer/orderer2.yaml</p><p>deploy/k8s/orderer/orderer3.yaml</p><p>deploy/k8s/orderer/orderer4.yaml</p><p>deploy/k8s/orderer/orderer5.yaml</p><pre>apiVersion: apps/v1<br>kind: Deployment<br>metadata:<br>  name: orderer<br>  labels: <br>    app: orderer</pre><ul><li>The Deployment for orderer 1, named orderer.</li></ul><pre>apiVersion: apps/v1<br>kind: Deployment<br>metadata:<br>  name: orderer5<br>  labels: <br>    app: orderer5</pre><ul><li>The Deployment for orderer 5, named orderer5.In other orderers, this value is assigned to orderer2,orderer3,orderer4.</li></ul><pre>spec:<br>  selector:<br>    matchLabels:<br>      app: orderer<br>  replicas: 1</pre><ul><li>The Deployment, has a Spec that indicates that 1 replicas of the orderer container will be launched in unique Pods.This value is assigned the same for each orderer.</li></ul><pre>volumes:<br>  - name: fabricfiles<br>    persistentVolumeClaim:<br>      claimName: fabricfiles-pvc</pre><ul><li>The volumeClaimTemplates will provide stable storage using <a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/\">PersistentVolumes</a> provisioned by a PersistentVolume Provisioner.It should be the same as the pvc metadata name.This value is assigned the same for each orderer.</li></ul><pre>volumeMounts:<br>  - name:  fabricfiles<br>    mountPath: /organizations<br>    subPath: organizations <em># </em>required for certificates.<em><br>  </em>- name: fabricfiles<br>    mountPath: /system-genesis-block<br>    subPath: system-genesis-block <em># </em>It needs a genesis block.<em><br>  </em>- name: fabricfiles<br>    mountPath: /var/hyperledger/production/orderer<br>    subPath: state/orderer <em># orderer persistence data</em></pre><p>The Orderer container mounts the PV at /organizations for certificates.</p><p>The Orderer container mounts the PV at /system-genesis-block for genesis block.<em> </em>It needs a genesis block.</p><p>The Orderer container mounts the PV at /var/hyperledger/production/orderer for orderer persistence data.<em> </em>This data will be kept in state/orderer sub directory on nfs server.</p><pre>- name: fabricfiles<br>  mountPath: /var/hyperledger/production/orderer<br>  subPath: state/orderer5</pre><p>The persistence data of other orderers are also kept in subdirectories such as state/orderer2, state/orderer5 on the nfs server.</p><pre>livenessProbe:<br>  httpGet:<br>    port: 9444<br>    path: /healthz<br>  initialDelaySeconds: 60<br>  timeoutSeconds: 5<br>  failureThreshold: 6<br>readinessProbe:<br>    httpGet:<br>      port: 9444<br>      path: /healthz<br>    initialDelaySeconds: 5<br>    timeoutSeconds: 3<br>    periodSeconds: 5</pre><p>when a container get in the ready state, kubernetes starts to route traffic to the relavent pod. But the pod in the container may not be ready to accept traffic. Therefore, we need to specify “<em>liveness</em>” and “<em>readiness</em>” probes for applications in order kubernetes to do this process more efficiently.</p><p>Kubelet will check whether the container is alive and healthy by sending requests to the /healthz path on port 9443 and expect a success result code.</p><pre>spec:<br>  containers:<br>  - name: orderer<br>    image: hyperledger/fabric-orderer:2.3<br>    imagePullPolicy: IfNotPresent</pre><p>2.3 is assigned as Hyperledger fabric orderer docker image version.</p><p>Set imagePullPolicy to IfNotPresent or Never and <a href=\"http://kubernetes.io/docs/user-guide/images/#pre-pulling-images\"><em>pre-pull</em></a>: Pull manually images on each cluster node so the latest is cached, then do a kubectl rolling-update or similar to restart Pods.</p><p>The following environment variables are assigned for orderer.</p><pre>env:<br>  - name: CONFIGTX_ORDERER_ADDRESSES<br>    value: &quot;orderer:7050&quot;<br>  - name: ORDERER_GENERAL_LISTENADDRESS<br>    value: &quot;0.0.0.0&quot;<br>  - name: ORDERER_GENERAL_LISTENPORT<br>    value: &quot;7050&quot;<br>  - name: ORDERER_GENERAL_LOGLEVEL<br>    value: debug<br>  - name: ORDERER_GENERAL_LOCALMSPDIR<br>    value: /organizations/ordererOrganizations/example.com/orderers/orderer.example.com/msp<br>  - name: ORDERER_GENERAL_LOCALMSPID<br>    value: OrdererMSP<br>  - name: ORDERER_GENERAL_GENESISMETHOD<br>    value: file<br>  - name: ORDERER_GENERAL_GENESISFILE<br>    value: /system-genesis-block/genesis.block<br>  - name: ORDERER_GENERAL_TLS_ENABLED<br>    value: &quot;true&quot;<br>  - name: ORDERER_GENERAL_TLS_PRIVATEKEY<br>    value: /organizations/ordererOrganizations/example.com/orderers/orderer.example.com/tls/server.key<br>  - name: ORDERER_GENERAL_TLS_CERTIFICATE<br>    value: /organizations/ordererOrganizations/example.com/orderers/orderer.example.com/tls/server.crt<br>  - name: ORDERER_GENERAL_TLS_ROOTCAS<br>    value: /organizations/ordererOrganizations/example.com/orderers/orderer.example.com/tls/ca.crt<br>  - name:  ORDERER_GENERAL_CLUSTER_CLIENTPRIVATEKEY<br>    value: /organizations/ordererOrganizations/example.com/orderers/orderer.example.com/tls/server.key<br>  - name:  ORDERER_GENERAL_CLUSTER_CLIENTCERTIFICATE<br>    value: /organizations/ordererOrganizations/example.com/orderers/orderer.example.com/tls/server.crt<br>  - name: ORDERER_OPERATIONS_LISTENADDRESS <em># metric endpoint<br>    </em>value: 0.0.0.0:9444<br>  - name: ORDERER_METRICS_PROVIDER<br>    value: prometheus<br>  - name: CONFIGTX_ORDERER_ORDERERTYPE<br>    value: kafka<br>  - name: CONFIGTX_ORDERER_KAFKA_BROKERS<br>    value: &quot;broker-0.broker:9092,broker-1.broker:9092&quot;<br>  - name: ORDERER_KAFKA_RETRY_SHORTINTERVAL<br>    value: 1s<br>  - name: ORDERER_KAFKA_RETRY_SHORTTOTAL<br>    value: 30s<br>  - name: ORDERER_KAFKA_VERBOSE<br>    value: &quot;true&quot;</pre><p>ORDERER_GENERAL_GENESISFILE: path to genesis file path</p><p>ORDERER_GENERAL_LOCALMSPID: ID to load the MSP definition</p><p>ORDERER_GENERAL_LOCALMSPDIR: MSPDir is the filesystem path which contains the MSP configuration</p><p>ORDERER_GENERAL_TLS_ENABLED: enable TLS with client authentication.</p><p>ORDERER_GENERAL_TLS_PRIVATEKEY: fully qualified path of the file that contains the server private key</p><p>ORDERER_GENERAL_TLS_CERTIFICATE: fully qualified path of the file that contains the server certificate</p><p>ORDERER_GENERAL_TLS_ROOTCAS : fully qualified path of the file that contains the certificate chain of the CA that issued TLS server certificate</p><p>ORDERER_GENERAL_GENESISMETHOD: <em>file</em> is used when you want provide the genesis block as file to the container</p><p>ORDERER_GENERAL_TLS_CLIENTROOTCAS: fully qualified path of the file that contains the certificate chain of the CA that issued TLS server certificate</p><p>CONFIGTX_ORDERER_ORDERERTYPE: The orderer implementation to start.Available types are solo,kafka and etcdraft.</p><p>ORDERER_KAFKA_RETRY_SHORTINTERVAL: The order node may fail to connect kafka_ kafka_ RETRY_ Shortentreval is the interval between retries.</p><p>ORDERER_KAFKA_RETRY_SHORTTOTAL: Total number of retries.</p><p>CONFIGTX_ORDERER_KAFKA_BROKERS:Instructs Orderer how to get in touch with Kafka.</p><p>ORDERER_GENERAL_LISTENPORT: This value is the port that the orderer listens to.</p><p>ORDERER_KAFKA_RETRY_SHORTINTERVAL:orderer node may fail to connect to kafka, This value is the retry interval.</p><p>broker-0.broker: kafka service name</p><p>9092:kafka service port</p><p>orderer metrics will be dump on the below port.</p><pre>- name: ORDERER_OPERATIONS_LISTENADDRESS <em># metric endpoint<br>    </em>value: 0.0.0.0:9444<br>- name: ORDERER_METRICS_PROVIDER<br>   value: prometheus</pre><h3>Orderer Service</h3><p>Let’s create a service for orderer.</p><p>The yaml files that create it are in the following directory in the project below.</p><p>deploy/orderer/order-svc.yaml</p><p>deploy/orderer/order2-svc.yaml</p><p>deploy/orderer/order3-svc.yaml</p><p>deploy/orderer/order4-svc.yaml</p><p>deploy/orderer/order5-svc.yaml</p><pre>apiVersion: v1<br>kind: Service<br>metadata:<br>  name: orderer<br>  labels: <br>    app: orderer</pre><p>This specification creates a new Service object named “orderer”.The Service for orderer 5, named orderer5.In other orderers, this value is assigned to orderer2,orderer3,orderer4.</p><pre>ports:<br>- name: grpc<br>  protocol: TCP<br>  targetPort: 7050 <em><br>  </em>port: 7050</pre><p>targetPort: container port.7050 is assigned.</p><p>port: kubernetes service port.7050 is assigned.</p><pre>apiVersion: v1<br>kind: Service<br>metadata:<br>  name: orderer-metrics<br>  labels:<br>    app: orderer<br>    metrics-service: &quot;true&quot;</pre><p>A new service has been created object named “orderer-metrics” to retrieve orderer metric information.The metric service for orderer 5, named orderer5-metrics.</p><pre>spec:<br>  type: ClusterIP<br>  selector:<br>    app: orderer<br>  ports:<br>  - name: &quot;orderer-metrics&quot;<br>    targetPort: 9444 <em># container metric port<br>    </em>port: 9444</pre><p>The target port 9444 is the container metric portit has an open port 9444.</p><p>Service maps port 9444 of the container to the node’s external IP:Port for all containers with the labels app:orderer.</p><p>My article ends here. In general,I introduced orderer and explained the deployment of orderer on Kubernetes.</p><p>See you in the next articles.</p><h3>Deploy Orderer on Kubernetes</h3><p>Let’s connect to the kubernetes master node virtual machine with the <strong>vagrant ssh</strong> command.</p><pre>$ vagrant ssh k8smaster</pre><p>Let’s go to the directory where the kubernetes installation scripts are located.This directory is the same as the deploy/k8s folder in the project. With Vagrant, this directory is synchronized to the virtual machine.</p><pre>$ <em>cd</em> /vagrant/k8s</pre><p>Deploying the deployments,services for orderer.</p><pre>$ <em>kubectl </em>apply -f orderer/</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*M8DUxLJAZ4IeN-KWy1T2nA.png\" /></figure><p>Orderer creation pending completion.</p><pre>$ kubectl wait --for condition=available --timeout=300s deployment -l &quot;app in (orderer,orderer2,orderer3,orderer4,orderer5)&quot;</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*OpDvUmXh9jGdKJjJNl_FlA.png\" /></figure><p>Orderer created successfully.</p><p>Finally, let’s check the conditions of the pods we run from the lens ide.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*CfG5t_Z4l7qE6WRxnhlY2g.png\" /></figure><p>Finally, let’s check the conditions of the pods we run from the lens ide.</p><p>The state of orderer pods appear to be running.</p><p>My article ends here. In general,I introduced Orderer and explained the deployment of these tools on Kubernetes.</p><p>See you in the next articles.</p><h3>Project Links</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*_SkaFgOe_jFbbYOo.png\" /></figure><p>Spring Boot Hlf Starter Project details and installation can be accessed via <a href=\"https://github.com/susimsek/spring-boot-hlf-starter\">this link</a>.</p><p>Asset Transfer Project details and installation can be accessed via <a href=\"https://github.com/susimsek/spring-boot-hlf-starter\">this link</a></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=b82662ada0a4\" width=\"1\" height=\"1\" alt=\"\">",
      "content:encodedSnippet": "Spring Boot Fullstack Blockchain Application With Hyperledger Fabric running on Kubernetes (Part 6) — Orderer\n\nHello everyone, through this article series we will the Hyperledger Fabric integration with Spring Boot.In this article, we will look into orderer.Also,I will also explain how to deploy orderer service on Kubernetes.\nOther articles on Hyperledger Fabric integration with Spring Boot can be accessed from the links below.\nPart 1 — Introduction\nPart 2 — Kubernetes Cluster Setup\nPart 3 — Fabric CA Server\nPart 4 — Generating Certificates and Artifacts\nPart 5 — Kafka\nPart 6— Orderer\nOrderer\nThe Orderer is responsible for packaging transactions into Blocks, and distribute them to Anchor Peers across the network.\n\nThe transaction flow of Fabric have the steps Proposal, Packaging and Validation. The orderer is responsible for Packaging and involved in the Validation step for distribution of new blocks on the network.\nOrdering service provides a shared communication channel to clients and peers, offering a broadcast service for messages containing transactions. Clients connect to the channel and may broadcast messages on the channel which are then delivered to all peers. The channel supports atomic delivery of all messages, that is, message communication with total-order delivery and (implementation specific) reliability. In other words, the channel outputs the same messages to all connected peers and outputs them to all peers in the same logical order.\nOrdering service is not capable of transaction validations, it’s primary goal to provide total order for transactions published, cut blocks with ordered transactions.\nOrdering Service Implementations\nWhile every ordering service currently available handles transactions and configuration updates the same way, there are nevertheless several different implementations for achieving consensus on the strict ordering of transactions between ordering service nodes\nRaft\nNew as of v1.4.1, Raft is a crash fault tolerant (CFT) ordering service based on an implementation of Raft protocol in etcd\n\nRaft follows a “leader and follower” model, where a leader node is elected (per channel) and its decisions are replicated by the followers.\nRaft ordering services should be easier to set up and manage than Kafka-based ordering services, and their design allows different organizations to contribute nodes to a distributed ordering service.\nKafka\nSimilar to Raft-based ordering, Apache Kafka is a CFT implementation that uses a “leader and follower” node configuration.\n\nKafka utilizes a ZooKeeper ensemble for management purposes.Asset transfer projesinde kafka kullanıyoruz.We will use kafka.\nSolo (deprecated in v2.x)\nThe Solo implementation of the ordering service is intended for test only and consists only of a single ordering node.It has been deprecated and may be removed entirely in a future release.\n\nExisting users of Solo should move to a single node Raft network for equivalent function.\nInstallation of Orderer on Kubernetes\nFor the asset transfer project, we will set up kafka and zookeeper to run 5 pods on kubernetes.\n\nLet’s open the project we downloaded from this link and go to the directory where the k8s is located.\n$ cd deploy/k8s\nKafka and Zookeeper Deployment\nLet’s create a deployment for each orderer.\nThe yaml files that create it are in the following directory in the project below.\ndeploy/k8s/orderer/orderer.yaml\ndeploy/k8s/orderer/orderer2.yaml\ndeploy/k8s/orderer/orderer3.yaml\ndeploy/k8s/orderer/orderer4.yaml\ndeploy/k8s/orderer/orderer5.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: orderer\n  labels: \n    app: orderer\n\nThe Deployment for orderer 1, named orderer.\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: orderer5\n  labels: \n    app: orderer5\n\nThe Deployment for orderer 5, named orderer5.In other orderers, this value is assigned to orderer2,orderer3,orderer4.\n\nspec:\n  selector:\n    matchLabels:\n      app: orderer\n  replicas: 1\n\nThe Deployment, has a Spec that indicates that 1 replicas of the orderer container will be launched in unique Pods.This value is assigned the same for each orderer.\n\nvolumes:\n  - name: fabricfiles\n    persistentVolumeClaim:\n      claimName: fabricfiles-pvc\n\nThe volumeClaimTemplates will provide stable storage using PersistentVolumes provisioned by a PersistentVolume Provisioner.It should be the same as the pvc metadata name.This value is assigned the same for each orderer.\n\nvolumeMounts:\n  - name:  fabricfiles\n    mountPath: /organizations\n    subPath: organizations # required for certificates.\n  - name: fabricfiles\n    mountPath: /system-genesis-block\n    subPath: system-genesis-block # It needs a genesis block.\n  - name: fabricfiles\n    mountPath: /var/hyperledger/production/orderer\n    subPath: state/orderer # orderer persistence data\nThe Orderer container mounts the PV at /organizations for certificates.\nThe Orderer container mounts the PV at /system-genesis-block for genesis block. It needs a genesis block.\nThe Orderer container mounts the PV at /var/hyperledger/production/orderer for orderer persistence data. This data will be kept in state/orderer sub directory on nfs server.\n- name: fabricfiles\n  mountPath: /var/hyperledger/production/orderer\n  subPath: state/orderer5\nThe persistence data of other orderers are also kept in subdirectories such as state/orderer2, state/orderer5 on the nfs server.\nlivenessProbe:\n  httpGet:\n    port: 9444\n    path: /healthz\n  initialDelaySeconds: 60\n  timeoutSeconds: 5\n  failureThreshold: 6\nreadinessProbe:\n    httpGet:\n      port: 9444\n      path: /healthz\n    initialDelaySeconds: 5\n    timeoutSeconds: 3\n    periodSeconds: 5\nwhen a container get in the ready state, kubernetes starts to route traffic to the relavent pod. But the pod in the container may not be ready to accept traffic. Therefore, we need to specify “liveness” and “readiness” probes for applications in order kubernetes to do this process more efficiently.\nKubelet will check whether the container is alive and healthy by sending requests to the /healthz path on port 9443 and expect a success result code.\nspec:\n  containers:\n  - name: orderer\n    image: hyperledger/fabric-orderer:2.3\n    imagePullPolicy: IfNotPresent\n2.3 is assigned as Hyperledger fabric orderer docker image version.\nSet imagePullPolicy to IfNotPresent or Never and pre-pull: Pull manually images on each cluster node so the latest is cached, then do a kubectl rolling-update or similar to restart Pods.\nThe following environment variables are assigned for orderer.\nenv:\n  - name: CONFIGTX_ORDERER_ADDRESSES\n    value: \"orderer:7050\"\n  - name: ORDERER_GENERAL_LISTENADDRESS\n    value: \"0.0.0.0\"\n  - name: ORDERER_GENERAL_LISTENPORT\n    value: \"7050\"\n  - name: ORDERER_GENERAL_LOGLEVEL\n    value: debug\n  - name: ORDERER_GENERAL_LOCALMSPDIR\n    value: /organizations/ordererOrganizations/example.com/orderers/orderer.example.com/msp\n  - name: ORDERER_GENERAL_LOCALMSPID\n    value: OrdererMSP\n  - name: ORDERER_GENERAL_GENESISMETHOD\n    value: file\n  - name: ORDERER_GENERAL_GENESISFILE\n    value: /system-genesis-block/genesis.block\n  - name: ORDERER_GENERAL_TLS_ENABLED\n    value: \"true\"\n  - name: ORDERER_GENERAL_TLS_PRIVATEKEY\n    value: /organizations/ordererOrganizations/example.com/orderers/orderer.example.com/tls/server.key\n  - name: ORDERER_GENERAL_TLS_CERTIFICATE\n    value: /organizations/ordererOrganizations/example.com/orderers/orderer.example.com/tls/server.crt\n  - name: ORDERER_GENERAL_TLS_ROOTCAS\n    value: /organizations/ordererOrganizations/example.com/orderers/orderer.example.com/tls/ca.crt\n  - name:  ORDERER_GENERAL_CLUSTER_CLIENTPRIVATEKEY\n    value: /organizations/ordererOrganizations/example.com/orderers/orderer.example.com/tls/server.key\n  - name:  ORDERER_GENERAL_CLUSTER_CLIENTCERTIFICATE\n    value: /organizations/ordererOrganizations/example.com/orderers/orderer.example.com/tls/server.crt\n  - name: ORDERER_OPERATIONS_LISTENADDRESS # metric endpoint\n    value: 0.0.0.0:9444\n  - name: ORDERER_METRICS_PROVIDER\n    value: prometheus\n  - name: CONFIGTX_ORDERER_ORDERERTYPE\n    value: kafka\n  - name: CONFIGTX_ORDERER_KAFKA_BROKERS\n    value: \"broker-0.broker:9092,broker-1.broker:9092\"\n  - name: ORDERER_KAFKA_RETRY_SHORTINTERVAL\n    value: 1s\n  - name: ORDERER_KAFKA_RETRY_SHORTTOTAL\n    value: 30s\n  - name: ORDERER_KAFKA_VERBOSE\n    value: \"true\"\nORDERER_GENERAL_GENESISFILE: path to genesis file path\nORDERER_GENERAL_LOCALMSPID: ID to load the MSP definition\nORDERER_GENERAL_LOCALMSPDIR: MSPDir is the filesystem path which contains the MSP configuration\nORDERER_GENERAL_TLS_ENABLED: enable TLS with client authentication.\nORDERER_GENERAL_TLS_PRIVATEKEY: fully qualified path of the file that contains the server private key\nORDERER_GENERAL_TLS_CERTIFICATE: fully qualified path of the file that contains the server certificate\nORDERER_GENERAL_TLS_ROOTCAS : fully qualified path of the file that contains the certificate chain of the CA that issued TLS server certificate\nORDERER_GENERAL_GENESISMETHOD: file is used when you want provide the genesis block as file to the container\nORDERER_GENERAL_TLS_CLIENTROOTCAS: fully qualified path of the file that contains the certificate chain of the CA that issued TLS server certificate\nCONFIGTX_ORDERER_ORDERERTYPE: The orderer implementation to start.Available types are solo,kafka and etcdraft.\nORDERER_KAFKA_RETRY_SHORTINTERVAL: The order node may fail to connect kafka_ kafka_ RETRY_ Shortentreval is the interval between retries.\nORDERER_KAFKA_RETRY_SHORTTOTAL: Total number of retries.\nCONFIGTX_ORDERER_KAFKA_BROKERS:Instructs Orderer how to get in touch with Kafka.\nORDERER_GENERAL_LISTENPORT: This value is the port that the orderer listens to.\nORDERER_KAFKA_RETRY_SHORTINTERVAL:orderer node may fail to connect to kafka, This value is the retry interval.\nbroker-0.broker: kafka service name\n9092:kafka service port\norderer metrics will be dump on the below port.\n- name: ORDERER_OPERATIONS_LISTENADDRESS # metric endpoint\n    value: 0.0.0.0:9444\n- name: ORDERER_METRICS_PROVIDER\n   value: prometheus\nOrderer Service\nLet’s create a service for orderer.\nThe yaml files that create it are in the following directory in the project below.\ndeploy/orderer/order-svc.yaml\ndeploy/orderer/order2-svc.yaml\ndeploy/orderer/order3-svc.yaml\ndeploy/orderer/order4-svc.yaml\ndeploy/orderer/order5-svc.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: orderer\n  labels: \n    app: orderer\nThis specification creates a new Service object named “orderer”.The Service for orderer 5, named orderer5.In other orderers, this value is assigned to orderer2,orderer3,orderer4.\nports:\n- name: grpc\n  protocol: TCP\n  targetPort: 7050 \n  port: 7050\ntargetPort: container port.7050 is assigned.\nport: kubernetes service port.7050 is assigned.\napiVersion: v1\nkind: Service\nmetadata:\n  name: orderer-metrics\n  labels:\n    app: orderer\n    metrics-service: \"true\"\nA new service has been created object named “orderer-metrics” to retrieve orderer metric information.The metric service for orderer 5, named orderer5-metrics.\nspec:\n  type: ClusterIP\n  selector:\n    app: orderer\n  ports:\n  - name: \"orderer-metrics\"\n    targetPort: 9444 # container metric port\n    port: 9444\nThe target port 9444 is the container metric portit has an open port 9444.\nService maps port 9444 of the container to the node’s external IP:Port for all containers with the labels app:orderer.\nMy article ends here. In general,I introduced orderer and explained the deployment of orderer on Kubernetes.\nSee you in the next articles.\nDeploy Orderer on Kubernetes\nLet’s connect to the kubernetes master node virtual machine with the vagrant ssh command.\n$ vagrant ssh k8smaster\nLet’s go to the directory where the kubernetes installation scripts are located.This directory is the same as the deploy/k8s folder in the project. With Vagrant, this directory is synchronized to the virtual machine.\n$ cd /vagrant/k8s\nDeploying the deployments,services for orderer.\n$ kubectl apply -f orderer/\n\nOrderer creation pending completion.\n$ kubectl wait --for condition=available --timeout=300s deployment -l \"app in (orderer,orderer2,orderer3,orderer4,orderer5)\"\n\nOrderer created successfully.\nFinally, let’s check the conditions of the pods we run from the lens ide.\n\nFinally, let’s check the conditions of the pods we run from the lens ide.\nThe state of orderer pods appear to be running.\nMy article ends here. In general,I introduced Orderer and explained the deployment of these tools on Kubernetes.\nSee you in the next articles.\nProject Links\n\nSpring Boot Hlf Starter Project details and installation can be accessed via this link.\nAsset Transfer Project details and installation can be accessed via this link",
      "dc:creator": "Şuayb Şimşek",
      "guid": "https://medium.com/p/b82662ada0a4",
      "categories": ["orderer", "blockchain", "kubernetes", "spring-boot", "hyperledger-fabric"],
      "isoDate": "2022-01-10T20:01:06.000Z"
    },
    {
      "creator": "Şuayb Şimşek",
      "title": "Spring Boot Fullstack Blockchain Application With Hyperledger Fabric running on Kubernetes (Part 5)",
      "link": "https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-5-1723502770e7?source=rss-bda589f2335a------2",
      "pubDate": "Mon, 27 Dec 2021 19:30:57 GMT",
      "content:encoded": "<h3>Spring Boot Fullstack Blockchain Application With Hyperledger Fabric running on Kubernetes (Part 5) — Kafka</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*cN6xIURs0Xub9a7A38II-g.png\" /></figure><p>Hello everyone, through this article series we will the Hyperledger Fabric integration with Spring Boot.In this article, we will look into kafka.Also,I will also explain how to deploy Kafka on Kubernetes.</p><p>Other articles on Hyperledger Fabric integration with Spring Boot can be accessed from the links below.</p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-1-c8f7c87d60eb\">Part 1 — Introduction</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-ddce8d25858d\">Part 2 — Kubernetes Cluster Setup</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-3-7eb2fc75ba60\">Part 3 — Fabric CA Server</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-4-d6583a57153c\">Part 4 — Generating Certificates and Artifacts</a></p><p><a href=\"https://suaybsimsek58.medium.com/1723502770e7\">Part 5— Kafka</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-6-b82662ada0a4\">Part 6 — Orderer</a></p><p>First of all, I will briefly explain Kafka.</p><h3>Kafka</h3><p>Apache Kafka is an open source framework for instant storage and analysis of big data, developed by LinkedIn and now part of Apache.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/410/1*Jnw877FtV8fUpyFvwQj4Xw.png\" /></figure><p>It uses the messaging system (queue) to quickly store and analyze big data.</p><p>Hyperledger Fabric ordering service nodes (OSNs) use your Kafka cluster and provide an ordering service to your blockchain network.</p><p>Let’s examine the Topic, Producer and Consumer concepts in Kafka.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/700/0*f1aFyQ6YwoWmudVZ.png\" /></figure><h3>Topic</h3><p>Topics is a user-nameable area where data (messages) are stored. Topics are divided into Partitions and the number of Partitions they are stored in can be determined by the user.</p><h3>Producer</h3><p>Producers, on the other hand, are Publisher when we look at the Publish-Subscribe structure of Apache Kafka, which can send messages to these Topics. They can send data, ie messages, into the Topics and can be linked to more than one topic at the same time.</p><h3>Consumer</h3><p>Consumers, on the other hand, are Subscribers who consume the messages sent by the Producers to the Topics, as we can understand from the name. More than one Producer can send messages to a topic (Topic), and more than one Consumer can be included in a topic and read the data sent to the Topic. After a consumer reads these messages sent by the producers, this data is not deleted from the Topic.</p><h3>Zookeeper</h3><p>Apache ZooKeeper is an open source Apache project that allows clusters to distribute information such as configuration, naming, and group services over large clusters.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/453/1*yoaG23tcn2RTH7JIYRCH_A.png\" /></figure><p>ZooKeeper uses a key-value store in a hierarchical fashion. Used for high availability environments. Apache ZooKeeper is written in Java and licensed under the Apache License 2.0. It is used by some big companies like Rackspace, Yahoo, eBay and Reddit.</p><p>Zookeeper keeps track of status of the Kafka cluster nodes and it also keeps track of Kafka topics, partitions etc.</p><h3>Installation of Kafka on Kubernetes</h3><p>For the asset transfer project, we will set up kafka and zookeeper to run 2 pods on kubernetes.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/629/1*hlkxZN68vIdm_U-1iptB5A.png\" /></figure><p>Let’s open the project we downloaded from <a href=\"https://github.com/susimsek/spring-boot-hlf-k8s-fullstack-blockchain-app\">this link</a> and go to the directory where the k8s is located.</p><pre>$ cd deploy/k8s</pre><h3>Kafka and Zookeeper Persistence Volume</h3><p>First of all, let’s create a persistence volume for kafka and zookeper. The yaml files that create it are in the following directory in the project below.</p><p>deploy/k8s/pv/kafka-pv.yaml</p><p>deploy/k8s/pv/zookeeper-pv.yaml</p><p>For Kafka;</p><pre>nfs:<br>  path: /srv/kubedata/fabricfiles/broker/kafka1<br>  server: 192.168.12.9</pre><p>It specifies the path where the permanent data of Kafka is kept on the nfs server and the ip of the nfs server.</p><p>For Zookeper</p><pre>nfs:<br>  path: /srv/kubedata/fabricfiles/broker/zookeeper0<br>  server: 192.168.12.9</pre><p>It specifies the path where the permanent data of Zookoper is kept on the nfs server and the ip of the nfs server.</p><pre>spec:<br>  storageClassName: default<br>  volumeMode: Filesystem<br>  capacity:<br>    storage: 1Gi</pre><p>Storage capacity is limited and may vary depending on the node on which a pod runs: network-attached storage might not be accessible by all nodes, or storage is local to a node to begin with.It is assigned as 1gb.</p><pre>metadata:<br>  name: kafka1-pv<br>  labels:<br>    app: kafka<br>    podindex: &quot;0&quot;</pre><p>A podindex label was assigned to attach persistence volume with the relevant persistence volume claims.podindex: “0” represents the persistent data of the first kafka.</p><pre>metadata:<br>  name: kafka1-pv<br>  labels:<br>    app: kafka<br>    podindex: &quot;1&quot;</pre><p>podindex: “1” represents the persistent data of the second kafka.</p><pre>metadata:<br>  name: zookeeper1-pv<br>  labels:<br>    app: zookeeper<br>    podindex: &quot;0&quot;</pre><p>podindex: “0” represents the persistent data of the first zookeeper.podindex: “1” represents the persistent data of the second zookeeper.</p><h3>Kafka and Zookeeper Persistence Volume Claim</h3><p>Let’s create a persistence volume claim for kafka and zookeper. The yaml files that create it are in the following directory in the project below.</p><p>deploy/k8s/pvc/kafka-pvc.yaml</p><p>deploy/k8s/pvc/zookeeper-pvc.yaml</p><pre>selector:<br>  matchLabels:<br>    app: kafka<br>    podindex: &quot;0&quot;</pre><p>Claims can specify a <a href=\"https://v1-20.docs.kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors\">label selector</a> to further filter the set of volumes. Only the volumes whose labels match the selector can be bound to the claim.kafka persistence volume must have ‘podindex: “0”’ label.</p><pre>selector:<br>  matchLabels:<br>    app: zookeeper<br>    podindex: &quot;0&quot;</pre><p>Same way,zookeeper persistence volume must have podindex: “0” label.</p><pre>resources:<br>  requests:<br>    storage: 1Gi</pre><p>Claims, like Pods, can request specific quantities of a resource. In this case, the request is for storage.It is assigned as 1gb.</p><h3>Kafka and Zookeeper StatefulSet</h3><p>Let’s create a statefulset for kafka and zookeper. StatefulSet is the workload API object used to manage stateful applications.Manages the deployment and scaling of a set of <a href=\"https://kubernetes.io/docs/concepts/workloads/pods/\">Pods</a>, <em>and provides guarantees about the ordering and uniqueness</em> of these Pods.</p><p>The yaml files that create it are in the following directory in the project below.</p><p>deploy/k8s/kafka/kafka.yaml</p><p>deploy/k8s/kafka/zookeeper.yaml</p><pre>apiVersion: apps/v1<br>kind: StatefulSet<br>metadata:<br>  name: broker<br>  labels:<br>    app: kafka</pre><ul><li>The StatefulSet, named broker.</li></ul><pre>apiVersion: apps/v1<br>kind: StatefulSet<br>metadata:<br>  name: zoo<br>  labels:<br>    app: zookeeper</pre><ul><li>The StatefulSet, named zoo.</li></ul><pre>spec:<br>  selector:<br>    matchLabels:<br>      app: kafka<br>  serviceName: broker<br>  replicas: 2</pre><ul><li>The StatefulSet, has a Spec that indicates that 2 replicas of the kafka container will be launched in unique Pods.</li></ul><pre>selector:<br>  matchLabels:<br>    app: zookeeper<br>serviceName: zoo<br>replicas: 2</pre><p>Same way,The StatefulSet, has a Spec that indicates that 2 replicas of the zookeeper container will be launched in unique Pods.</p><pre>volumeClaimTemplates:<br>- metadata:<br>    name: kafka</pre><ul><li>The volumeClaimTemplates will provide stable storage using <a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/\">PersistentVolumes</a> provisioned by a PersistentVolume Provisioner.It should be the same as the pvc metadata name.</li></ul><pre>volumeClaimTemplates:<br>- metadata:<br>    name: zookeeper</pre><p>Same way,it should be the same as the pvc metadata name.</p><p>The following environment variables are assigned for Kafka.</p><pre>env:<br>  - name: KAFKA_MESSAGE_MAX_BYTES<br>    value: &quot;102760448&quot;<br>  - name: KAFKA_REPLICA_FETCH_MAX_BYTES<br>    value: &quot;102760448&quot;<br>  - name: KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE<br>    value: &quot;false&quot;<br>  - name: KAFKA_DEFAULT_REPLICATION_FACTOR<br>    value: &quot;2&quot;<br>  - name: KAFKA_MIN_INSYNC_REPLICAS<br>    value: &quot;2&quot;<br>  - name: KAFKA_ZOOKEEPER_CONNECT<br>    value: zoo-0.zoo:2181,zoo-1.zoo:2181<br>  - name: KAFKA_PORT<br>    value: &quot;9092&quot;<br>  - name: GODEBUG<br>    value: netdns=go   <br>  - name: KAFKA_ZOOKEEPER_CONNECTION_TIMEOUT_MS<br>    value: &quot;30000&quot;<br>  - name: KAFKA_LOG_DIRS<br>    value: /opt/kafka/data</pre><p>KAFKA_MESSAGE_MAX_BYTES: Maximum transmit message size.</p><p>KAFKA_REPLICA_FETCH_MAX_BYTES:Initial maximum number of bytes per topic+partition to request when fetching messages from the broker.</p><p>KAFKA_MIN_INSYNC_REPLICAS:is used when there is a problem in the topic, maybe one of the partitions is not in-sync, or offline. When this is the case the cluster will send an ack when KAFKA_MIN_INSYNC_REPLICAS is satisfied. So 2 replicas, with KAFKA_MIN_INSYNC_REPLICAS=2 will still be able to write.</p><p>KAFKA_ZOOKEEPER_CONNECT:Instructs Kafka how to get in touch with ZooKeeper.</p><p>zoo: zookeeper service name</p><p>2181:zookeeper service port</p><p>KAFKA_ZOOKEEPER_CONNECTION_TIMEOUT_MS:The max time that the client waits while establishing a connection to zookeeper</p><p>KAFKA_DEFAULT_REPLICATION_FACTOR:The replication factor for the tier metadata topic (set higher to ensure availability).</p><p>The following environment variables are assigned for Zookeeper.</p><pre>env:<br>  - name: ZOO_SERVERS<br>    value: server.0=zoo-0.zoo.default.svc.cluster.local:2888:3888 server.1=zoo-1.zoo.default.svc.cluster.local:2888:3888<br>  - name: ZOO_4LW_COMMANDS_WHITELIST<br>    value: srvr, mntr, ruok<br>  - name: ZOO_MAX_SESSION_TIMEOUT<br>    value: &quot;40000&quot;<br>  - name: ZOO_TICK_TIME<br>    value: &quot;2000&quot;</pre><p>ZOO_SERVERS: This variable allows you to specify a list of machines of the Zookeeper ensemble. Each entry should be specified as such: server.id=&lt;address1&gt;:&lt;port1&gt;:&lt;port2&gt;</p><p>ZOO_4LW_COMMANDS_WHITELIST:A list of comma separated Four Letter Words commands that user wants to use. A valid Four Letter Words command must be put in this list else ZooKeeper server will not enable the command.Defaults to srvr</p><p>ZOO_MAX_SESSION_TIMEOUT:Maximum session timeout in milliseconds that the server will allow the client to negotiate</p><p>ZOO_TICK_TIME:Basic time unit in milliseconds used by Apache ZooKeeper for heartbeats</p><pre>ports:<br>  - name: broker<br>    containerPort: 9092</pre><p>Kafka listens one port: 9092 is the default port used by Kafka.</p><pre>ports:<br>  - name: client<br>    containerPort: 2181<br>  - name: peer<br>    containerPort: 2888<br>  - name: leader-election<br>    containerPort: 3888</pre><p>Zookeeper listens on three ports: 2181 for client connections; 2888 for follower connections, if they are the leader; and 3888 for other server connections during the leader election phase .</p><h3>Kafka and Zookeeper Service</h3><p>Let’s create a service for kafka and zookeper.</p><p>The yaml files that create it are in the following directory in the project below.</p><p>deploy/k8s/kafka/kafka-svc.yaml</p><p>deploy/k8s/kafka/zookeeper-svc.yaml</p><pre>apiVersion: v1<br>kind: Service<br>metadata:<br>  name: kafka<br>  labels:<br>    app: kafka</pre><p>This specification creates a new Service object named “kafka”.</p><pre>apiVersion: v1<br>kind: Service<br>metadata:<br>  name: broker</pre><p>This specification creates a new headless Service object named “broker”.A headless service is a service with a service IP but instead of load-balancing it will return the IPs of our associated Pods. This allows us to interact directly with the Pods instead of a proxy.Required for configuring the Kafka in cluster mode.</p><pre>ports:<br>- name: &quot;broker&quot;<br>  targetPort: 9092<em><br>  </em>port: 9092</pre><p>targetPort: container port.The default port used by Kafka is 9092.</p><p>port: kubernetes service port.</p><pre>apiVersion: v1<br>kind: Service<br>metadata:<br>  name: zookeeper<br>  labels:<br>    app: zookeeper</pre><p>This specification creates a new Service object named “zookeeper”.</p><pre>apiVersion: v1<br>kind: Service<br>metadata:<br>  name: zoo<br>spec:<br>  type: ClusterIP<br>  clusterIP: None<br>  selector:<br>    app: zookeeper</pre><p>This specification creates a new headless Service object named “zoo”.Required for configuring the Zookeeper in cluster mode.</p><pre>ports:<br>- name: &quot;peer&quot;<br>  targetPort: 2888 <em><br>  </em>port: 2888<br>- name: &quot;leader-election&quot;<br>  targetPort: 3888 <em><br>  </em>port: 3888</pre><p>The zookeeper’s peer and leader election default ports are exposed via headless service.</p><pre>ports:<br>  - name: client<br>    protocol: TCP<br>    targetPort: 2181<br>    port: 2181</pre><p>The zookeeper’s client default port are exposed via zookeeper service.</p><h3>Deploy Kafka and Zookeeper on Kubernetes</h3><p>Let’s connect to the kubernetes master node virtual machine with the <strong>vagrant ssh</strong> command.</p><pre>$ vagrant ssh k8smaster</pre><p>Let’s go to the directory where the kubernetes installation scripts are located.This directory is the same as the deploy/k8s folder in the project. With Vagrant, this directory is synchronized to the virtual machine.</p><pre>$ <em>cd</em> /vagrant/k8s</pre><p>Deploying the persistence volume for zookeeper and kafka.</p><pre>$ kubectl apply -f pv/kafka-pv.yaml</pre><pre>$ kubectl apply -f pv/zookeeper-pv.yaml</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*xHAW5fDsvHsG55DDktp9gA.png\" /></figure><p>After the pv is created, let’s create the pvc.</p><p>Deploying the persistence volume claim for zookeeper and kafka.</p><pre>$ kubectl apply -f pvc/kafka-pvc.yaml</pre><pre>$ kubectl apply -f pvc/zookeeper-pvc.yaml</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*KWsQuG3-CrFZWJ2WBenD2w.png\" /></figure><p>Deploying the statefulset for zookeeper and kafka.</p><pre>$ <em>kubectl </em>apply -f kafka/</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*2bRMmiS3KuWcxpq2luty4A.png\" /></figure><p>Kafka and zookeeper creation pending completion.</p><pre>$ kubectl wait --for condition=ready --timeout=300s pod -l &quot;app in (zookeeper,kafka)&quot;</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*doF0DyLwE320Aofr0d1T6g.png\" /></figure><p>Kafka and zookeeper created successfully.</p><p>Finally, let’s check the conditions of the pods we run from the lens ide.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*6ezjpOsVvrcGxFiZ8kjnZw.png\" /></figure><p>The state of Kafka and zookeeper pods appear to be running.</p><p>My article ends here. In general,I introduced Kafka and Zookeeper and explained the deployment of these tools on Kubernetes.</p><p>See you in the next articles.</p><h3>Project Links</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*_SkaFgOe_jFbbYOo.png\" /></figure><p>Spring Boot Hlf Starter Project details and installation can be accessed via <a href=\"https://github.com/susimsek/spring-boot-hlf-starter\">this link</a>.</p><p>Asset Transfer Project details and installation can be accessed via <a href=\"https://github.com/susimsek/spring-boot-hlf-starter\">this link</a></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=1723502770e7\" width=\"1\" height=\"1\" alt=\"\">",
      "content:encodedSnippet": "Spring Boot Fullstack Blockchain Application With Hyperledger Fabric running on Kubernetes (Part 5) — Kafka\n\nHello everyone, through this article series we will the Hyperledger Fabric integration with Spring Boot.In this article, we will look into kafka.Also,I will also explain how to deploy Kafka on Kubernetes.\nOther articles on Hyperledger Fabric integration with Spring Boot can be accessed from the links below.\nPart 1 — Introduction\nPart 2 — Kubernetes Cluster Setup\nPart 3 — Fabric CA Server\nPart 4 — Generating Certificates and Artifacts\nPart 5— Kafka\nPart 6 — Orderer\nFirst of all, I will briefly explain Kafka.\nKafka\nApache Kafka is an open source framework for instant storage and analysis of big data, developed by LinkedIn and now part of Apache.\n\nIt uses the messaging system (queue) to quickly store and analyze big data.\nHyperledger Fabric ordering service nodes (OSNs) use your Kafka cluster and provide an ordering service to your blockchain network.\nLet’s examine the Topic, Producer and Consumer concepts in Kafka.\n\nTopic\nTopics is a user-nameable area where data (messages) are stored. Topics are divided into Partitions and the number of Partitions they are stored in can be determined by the user.\nProducer\nProducers, on the other hand, are Publisher when we look at the Publish-Subscribe structure of Apache Kafka, which can send messages to these Topics. They can send data, ie messages, into the Topics and can be linked to more than one topic at the same time.\nConsumer\nConsumers, on the other hand, are Subscribers who consume the messages sent by the Producers to the Topics, as we can understand from the name. More than one Producer can send messages to a topic (Topic), and more than one Consumer can be included in a topic and read the data sent to the Topic. After a consumer reads these messages sent by the producers, this data is not deleted from the Topic.\nZookeeper\nApache ZooKeeper is an open source Apache project that allows clusters to distribute information such as configuration, naming, and group services over large clusters.\n\nZooKeeper uses a key-value store in a hierarchical fashion. Used for high availability environments. Apache ZooKeeper is written in Java and licensed under the Apache License 2.0. It is used by some big companies like Rackspace, Yahoo, eBay and Reddit.\nZookeeper keeps track of status of the Kafka cluster nodes and it also keeps track of Kafka topics, partitions etc.\nInstallation of Kafka on Kubernetes\nFor the asset transfer project, we will set up kafka and zookeeper to run 2 pods on kubernetes.\n\nLet’s open the project we downloaded from this link and go to the directory where the k8s is located.\n$ cd deploy/k8s\nKafka and Zookeeper Persistence Volume\nFirst of all, let’s create a persistence volume for kafka and zookeper. The yaml files that create it are in the following directory in the project below.\ndeploy/k8s/pv/kafka-pv.yaml\ndeploy/k8s/pv/zookeeper-pv.yaml\nFor Kafka;\nnfs:\n  path: /srv/kubedata/fabricfiles/broker/kafka1\n  server: 192.168.12.9\nIt specifies the path where the permanent data of Kafka is kept on the nfs server and the ip of the nfs server.\nFor Zookeper\nnfs:\n  path: /srv/kubedata/fabricfiles/broker/zookeeper0\n  server: 192.168.12.9\nIt specifies the path where the permanent data of Zookoper is kept on the nfs server and the ip of the nfs server.\nspec:\n  storageClassName: default\n  volumeMode: Filesystem\n  capacity:\n    storage: 1Gi\nStorage capacity is limited and may vary depending on the node on which a pod runs: network-attached storage might not be accessible by all nodes, or storage is local to a node to begin with.It is assigned as 1gb.\nmetadata:\n  name: kafka1-pv\n  labels:\n    app: kafka\n    podindex: \"0\"\nA podindex label was assigned to attach persistence volume with the relevant persistence volume claims.podindex: “0” represents the persistent data of the first kafka.\nmetadata:\n  name: kafka1-pv\n  labels:\n    app: kafka\n    podindex: \"1\"\npodindex: “1” represents the persistent data of the second kafka.\nmetadata:\n  name: zookeeper1-pv\n  labels:\n    app: zookeeper\n    podindex: \"0\"\npodindex: “0” represents the persistent data of the first zookeeper.podindex: “1” represents the persistent data of the second zookeeper.\nKafka and Zookeeper Persistence Volume Claim\nLet’s create a persistence volume claim for kafka and zookeper. The yaml files that create it are in the following directory in the project below.\ndeploy/k8s/pvc/kafka-pvc.yaml\ndeploy/k8s/pvc/zookeeper-pvc.yaml\nselector:\n  matchLabels:\n    app: kafka\n    podindex: \"0\"\nClaims can specify a label selector to further filter the set of volumes. Only the volumes whose labels match the selector can be bound to the claim.kafka persistence volume must have ‘podindex: “0”’ label.\nselector:\n  matchLabels:\n    app: zookeeper\n    podindex: \"0\"\nSame way,zookeeper persistence volume must have podindex: “0” label.\nresources:\n  requests:\n    storage: 1Gi\nClaims, like Pods, can request specific quantities of a resource. In this case, the request is for storage.It is assigned as 1gb.\nKafka and Zookeeper StatefulSet\nLet’s create a statefulset for kafka and zookeper. StatefulSet is the workload API object used to manage stateful applications.Manages the deployment and scaling of a set of Pods, and provides guarantees about the ordering and uniqueness of these Pods.\nThe yaml files that create it are in the following directory in the project below.\ndeploy/k8s/kafka/kafka.yaml\ndeploy/k8s/kafka/zookeeper.yaml\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: broker\n  labels:\n    app: kafka\n\nThe StatefulSet, named broker.\n\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: zoo\n  labels:\n    app: zookeeper\n\nThe StatefulSet, named zoo.\n\nspec:\n  selector:\n    matchLabels:\n      app: kafka\n  serviceName: broker\n  replicas: 2\n\nThe StatefulSet, has a Spec that indicates that 2 replicas of the kafka container will be launched in unique Pods.\n\nselector:\n  matchLabels:\n    app: zookeeper\nserviceName: zoo\nreplicas: 2\nSame way,The StatefulSet, has a Spec that indicates that 2 replicas of the zookeeper container will be launched in unique Pods.\nvolumeClaimTemplates:\n- metadata:\n    name: kafka\n\nThe volumeClaimTemplates will provide stable storage using PersistentVolumes provisioned by a PersistentVolume Provisioner.It should be the same as the pvc metadata name.\n\nvolumeClaimTemplates:\n- metadata:\n    name: zookeeper\nSame way,it should be the same as the pvc metadata name.\nThe following environment variables are assigned for Kafka.\nenv:\n  - name: KAFKA_MESSAGE_MAX_BYTES\n    value: \"102760448\"\n  - name: KAFKA_REPLICA_FETCH_MAX_BYTES\n    value: \"102760448\"\n  - name: KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE\n    value: \"false\"\n  - name: KAFKA_DEFAULT_REPLICATION_FACTOR\n    value: \"2\"\n  - name: KAFKA_MIN_INSYNC_REPLICAS\n    value: \"2\"\n  - name: KAFKA_ZOOKEEPER_CONNECT\n    value: zoo-0.zoo:2181,zoo-1.zoo:2181\n  - name: KAFKA_PORT\n    value: \"9092\"\n  - name: GODEBUG\n    value: netdns=go   \n  - name: KAFKA_ZOOKEEPER_CONNECTION_TIMEOUT_MS\n    value: \"30000\"\n  - name: KAFKA_LOG_DIRS\n    value: /opt/kafka/data\nKAFKA_MESSAGE_MAX_BYTES: Maximum transmit message size.\nKAFKA_REPLICA_FETCH_MAX_BYTES:Initial maximum number of bytes per topic+partition to request when fetching messages from the broker.\nKAFKA_MIN_INSYNC_REPLICAS:is used when there is a problem in the topic, maybe one of the partitions is not in-sync, or offline. When this is the case the cluster will send an ack when KAFKA_MIN_INSYNC_REPLICAS is satisfied. So 2 replicas, with KAFKA_MIN_INSYNC_REPLICAS=2 will still be able to write.\nKAFKA_ZOOKEEPER_CONNECT:Instructs Kafka how to get in touch with ZooKeeper.\nzoo: zookeeper service name\n2181:zookeeper service port\nKAFKA_ZOOKEEPER_CONNECTION_TIMEOUT_MS:The max time that the client waits while establishing a connection to zookeeper\nKAFKA_DEFAULT_REPLICATION_FACTOR:The replication factor for the tier metadata topic (set higher to ensure availability).\nThe following environment variables are assigned for Zookeeper.\nenv:\n  - name: ZOO_SERVERS\n    value: server.0=zoo-0.zoo.default.svc.cluster.local:2888:3888 server.1=zoo-1.zoo.default.svc.cluster.local:2888:3888\n  - name: ZOO_4LW_COMMANDS_WHITELIST\n    value: srvr, mntr, ruok\n  - name: ZOO_MAX_SESSION_TIMEOUT\n    value: \"40000\"\n  - name: ZOO_TICK_TIME\n    value: \"2000\"\nZOO_SERVERS: This variable allows you to specify a list of machines of the Zookeeper ensemble. Each entry should be specified as such: server.id=<address1>:<port1>:<port2>\nZOO_4LW_COMMANDS_WHITELIST:A list of comma separated Four Letter Words commands that user wants to use. A valid Four Letter Words command must be put in this list else ZooKeeper server will not enable the command.Defaults to srvr\nZOO_MAX_SESSION_TIMEOUT:Maximum session timeout in milliseconds that the server will allow the client to negotiate\nZOO_TICK_TIME:Basic time unit in milliseconds used by Apache ZooKeeper for heartbeats\nports:\n  - name: broker\n    containerPort: 9092\nKafka listens one port: 9092 is the default port used by Kafka.\nports:\n  - name: client\n    containerPort: 2181\n  - name: peer\n    containerPort: 2888\n  - name: leader-election\n    containerPort: 3888\nZookeeper listens on three ports: 2181 for client connections; 2888 for follower connections, if they are the leader; and 3888 for other server connections during the leader election phase .\nKafka and Zookeeper Service\nLet’s create a service for kafka and zookeper.\nThe yaml files that create it are in the following directory in the project below.\ndeploy/k8s/kafka/kafka-svc.yaml\ndeploy/k8s/kafka/zookeeper-svc.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: kafka\n  labels:\n    app: kafka\nThis specification creates a new Service object named “kafka”.\napiVersion: v1\nkind: Service\nmetadata:\n  name: broker\nThis specification creates a new headless Service object named “broker”.A headless service is a service with a service IP but instead of load-balancing it will return the IPs of our associated Pods. This allows us to interact directly with the Pods instead of a proxy.Required for configuring the Kafka in cluster mode.\nports:\n- name: \"broker\"\n  targetPort: 9092\n  port: 9092\ntargetPort: container port.The default port used by Kafka is 9092.\nport: kubernetes service port.\napiVersion: v1\nkind: Service\nmetadata:\n  name: zookeeper\n  labels:\n    app: zookeeper\nThis specification creates a new Service object named “zookeeper”.\napiVersion: v1\nkind: Service\nmetadata:\n  name: zoo\nspec:\n  type: ClusterIP\n  clusterIP: None\n  selector:\n    app: zookeeper\nThis specification creates a new headless Service object named “zoo”.Required for configuring the Zookeeper in cluster mode.\nports:\n- name: \"peer\"\n  targetPort: 2888 \n  port: 2888\n- name: \"leader-election\"\n  targetPort: 3888 \n  port: 3888\nThe zookeeper’s peer and leader election default ports are exposed via headless service.\nports:\n  - name: client\n    protocol: TCP\n    targetPort: 2181\n    port: 2181\nThe zookeeper’s client default port are exposed via zookeeper service.\nDeploy Kafka and Zookeeper on Kubernetes\nLet’s connect to the kubernetes master node virtual machine with the vagrant ssh command.\n$ vagrant ssh k8smaster\nLet’s go to the directory where the kubernetes installation scripts are located.This directory is the same as the deploy/k8s folder in the project. With Vagrant, this directory is synchronized to the virtual machine.\n$ cd /vagrant/k8s\nDeploying the persistence volume for zookeeper and kafka.\n$ kubectl apply -f pv/kafka-pv.yaml\n$ kubectl apply -f pv/zookeeper-pv.yaml\n\nAfter the pv is created, let’s create the pvc.\nDeploying the persistence volume claim for zookeeper and kafka.\n$ kubectl apply -f pvc/kafka-pvc.yaml\n$ kubectl apply -f pvc/zookeeper-pvc.yaml\n\nDeploying the statefulset for zookeeper and kafka.\n$ kubectl apply -f kafka/\n\nKafka and zookeeper creation pending completion.\n$ kubectl wait --for condition=ready --timeout=300s pod -l \"app in (zookeeper,kafka)\"\n\nKafka and zookeeper created successfully.\nFinally, let’s check the conditions of the pods we run from the lens ide.\n\nThe state of Kafka and zookeeper pods appear to be running.\nMy article ends here. In general,I introduced Kafka and Zookeeper and explained the deployment of these tools on Kubernetes.\nSee you in the next articles.\nProject Links\n\nSpring Boot Hlf Starter Project details and installation can be accessed via this link.\nAsset Transfer Project details and installation can be accessed via this link",
      "dc:creator": "Şuayb Şimşek",
      "guid": "https://medium.com/p/1723502770e7",
      "categories": ["hyperledger-fabric", "kubernetes", "blockchain", "spring-boot", "kafka"],
      "isoDate": "2021-12-27T19:30:57.000Z"
    },
    {
      "creator": "Şuayb Şimşek",
      "title": "Reducing Spring Native and Golang Docker Image Size by 70% with Upx",
      "link": "https://suaybsimsek58.medium.com/reducing-spring-native-and-golang-docker-image-size-by-70-with-upx-daf84e4f9227?source=rss-bda589f2335a------2",
      "pubDate": "Mon, 20 Dec 2021 18:03:04 GMT",
      "content:encoded": "<h3>Reducing Docker Image Size by 70% in Spring Native and Golang Projects with Upx</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*ssImHh3iXkYz014MF6ukrw.png\" /></figure><p>Hi, In this article, I will explain how we can reduce the docker image size by 70% using upx in spring native and golang projects.</p><h3>What is UPX(Ultimate Executable Packer)</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/200/0*BOTRiDRcC6oCosKm.png\" /></figure><p>UPX is an advanced file compressor that compresses executable files. UPX typically reduces the file size of programs and DLLs by about 50–70%, reducing programs’ disk space, network load times, download times, and other distribution and storage costs.</p><p>When spring native or golang projects are compiled, since they produce executable programs directly, we can reduce the size of these programs by 50–70% with upx.</p><h3>Reducing Docker Image Size with UPX in Golang Projects</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/964/1*ezPXDsdB-rSHM5IP98NB8Q.png\" /></figure><p>The docker image size of the web service I developed using the echo framework written in Golang was 40 MB. I reduced the image size to 5.05 MB by making some adjustments to the dockerfile.</p><p>The Dockerfile is as follows.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/e2ebffced551beb90031272f4d96ba94/href\">https://medium.com/media/e2ebffced551beb90031272f4d96ba94/href</a></iframe><p><strong>ARG upx_version=3.96 </strong>: Upx version to be installed. Currently the most updated version.</p><p><strong>CGO_ENABLED=0 : </strong>you have a statically linked binary file. so it works without any external dependencies. In the scratch docker image, it makes our application run.</p><p><strong>GOOS=linux</strong>: the compiler compiles the application to the linux operating system.</p><p><strong>GOARCH=amd64:</strong> it tells the compiler that it must conform to the amd64 processor architecture.</p><p><strong>ldflags=”-s -w” </strong>: removes debug information from the binary file.</p><p><strong>RUN upx — ultra-brute -qq server &amp;&amp; \\upx -t server</strong> : we compressed the executable using upx.</p><p><strong>FROM scratch</strong>: zero size out of the box docker image. This means that the resulting image size is equal to the binary size.</p><p><strong>COPY — from=builder /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/ca-certificates.crt </strong>: adds root certificates to struct docker images.</p><p><strong>CMD [“./server”]</strong> : runs the application.</p><p>Let’s create docker images of the application.</p><pre>$ docker build -t app:1.0.0 .</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*r-bH6u4EiAWtSvYbA0vO6w.png\" /></figure><p>The docker image size of the application is 5.05 mb</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*LhmwhueJXVvwrd4wmfFTlw.png\" /></figure><h3>Reducing Docker Image Size with UPX in Spring Native Projects</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/964/1*zw3ZV4Pa--Ae9pQGsOG9Uw.png\" /></figure><p>We can compile Spring Boot application as native with Spring native.When I compiled the application as native, the docker image size of the web service was 169 MB. I made some adjustments in the spring-boot-maven-plugin and reduced the image size to 53.4 MB.</p><p>The pom.xml is as follows.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/32c7eb2c8d61f0ee337ae09be1553a23/href\">https://medium.com/media/32c7eb2c8d61f0ee337ae09be1553a23/href</a></iframe><p><strong>&lt;java.version&gt;17&lt;/java.version&gt;</strong> : the application was compiled according to Java 17</p><p><strong>&lt;spring-native.version&gt;0.11.0-RC1&lt;/spring-native.version&gt;</strong> : the current most up-to-date spring native version is compatible with spring boot 2.6.1.</p><p><strong>&lt;name&gt;docker.io/suayb/native-app:1.0.0&lt;/name&gt; : t</strong>he tag of the docker image to be created.</p><p><strong>&lt;BP_NATIVE_IMAGE&gt;true&lt;/BP_NATIVE_IMAGE&gt;</strong> :the native image build pack is enabled. In Spring Native 0.11, the Liberica Native Image Kit (NIK) is the native image compiler distribution used by default with Buildpacks.</p><p><strong>&lt;BP_BINARY_COMPRESSION_METHOD&gt;upx&lt;/BP_BINARY_COMPRESSION_METHOD&gt;</strong>: it creates a compressed executable using upx. Required to be able to reduce the docker image size.</p><p><strong>— enable-https — enable-http</strong> : http and https are enabled as url protocols.</p><p><strong>&lt;publish&gt;false&lt;/publish&gt;</strong> : does not publish the created image in the docker registry.</p><p><strong>&lt;pullPolicy&gt;IF_NOT_PRESENT&lt;/pullPolicy&gt;</strong> : if the builder docker images do not exist locally, it pulls. This value is ALWAYS by default.</p><p>Let’s create docker images of the application.</p><pre>$ <em>mvn</em> spring-boot:build-image</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*rLSQ5-kywTOGYQIICjtxDQ.png\" /></figure><p>The docker image size of the application is 53.04 mb.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*9HVwcS-SSdc3kwSCd7yH6Q.png\" /></figure><p>My article ends here. As seen with Upx, we can reduce docker image sizes by 70%. In Docker, we prefer small size image sizes because of the disposability principle. Because large containers take a long time to stand up.</p><p>Image size doesn’t affect your program’s performance, but it can speed up many processes.</p><p>See you in the next articles.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=daf84e4f9227\" width=\"1\" height=\"1\" alt=\"\">",
      "content:encodedSnippet": "Reducing Docker Image Size by 70% in Spring Native and Golang Projects with Upx\n\nHi, In this article, I will explain how we can reduce the docker image size by 70% using upx in spring native and golang projects.\nWhat is UPX(Ultimate Executable Packer)\n\nUPX is an advanced file compressor that compresses executable files. UPX typically reduces the file size of programs and DLLs by about 50–70%, reducing programs’ disk space, network load times, download times, and other distribution and storage costs.\nWhen spring native or golang projects are compiled, since they produce executable programs directly, we can reduce the size of these programs by 50–70% with upx.\nReducing Docker Image Size with UPX in Golang Projects\n\nThe docker image size of the web service I developed using the echo framework written in Golang was 40 MB. I reduced the image size to 5.05 MB by making some adjustments to the dockerfile.\nThe Dockerfile is as follows.\nhttps://medium.com/media/e2ebffced551beb90031272f4d96ba94/href\nARG upx_version=3.96 : Upx version to be installed. Currently the most updated version.\nCGO_ENABLED=0 : you have a statically linked binary file. so it works without any external dependencies. In the scratch docker image, it makes our application run.\nGOOS=linux: the compiler compiles the application to the linux operating system.\nGOARCH=amd64: it tells the compiler that it must conform to the amd64 processor architecture.\nldflags=”-s -w” : removes debug information from the binary file.\nRUN upx — ultra-brute -qq server && \\upx -t server : we compressed the executable using upx.\nFROM scratch: zero size out of the box docker image. This means that the resulting image size is equal to the binary size.\nCOPY — from=builder /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/ca-certificates.crt : adds root certificates to struct docker images.\nCMD [“./server”] : runs the application.\nLet’s create docker images of the application.\n$ docker build -t app:1.0.0 .\n\nThe docker image size of the application is 5.05 mb\n\nReducing Docker Image Size with UPX in Spring Native Projects\n\nWe can compile Spring Boot application as native with Spring native.When I compiled the application as native, the docker image size of the web service was 169 MB. I made some adjustments in the spring-boot-maven-plugin and reduced the image size to 53.4 MB.\nThe pom.xml is as follows.\nhttps://medium.com/media/32c7eb2c8d61f0ee337ae09be1553a23/href\n<java.version>17</java.version> : the application was compiled according to Java 17\n<spring-native.version>0.11.0-RC1</spring-native.version> : the current most up-to-date spring native version is compatible with spring boot 2.6.1.\n<name>docker.io/suayb/native-app:1.0.0</name> : the tag of the docker image to be created.\n<BP_NATIVE_IMAGE>true</BP_NATIVE_IMAGE> :the native image build pack is enabled. In Spring Native 0.11, the Liberica Native Image Kit (NIK) is the native image compiler distribution used by default with Buildpacks.\n<BP_BINARY_COMPRESSION_METHOD>upx</BP_BINARY_COMPRESSION_METHOD>: it creates a compressed executable using upx. Required to be able to reduce the docker image size.\n— enable-https — enable-http : http and https are enabled as url protocols.\n<publish>false</publish> : does not publish the created image in the docker registry.\n<pullPolicy>IF_NOT_PRESENT</pullPolicy> : if the builder docker images do not exist locally, it pulls. This value is ALWAYS by default.\nLet’s create docker images of the application.\n$ mvn spring-boot:build-image\n\nThe docker image size of the application is 53.04 mb.\n\nMy article ends here. As seen with Upx, we can reduce docker image sizes by 70%. In Docker, we prefer small size image sizes because of the disposability principle. Because large containers take a long time to stand up.\nImage size doesn’t affect your program’s performance, but it can speed up many processes.\nSee you in the next articles.",
      "dc:creator": "Şuayb Şimşek",
      "guid": "https://medium.com/p/daf84e4f9227",
      "categories": ["upx", "golang", "docker", "spring-boot", "spring-native"],
      "isoDate": "2021-12-20T18:03:04.000Z"
    },
    {
      "creator": "Şuayb Şimşek",
      "title": "Spring Boot Fullstack Blockchain Application With Hyperledger Fabric running on Kubernetes (Part 4)",
      "link": "https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-4-d6583a57153c?source=rss-bda589f2335a------2",
      "pubDate": "Sat, 27 Nov 2021 13:47:38 GMT",
      "content:encoded": "<h3>Spring Boot Fullstack Blockchain Application With Hyperledger Fabric running on Kubernetes (Part 4) — Generating Certificates and Artifacts</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*RynSwoXhv_P64QKrH4q2ZQ.png\" /></figure><p>Hello everyone, through this article series we will the Hyperledger Fabric integration with Spring Boot.In this article, we will look into generating certificates,genesis block and channel transaction.Also,we will deploy job on kubernetes for this certificates and artifacts generating.</p><p>Other articles on Hyperledger Fabric integration with Spring Boot can be accessed from the links below.</p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-1-c8f7c87d60eb\">Part 1 — Introduction</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-ddce8d25858d\">Part 2 — Kubernetes Cluster Setup</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-3-7eb2fc75ba60\">Part 3 — Fabric CA Server</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-4-d6583a57153c\">Part 4— Generating Certificates and Artifacts</a></p><p><a href=\"https://suaybsimsek58.medium.com/1723502770e7\">Part 5 — Kafka</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-6-b82662ada0a4\">Part 6 — Orderer</a></p><p>First, let’s explain some concepts that are often repeated in the article.</p><h3>Anchor Peer</h3><p>Used by gossip to make sure peers in different organizations know about each other.</p><p>When a configuration block that contains an update to the anchor peers is committed, peers reach out to the anchor peers and learn from them about all of the peers known to the anchor peer(s). Once at least one peer from each organization has contacted an anchor peer, the anchor peer learns about every peer in the channel.</p><h3>ACL</h3><p>An ACL, or Access Control List, associates access to specific peer resources (such as system chaincode APIs or event services) to a <a href=\"https://hyperledger-fabric.readthedocs.io/en/release-2.2/glossary.html#policy\">Policy</a> (which specifies how many and what types of organizations or roles are required).</p><p>The ACL is part of a channel’s configuration.</p><p>A set of default ACLs is provided in the configtx.yaml file which is used by configtxgen to build channel configurations.</p><h3>Block</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/325/0*IJw0E3lqVitY8LeI.png\" /></figure><p>A block contains an ordered set of transactions.</p><p>It is cryptographically linked to the preceding block, and in turn it is linked to be subsequent blocks.</p><p>The first block in such a chain of blocks is called the <strong>genesis block</strong>.</p><p>Blocks are created by the ordering service, and then validated and committed by peers.</p><h3>Channel</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/325/0*prvgWW1lLE62irrC.png\" /></figure><p>A channel is a private blockchain overlay which allows for data isolation and confidentiality.</p><p>A channel-specific ledger is shared across the peers in the channel, and transacting parties must be authenticated to a channel in order to interact with it.</p><h3><strong>Configuration Settings</strong></h3><p>Let’s open the project we downloaded from <a href=\"https://github.com/susimsek/spring-boot-hlf-k8s-fullstack-blockchain-app\">this link</a> and go to the directory where the k8s is located.</p><pre>$ cd deploy/k8s</pre><p>We need the configtx.yaml file to create generate genesis block and channel transaction.The yaml file is in deploy/k8s/fabricfiles/configtx/configtx.yaml.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/695e23290dcec3efa57a53c40d5e80e7/href\">https://medium.com/media/695e23290dcec3efa57a53c40d5e80e7/href</a></iframe><pre>- &amp;OrdererOrg<br><br>    <em># ID to load the MSP definition as<br>    </em>ID: OrdererMSP</pre><p>The ID field is required to load the Orderer MSP definitions.Likewise, this parameter is mandatory in Org1,Org2 and Org3.</p><pre>MSPDir: ../organizations/ordererOrganizations/example.com/msp</pre><p>MSPDir is the filesystem path which contains the MSP configuration.This parameter is mandatory in Org1,Org2 and Org3.</p><pre>OrdererEndpoints:<br>    - orderer:7050</pre><p>For OrdererOrg, it is the service name and service port information of the orderer on kubernetes.</p><pre>AnchorPeers:<br>    <em># AnchorPeers defines the location of peers which can be used<br>    # for cross org gossip communication.  Note, this value is only<br>    # encoded in the genesis block in the Application section context<br>    </em>- Host: peer0-org1<br>      Port: 7051</pre><p>Anchor Peer defines the location of peers which can be used.It must be defined for each organization.it is the service name and service port definition of the peer to be used as the anchor peer for Org1 on kubernetes.The anchor peer must be defined in Org2 and Org3.</p><pre>Orderer: &amp;OrdererDefaults<br><br>    <em># Orderer Type: The orderer implementation to start<br>    </em>OrdererType: kafka</pre><p>With this configuration,Hyperledger Fabric ordering service nodes (OSNs) use your Kafka cluster and provide an ordering service to your blockchain network.</p><pre>Addresses:<br>    - orderer:7050<br>    - orderer2:7050<br>    - orderer3:7050<br>    - orderer4:7050<br>    - orderer5:7050</pre><p>The service name and service port of orderers on kubernetes are defined. 5 orderers are running on blockchain network.</p><pre>Kafka:<br>    <em># Brokers: A list of Kafka brokers to which the orderer connects<br>    # NOTE: Use IP:port notation<br>    </em>Brokers:<br>        - broker-0.broker:9092<br>        - broker-1.broker:9092</pre><p>The service name and service port of the kafka brokers on kubernetes to which the orderer will be connected are defined.We will set up Kafka cluster in this project. Kafka will run 2 instances.</p><pre>TwoOrgsOrdererGenesis:<br>    ...<br>            Organizations:<br>                - *Org1<br>                - *Org2<br>                - *Org3</pre><p>The TwoOrgsOrdererGenesis profile needs to be defined to create a genesis block.3 organizations are defined in this configuration.</p><pre>TwoOrgsChannel:<br>    ...<br>        Organizations:<br>            - *Org1<br>            - *Org2<br>            - *Org3</pre><p>The TwoOrgsChannel profile needs to be defined to create the channel transaction.3 organizations are defined in this configuration.</p><p>The generating certificate scripts defined in the project for the Orderer is as follows.The script file is in the deploy/k8s/fabricfiles/scripts/orderer-certs.sh .</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/f341500919f04e7f641fd98534490d46/href\">https://medium.com/media/f341500919f04e7f641fd98534490d46/href</a></iframe><p>.Certificate Authorities are used to generate the identities assigned to admins, nodes, and users (client applications).This script creates tls certificate, msp for each of the orderers and register and enroll identities with CA.</p><p>The generating certificate scripts defined in the project for the Org1,Org2 and Org3 are as follows.The scripts files are in the deploy/k8s/fabricfiles/scripts folder.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/dc3a5f7fcd52849a0b95a067bb9fd876/href\">https://medium.com/media/dc3a5f7fcd52849a0b95a067bb9fd876/href</a></iframe><p>This scripts create tls certificate, msp for each of the organization and register and enroll identities with CA.</p><p>The generating artifact scripts defined in the project are as follows.The scripts files are in the deploy/k8s/fabricfiles/scripts folder.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/57b25494ed8dd23a88d3376b60ed5d86/href\">https://medium.com/media/57b25494ed8dd23a88d3376b60ed5d86/href</a></iframe><pre><em>configtxgen </em>-profile TwoOrgsOrdererGenesis</pre><p>The TwoOrgsOrdererGenesis profile must be defined in the configtx.yaml file. It is required to create a Genesis block.</p><pre>-channelID system-channel</pre><p>To create a Genesis block, the channel id must be system-channel.</p><pre><em>configtxgen </em>-profile TwoOrgsChannel</pre><p>The TwoOrgsChannel profile must be defined in the configtx.yaml file. It is required to create channel transaction.</p><pre>CHANNEL_NAME:=&quot;mychannel&quot;</pre><p>Channel is a private “subnet” of communication between two or more specific network members, for the purpose of conducting private and confidential transactions.Channel id information is required to create a channel transaction.The channel id is assigned “mychannel”.</p><pre><em>for </em>orgmsp <em>in </em>Org1MSP Org2MSP Org3MSP; <em>do</em><br><em>set </em>-x<br><em>configtxgen </em>-profile TwoOrgsChannel -outputAnchorPeersUpdate ./channel-artifacts/<em>$</em>{orgmsp}anchors.tx -channelID $CHANNEL_NAME -asOrg <em>$</em>{orgmsp}<em><br>done</em></pre><p>Generate anchor peer update transaction for each organization.</p><p>Lets create an certificate creation job, create-certs.yaml .the yaml file is in the deploy/k8s/job folder.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/74694427ddbd7822ba8a6d87c17cece6/href\">https://medium.com/media/74694427ddbd7822ba8a6d87c17cece6/href</a></iframe><pre>- |<br>  ./scripts/orderer-certs.sh &amp;&amp;<br>  ./scripts/org1-certs.sh &amp;&amp;<br>  ./scripts/org2-certs.sh &amp;&amp;<br>  ./scripts/org3-certs.sh</pre><p>Scripts to run in job are defined to create certificates.</p><pre>volumeMounts:<br>  - name: fabricfiles<br>    mountPath: /organizations <em><br>    </em>subPath: organizations <em><br>  </em>- name: fabricfiles<br>    mountPath: /scripts<br>    subPath: scripts </pre><p>Required to access scripts folder and organizations folder on nfs server.Certificates are created in the organizations folder.</p><pre>volumes:<br>  - name: fabricfiles<br>    persistentVolumeClaim:<br>      claimName: fabricfiles-pvc</pre><p>Lets create an artifact creation job, create-certs.yaml .the yaml file is in the deploy/k8s/job folder.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/216e5a46deced1fb1eefa08659529323/href\">https://medium.com/media/216e5a46deced1fb1eefa08659529323/href</a></iframe><p>It allows us to mount the fabric files on the nfs server to the container.</p><pre>- |<br>  ./scripts/createGenesis.sh &amp;&amp;<br>  ./scripts/createChannel.sh</pre><p>Scripts to run in job are defined to create artifacts.</p><pre>volumeMounts:<br>  - name: fabricfiles<br>    mountPath: /organizations<br>    subPath: organizations <em><br>  </em>- name: fabricfiles<br>    mountPath: /configtx <br>    subPath: configtx <em><br>  </em>- name: fabricfiles<br>    mountPath: /system-genesis-block<br>    subPath: system-genesis-block <em><br>  </em>- name: fabricfiles<br>    mountPath: /channel-artifacts<br>    subPath: channel-artifacts <em><br>  </em>- name: fabricfiles<br>    mountPath: /scripts<br>    subPath: scripts</pre><p>Required to access scripts folder,configtx folder and organizations folder on nfs server.System Genesis Block is created in the system-genesis-block folder.Channel artifacts are created in the channel-artifacts folder.</p><h3>Installation of Jobs on Kubernetes</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*KdOJx0kdABrPqhpIs6ff6A.png\" /></figure><p>Let’s connect to the kubernetes master node virtual machine with the <strong>vagrant ssh</strong> command.</p><pre>$ vagrant ssh k8smaster</pre><p>Let’s go to the directory where the kubernetes installation scripts are located.This directory is the same as the deploy/k8s folder in the project. With Vagrant, this directory is synchronized to the virtual machine.</p><pre>$ <em>cd</em> /vagrant/k8s</pre><p>Deploying the certificate creating job for peers and orderers.</p><pre>$ kubectl apply -f job/create-certs.yaml</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*HBF19SzxO3yNL8juyCRzFg.png\" /></figure><p>Certificate creation job pending completion</p><pre>$ <em>kubectl </em>wait --for=condition=complete --timeout=300s job create-certs</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*ATKOqimZy7MD1EytVbin1g.png\" /></figure><p>After the job is completed, ordererOrganizations and peerOrganizations were created under the organizations folder on the nfs server.Let’s check if these folders are created.</p><p>Let’s open a new terminal and connect to the nfs server virtual machine.</p><pre>$ vagrant ssh nfsserver</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*TMqxU9UpwVleDXVyGWxVRw.png\" /></figure><p>Let’s list the folders under the organizations folder.</p><pre>$ ls /srv/kubedata/fabricfiles/organizations</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*vTl1bsXtsSCbE1t5BfPD9A.png\" /></figure><p>As you can see, the ordererOrganizations and peerOrganizations folders have been created.Certificate creation completed successfully.</p><p>Lets come back to kubernetes master node terminal.</p><p>Deploying the certificate artifacts job.</p><pre>$ kubectl apply -f job/create-artifacts.yaml</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*57cm7hRPa8_eUIlG-H0Z2g.png\" /></figure><p>Artifact creation job pending completion.</p><pre>$ <em>kubectl </em>wait --for=condition=complete --timeout=300s job create-artifacts</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*pX99koczynryuODs3veFDg.png\" /></figure><p>After the job is completed, system-genesis-block and channel-artifacts were created under the fabricfiles folder on the nfs server.Let’s check if these folders are created.</p><p>Lets come back to nfs server virtual machine terminal.</p><p>Let’s list the folders under the fabricfiles folder.</p><pre>$ ls /srv/kubedata/fabricfiles</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*lmNDYgzoM2z85WmCGinfwA.png\" /></figure><p>As you can see, the system-genesis-block and channel-artifacts folders have been created.Artifacts creation completed successfully.</p><p>Finally, let’s check the conditions of the jobs we run from the lens ide.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*8IgeihJwiV22mm0qB0yGwA.png\" /></figure><p>The jobs we run have been completed.</p><p>My article ends here. In general,I explained generating certificates for peers,orderer and generating genesis block and generating channel transactions on Kubernetes.</p><p>See you in the next articles.</p><h3>Project Links</h3><p>Spring Boot Hlf Starter Project details and installation can be accessed via <a href=\"https://github.com/susimsek/spring-boot-hlf-starter\">this link</a>.</p><p>Asset Transfer Project details and installation can be accessed via <a href=\"https://github.com/susimsek/spring-boot-hlf-starter\">this link</a></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=d6583a57153c\" width=\"1\" height=\"1\" alt=\"\">",
      "content:encodedSnippet": "Spring Boot Fullstack Blockchain Application With Hyperledger Fabric running on Kubernetes (Part 4) — Generating Certificates and Artifacts\n\nHello everyone, through this article series we will the Hyperledger Fabric integration with Spring Boot.In this article, we will look into generating certificates,genesis block and channel transaction.Also,we will deploy job on kubernetes for this certificates and artifacts generating.\nOther articles on Hyperledger Fabric integration with Spring Boot can be accessed from the links below.\nPart 1 — Introduction\nPart 2 — Kubernetes Cluster Setup\nPart 3 — Fabric CA Server\nPart 4— Generating Certificates and Artifacts\nPart 5 — Kafka\nPart 6 — Orderer\nFirst, let’s explain some concepts that are often repeated in the article.\nAnchor Peer\nUsed by gossip to make sure peers in different organizations know about each other.\nWhen a configuration block that contains an update to the anchor peers is committed, peers reach out to the anchor peers and learn from them about all of the peers known to the anchor peer(s). Once at least one peer from each organization has contacted an anchor peer, the anchor peer learns about every peer in the channel.\nACL\nAn ACL, or Access Control List, associates access to specific peer resources (such as system chaincode APIs or event services) to a Policy (which specifies how many and what types of organizations or roles are required).\nThe ACL is part of a channel’s configuration.\nA set of default ACLs is provided in the configtx.yaml file which is used by configtxgen to build channel configurations.\nBlock\n\nA block contains an ordered set of transactions.\nIt is cryptographically linked to the preceding block, and in turn it is linked to be subsequent blocks.\nThe first block in such a chain of blocks is called the genesis block.\nBlocks are created by the ordering service, and then validated and committed by peers.\nChannel\n\nA channel is a private blockchain overlay which allows for data isolation and confidentiality.\nA channel-specific ledger is shared across the peers in the channel, and transacting parties must be authenticated to a channel in order to interact with it.\nConfiguration Settings\nLet’s open the project we downloaded from this link and go to the directory where the k8s is located.\n$ cd deploy/k8s\nWe need the configtx.yaml file to create generate genesis block and channel transaction.The yaml file is in deploy/k8s/fabricfiles/configtx/configtx.yaml.\nhttps://medium.com/media/695e23290dcec3efa57a53c40d5e80e7/href\n- &OrdererOrg\n    # ID to load the MSP definition as\n    ID: OrdererMSP\nThe ID field is required to load the Orderer MSP definitions.Likewise, this parameter is mandatory in Org1,Org2 and Org3.\nMSPDir: ../organizations/ordererOrganizations/example.com/msp\nMSPDir is the filesystem path which contains the MSP configuration.This parameter is mandatory in Org1,Org2 and Org3.\nOrdererEndpoints:\n    - orderer:7050\nFor OrdererOrg, it is the service name and service port information of the orderer on kubernetes.\nAnchorPeers:\n    # AnchorPeers defines the location of peers which can be used\n    # for cross org gossip communication.  Note, this value is only\n    # encoded in the genesis block in the Application section context\n    - Host: peer0-org1\n      Port: 7051\nAnchor Peer defines the location of peers which can be used.It must be defined for each organization.it is the service name and service port definition of the peer to be used as the anchor peer for Org1 on kubernetes.The anchor peer must be defined in Org2 and Org3.\nOrderer: &OrdererDefaults\n    # Orderer Type: The orderer implementation to start\n    OrdererType: kafka\nWith this configuration,Hyperledger Fabric ordering service nodes (OSNs) use your Kafka cluster and provide an ordering service to your blockchain network.\nAddresses:\n    - orderer:7050\n    - orderer2:7050\n    - orderer3:7050\n    - orderer4:7050\n    - orderer5:7050\nThe service name and service port of orderers on kubernetes are defined. 5 orderers are running on blockchain network.\nKafka:\n    # Brokers: A list of Kafka brokers to which the orderer connects\n    # NOTE: Use IP:port notation\n    Brokers:\n        - broker-0.broker:9092\n        - broker-1.broker:9092\nThe service name and service port of the kafka brokers on kubernetes to which the orderer will be connected are defined.We will set up Kafka cluster in this project. Kafka will run 2 instances.\nTwoOrgsOrdererGenesis:\n    ...\n            Organizations:\n                - *Org1\n                - *Org2\n                - *Org3\nThe TwoOrgsOrdererGenesis profile needs to be defined to create a genesis block.3 organizations are defined in this configuration.\nTwoOrgsChannel:\n    ...\n        Organizations:\n            - *Org1\n            - *Org2\n            - *Org3\nThe TwoOrgsChannel profile needs to be defined to create the channel transaction.3 organizations are defined in this configuration.\nThe generating certificate scripts defined in the project for the Orderer is as follows.The script file is in the deploy/k8s/fabricfiles/scripts/orderer-certs.sh .\nhttps://medium.com/media/f341500919f04e7f641fd98534490d46/href\n.Certificate Authorities are used to generate the identities assigned to admins, nodes, and users (client applications).This script creates tls certificate, msp for each of the orderers and register and enroll identities with CA.\nThe generating certificate scripts defined in the project for the Org1,Org2 and Org3 are as follows.The scripts files are in the deploy/k8s/fabricfiles/scripts folder.\nhttps://medium.com/media/dc3a5f7fcd52849a0b95a067bb9fd876/href\nThis scripts create tls certificate, msp for each of the organization and register and enroll identities with CA.\nThe generating artifact scripts defined in the project are as follows.The scripts files are in the deploy/k8s/fabricfiles/scripts folder.\nhttps://medium.com/media/57b25494ed8dd23a88d3376b60ed5d86/href\nconfigtxgen -profile TwoOrgsOrdererGenesis\nThe TwoOrgsOrdererGenesis profile must be defined in the configtx.yaml file. It is required to create a Genesis block.\n-channelID system-channel\nTo create a Genesis block, the channel id must be system-channel.\nconfigtxgen -profile TwoOrgsChannel\nThe TwoOrgsChannel profile must be defined in the configtx.yaml file. It is required to create channel transaction.\nCHANNEL_NAME:=\"mychannel\"\nChannel is a private “subnet” of communication between two or more specific network members, for the purpose of conducting private and confidential transactions.Channel id information is required to create a channel transaction.The channel id is assigned “mychannel”.\nfor orgmsp in Org1MSP Org2MSP Org3MSP; do\nset -x\nconfigtxgen -profile TwoOrgsChannel -outputAnchorPeersUpdate ./channel-artifacts/${orgmsp}anchors.tx -channelID $CHANNEL_NAME -asOrg ${orgmsp}\ndone\nGenerate anchor peer update transaction for each organization.\nLets create an certificate creation job, create-certs.yaml .the yaml file is in the deploy/k8s/job folder.\nhttps://medium.com/media/74694427ddbd7822ba8a6d87c17cece6/href\n- |\n  ./scripts/orderer-certs.sh &&\n  ./scripts/org1-certs.sh &&\n  ./scripts/org2-certs.sh &&\n  ./scripts/org3-certs.sh\nScripts to run in job are defined to create certificates.\nvolumeMounts:\n  - name: fabricfiles\n    mountPath: /organizations \n    subPath: organizations \n  - name: fabricfiles\n    mountPath: /scripts\n    subPath: scripts \nRequired to access scripts folder and organizations folder on nfs server.Certificates are created in the organizations folder.\nvolumes:\n  - name: fabricfiles\n    persistentVolumeClaim:\n      claimName: fabricfiles-pvc\nLets create an artifact creation job, create-certs.yaml .the yaml file is in the deploy/k8s/job folder.\nhttps://medium.com/media/216e5a46deced1fb1eefa08659529323/href\nIt allows us to mount the fabric files on the nfs server to the container.\n- |\n  ./scripts/createGenesis.sh &&\n  ./scripts/createChannel.sh\nScripts to run in job are defined to create artifacts.\nvolumeMounts:\n  - name: fabricfiles\n    mountPath: /organizations\n    subPath: organizations \n  - name: fabricfiles\n    mountPath: /configtx \n    subPath: configtx \n  - name: fabricfiles\n    mountPath: /system-genesis-block\n    subPath: system-genesis-block \n  - name: fabricfiles\n    mountPath: /channel-artifacts\n    subPath: channel-artifacts \n  - name: fabricfiles\n    mountPath: /scripts\n    subPath: scripts\nRequired to access scripts folder,configtx folder and organizations folder on nfs server.System Genesis Block is created in the system-genesis-block folder.Channel artifacts are created in the channel-artifacts folder.\nInstallation of Jobs on Kubernetes\n\nLet’s connect to the kubernetes master node virtual machine with the vagrant ssh command.\n$ vagrant ssh k8smaster\nLet’s go to the directory where the kubernetes installation scripts are located.This directory is the same as the deploy/k8s folder in the project. With Vagrant, this directory is synchronized to the virtual machine.\n$ cd /vagrant/k8s\nDeploying the certificate creating job for peers and orderers.\n$ kubectl apply -f job/create-certs.yaml\n\nCertificate creation job pending completion\n$ kubectl wait --for=condition=complete --timeout=300s job create-certs\n\nAfter the job is completed, ordererOrganizations and peerOrganizations were created under the organizations folder on the nfs server.Let’s check if these folders are created.\nLet’s open a new terminal and connect to the nfs server virtual machine.\n$ vagrant ssh nfsserver\n\nLet’s list the folders under the organizations folder.\n$ ls /srv/kubedata/fabricfiles/organizations\n\nAs you can see, the ordererOrganizations and peerOrganizations folders have been created.Certificate creation completed successfully.\nLets come back to kubernetes master node terminal.\nDeploying the certificate artifacts job.\n$ kubectl apply -f job/create-artifacts.yaml\n\nArtifact creation job pending completion.\n$ kubectl wait --for=condition=complete --timeout=300s job create-artifacts\n\nAfter the job is completed, system-genesis-block and channel-artifacts were created under the fabricfiles folder on the nfs server.Let’s check if these folders are created.\nLets come back to nfs server virtual machine terminal.\nLet’s list the folders under the fabricfiles folder.\n$ ls /srv/kubedata/fabricfiles\n\nAs you can see, the system-genesis-block and channel-artifacts folders have been created.Artifacts creation completed successfully.\nFinally, let’s check the conditions of the jobs we run from the lens ide.\n\nThe jobs we run have been completed.\nMy article ends here. In general,I explained generating certificates for peers,orderer and generating genesis block and generating channel transactions on Kubernetes.\nSee you in the next articles.\nProject Links\nSpring Boot Hlf Starter Project details and installation can be accessed via this link.\nAsset Transfer Project details and installation can be accessed via this link",
      "dc:creator": "Şuayb Şimşek",
      "guid": "https://medium.com/p/d6583a57153c",
      "categories": ["blockchain", "kubernetes", "hyperledger", "hyperledger-fabric", "spring-boot"],
      "isoDate": "2021-11-27T13:47:38.000Z"
    },
    {
      "creator": "Şuayb Şimşek",
      "title": "Spring Boot Fullstack Blockchain Application With Hyperledger Fabric running on Kubernetes (Part 3)",
      "link": "https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-3-7eb2fc75ba60?source=rss-bda589f2335a------2",
      "pubDate": "Sun, 21 Nov 2021 14:33:12 GMT",
      "content:encoded": "<h3>Spring Boot Fullstack Blockchain Application With Hyperledger Fabric running on Kubernetes (Part 3) — Fabric CA Server</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*S9G4q0odpEbDjhcIowUDQw.png\" /></figure><p>Hello everyone, through this article series we will the Hyperledger Fabric integration with Spring Boot.In this article, we will look into Fabric CA Server and installation of Fabric CA Server on Kubernetes.</p><p>Other articles on Hyperledger Fabric integration with Spring Boot can be accessed from the links below.</p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-1-c8f7c87d60eb\">Part 1 — Introduction</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-ddce8d25858d\">Part 2— Kubernetes Cluster Setup</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-3-7eb2fc75ba60\">Part 3— Fabric CA Server</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-4-d6583a57153c\">Part 4 — Generating Certificates and Artifacts</a></p><p><a href=\"https://suaybsimsek58.medium.com/1723502770e7\">Part 5 — Kafka</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-6-b82662ada0a4\">Part 6 — Orderer</a></p><h3>What is Fabric CA Server?</h3><p>Fabric CA is a Certificate Authority (CA) for Hyperledger Fabric.</p><p>It provides features such as:</p><ul><li>registration of identities, or connects to LDAP as the user registry</li><li>issuance of Enrollment Certificates (ECerts)</li><li>certificate renewal and revocation</li></ul><p>The diagram below illustrates how the Hyperledger Fabric CA server fits into the overall Hyperledger Fabric architecture.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/884/1*EUg8Gmew5iO6kiqgYjEdbQ.png\" /></figure><p>There are two ways of interacting with a Hyperledger Fabric CA server: via the Hyperledger Fabric CA client or through one of the Fabric SDKs. All communication to the Hyperledger Fabric CA server is via REST APIs.</p><p>The Hyperledger Fabric CA client or SDK may connect to a server in a cluster of Hyperledger Fabric CA servers. This is illustrated in the top right section of the diagram. The client routes to an HA Proxy endpoint which load balances traffic to one of the fabric-ca-server cluster members.</p><p>A server may contain multiple CAs. Each CA is either a root CA or an intermediate CA. Each intermediate CA has a parent CA which is either a root CA or another intermediate CA.</p><h3><strong>Configuration</strong> Fabric CA Server</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*fKBNd-R9xUgwkybAqRo1jw.png\" /></figure><p>Let’s open the project we downloaded from <a href=\"https://github.com/susimsek/spring-boot-hlf-k8s-fullstack-blockchain-app\">this link</a> and go to the directory where the ca is located.</p><pre>$ cd deploy/k8s</pre><p><strong>Configuration Settings</strong></p><p>The Fabric CA provides 3 ways to configure settings on the Fabric CA server and client. The precedence order is:</p><p>1. CLI flags</p><p>2. Environment variables</p><p>3. Configuration file</p><p>We will configure the fabric ca server with the configuration file.</p><p>We will set up 4 Fabric CAservers for orderer, org1, org2 and org3.It is necessary to define a configuration file for each Fabric CA server.</p><p>The Fabric CA configuration file defined in the project for the Orderer is as follows.the configuration file is in the deploy/k8s/fabricfiles/organizations/fabric-ca/ordererOrg folder.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/29e4118b63e1c508812e25622f641b59/href\">https://medium.com/media/29e4118b63e1c508812e25622f641b59/href</a></iframe><pre>csr:<br>   ...<br>   hosts:<br>     - localhost<br>     - example.com<br>     - ca-orderer</pre><p>Added localhost,example.com,ca-orderer as csr host to the config file.</p><p>ca-orderer is the service name of fabric ca server in kubernetes.</p><pre>ca:<br>  <em># Name of this CA<br>  </em>name: OrdererCA</pre><p>OrdererCA is the CA name.</p><pre>csr:<br>   cn: ca-org1<br>   names:<br>      - C: US<br>        ST: &quot;New York&quot;<br>        L: &quot;New York&quot;<br>        O: ca-org1<br>        OU: ca-org1</pre><p>All of the fields above pertain to the X.509 signing key and certificate which is generated by the fabric-ca-server init. This corresponds to the ca.certfile and ca.keyfile files in the server’s configuration file. The fields are as follows:</p><ul><li><strong>cn</strong> is the Common Name</li><li><strong>O</strong> is the organization name</li><li><strong>OU</strong> is the organizational unit</li><li><strong>L</strong> is the location or city</li><li><strong>ST</strong> is the state</li><li><strong>C</strong> is the country</li></ul><pre>registry:<br>  ...<em><br>  </em>identities:<br>     - name: admin<br>       pass: adminpw<br>       type: client<br>       affiliation: &quot;&quot;</pre><p>it must be configured with at least one pre-registered bootstrap identity to enable you to register and enroll other identities.The -b option specifies the name and password for a bootstrap identity.</p><pre>db:<br>  type: sqlite3<br>  datasource: fabric-ca-server.db</pre><p>The default database is SQLite and the default database file is fabric-ca-server.db in the Fabric CA server’s home directory. Fabric CA server can also connect to PostgreSQL or MySQL databases.</p><p>Fabric CA supports the following database versions in a cluster setup:</p><ul><li>PostgreSQL: 9.5.5 or later</li><li>MySQL: 5.7 or later</li></ul><pre>affiliations:<br>   org1:<br>      - department1<br>      - department2<br>   org2:<br>      - department1</pre><p>I think of affiliations as hierarchical tags. Each identity can be tagged (affiliated) to (with) one affiliation in the hierarchy. When an identity is associated with an affiliation, it is affiliated with that and all the child affiliations.Affiliations are currently used during registration and revocation.</p><p>The Fabric CA configuration file defined in the project for the Org1 is as follows.The configuration file is in the deploy/k8s/fabricfiles/organizations/fabric-ca/org1 folder.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/295956ebcc3ae128e2627175f0876e64/href\">https://medium.com/media/295956ebcc3ae128e2627175f0876e64/href</a></iframe><pre>csr:<br>   ...<br>   hosts:<br>     - localhost<br>     - example.com<br>     - ca-org1</pre><p>Added localhost,example.com,ca-org1 as csr host to the config file.</p><p>ca-org1 is the service name of fabric ca server in kubernetes.</p><pre>ca:<br>  <em># Name of this CA<br>  </em>name: Org1CA</pre><p>Org1CA is the CA name.</p><pre>csr:<br>   cn: ca-org1<br>   names:<br>      - C: US<br>        ST: &quot;New York&quot;<br>        L: &quot;New York&quot;<br>        O: ca-org1<br>        OU: ca-org1</pre><p>All of the fields above pertain to the X.509 signing key and certificate which is generated by the fabric-ca-server init.</p><pre>affiliations:<br>   org1:<br>      - department1<br>      - department2<br>   org2:<br>      - department1<br>   org3:<br>      - department1</pre><p>The affiliation definitions differ from the CA Orderer configuration file.Affiliation definitions have been added in Org2 and Org3.Database and bootstrap user configurations are the same as ca orderer.</p><p>The Fabric CA configuration file defined in the project for the Org2 is as follows.The configuration file is in the deploy/k8s/fabricfiles/organizations/fabric-ca/org2 folder.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/214a2d3e86b8b7cded53e56907794e6b/href\">https://medium.com/media/214a2d3e86b8b7cded53e56907794e6b/href</a></iframe><p>Csr settings and CA name are different from CA Org1 configuration. Other configurations are the same as ca org1.</p><pre>csr:<br>   cn: ca-org2<br>   names:<br>      - C: US<br>        ST: &quot;New York&quot;<br>        L: &quot;New York&quot;<br>        O: ca-org2<br>        OU: ca-org2<br>   hosts:<br>     - localhost<br>     - example.com<br>     - ca-org2</pre><p>ca-org2 is the service name of fabric ca server in kubernetes.</p><pre>ca:<br>  <em># Name of this CA<br>  </em>name: Org2CA</pre><p>The Fabric CA configuration file defined in the project for the Org3 is as follows.the configuration file is in the deploy/k8s/fabricfiles/organizations/fabric-ca/org3 folder.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/aef09ba77b4e984307ac6bcdc4faf023/href\">https://medium.com/media/aef09ba77b4e984307ac6bcdc4faf023/href</a></iframe><p>Csr settings and CA name are different from CA Org1 configuration. Other configurations are the same as ca org1.</p><pre>csr:<br>   cn: ca-org3<br>   names:<br>      - C: US<br>        ST: &quot;New York&quot;<br>        L: &quot;New York&quot;<br>        O: ca-org3<br>        OU: ca-org3<br>   hosts:<br>     - localhost<br>     - example.com<br>     - ca-org3</pre><p>ca-org3 is the service name of fabric ca server in kubernetes.</p><pre>ca:<br>  <em># Name of this CA<br>  </em>name: Org3CA</pre><p>After setting Fabric CA Server configurations for Orderer,Org1,Org2 and Org3, we can deploy CA Servers to kubernetes.</p><h3>Installation of Fabric CA Server on Kubernetes</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/922/1*nojGPkEmyjulIIyPGv6k9w.png\" /></figure><p>Let’s create persistence volume and persistence volume claim to access fabric files from the pod,fabricfiles-pv.yaml and fabricfiles-pvc.yaml.</p><p>The fabricfiles-pv.yaml file is in the deploy/k8s/pv folder.</p><p>The fabricfiles-pvc.yaml file is in the deploy/k8s/pvc folder.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/cedebf67a436edecb0cedf0dfc21e5d3/href\">https://medium.com/media/cedebf67a436edecb0cedf0dfc21e5d3/href</a></iframe><pre>nfs:<br>  path: /srv/kubedata/fabricfiles<br>  server: 192.168.12.9</pre><p>192.168.12.9 is the ip of the nfs server.</p><p>/srv/kubedata/fabricfiles is the directory where the fabric files are located on the nfs server.</p><p>lets create an Fabric Orderer CA server deployment and service, ca-orderer.yaml and ca-orderer-svc.yaml.The yaml files is in the deploy/k8s/ca folder.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/64a53d8b9a94c102168ff620ddd6df47/href\">https://medium.com/media/64a53d8b9a94c102168ff620ddd6df47/href</a></iframe><pre>kind: Service<br>metadata:<br>  name: ca-orderer</pre><p>ca-orderer is the kubernetes service name of the Orderer Fabric CA server.It must be added in the csr host configuration in the Fabric CA Configuration file.</p><pre>ports:<br>- protocol: TCP<br>  targetPort: 10054 <em><br>  </em>port: 10054 </pre><p>targetPort is the container internal port.10054</p><p>port is the service port.10054</p><pre>&quot;fabric-ca-server&quot;,<br>&quot;start&quot;, &quot;-b&quot;, &quot;admin:adminpw&quot;, &quot;--port&quot;, &quot;10054&quot;, &quot;-d&quot;</pre><p>The -b (bootstrap identity) option is required for initialization when LDAP is disabled. At least one bootstrap identity is required to start the Fabric CA server; this identity is the server administrator.</p><pre>admin:adminpw </pre><p>server admin information.Admin username is admin,Admin password is adminpw.</p><pre>--port&quot;, &quot;10054&quot;</pre><p>10054 is assigned as the ca server container port.</p><pre>env:<br>  - name: FABRIC_CA_SERVER_CA_NAME<br>    value: ca-orderer<br>  - name: FABRIC_CA_SERVER_TLS_ENABLED<br>    value: &quot;true&quot;</pre><p>Tls aws active and ca name ca-orderer is assigned.</p><pre>volumeMounts:<br>  - name: data<br>    mountPath: /etc/hyperledger/fabric-ca-server<br>    subPath: organizations/fabric-ca/ordererOrg</pre><p>organizations/fabric-ca/ordererOrg: path of ca server configuration file in fabric files</p><p>lets create an Fabric CA Org1 server deployment and service, ca-org1.yaml and ca-org1-svc.yaml.the yaml files are in the deploy/k8s/ca folder.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/e0bae4d673e3b457a0647e232d5c98d0/href\">https://medium.com/media/e0bae4d673e3b457a0647e232d5c98d0/href</a></iframe><pre>kind: Service<br>metadata:<br>  name: ca-org1</pre><p>ca-org1 is the kubernetes service name of the Org1 Fabric CA server.It must be added in the csr host configuration in the Fabric CA Configuration file.</p><pre>ports:<br>- protocol: TCP<br>  targetPort: 7054 <em><br>  </em>port: 7054</pre><p>targetPort is the container internal port.7054</p><p>port is the service port.7054</p><pre>--port&quot;, &quot;7054&quot;</pre><p>7054 is assigned as the ca server container port.</p><pre>- name: FABRIC_CA_SERVER_CA_NAME<br>  value: ca-org1<br>- name: FABRIC_CA_SERVER_TLS_ENABLED<br>  value: &quot;true&quot;<br>- name: FABRIC_CA_SERVER_CSR_CN<br>  value: &quot;ca-org1&quot;<br>- name: FABRIC_CA_SERVER_CSR_HOSTS<br>  value: &quot;ca-org1&quot;</pre><p>Tls was activated, ca name ca-org1 was assigned, and csr host and cn ca-org1 were assigned.</p><pre>volumeMounts:<br>  - name: data<br>    mountPath: /etc/hyperledger/fabric-ca-server<br>    subPath: organizations/fabric-ca/org1</pre><p>organizations/fabric-ca/org1: path of ca server configuration file in fabric files</p><p>lets create an Fabric CA Org2 server deployment and service, ca-org2.yaml and ca-org2-svc.yaml.the yaml files are in the deploy/k8s/ca folder.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/fa8787671f4500c5f0ec8e5e957976bc/href\">https://medium.com/media/fa8787671f4500c5f0ec8e5e957976bc/href</a></iframe><pre>kind: Service<br>metadata:<br>  name: ca-org2</pre><p>ca-org2 is the kubernetes service name of the Org2 Fabric CA server.It must be added in the csr host configuration in the Fabric CA Configuration file.</p><pre>ports:<br>- protocol: TCP<br>  targetPort: 8054 <em><br>  </em>port: 8054 </pre><p>targetPort is the container internal port.8054</p><p>port is the service port.8054</p><pre>--port&quot;, &quot;8054&quot;</pre><p>8054 is assigned as the ca server container port.</p><pre>- name: FABRIC_CA_SERVER_CA_NAME<br>  value: ca-org2<br>- name: FABRIC_CA_SERVER_TLS_ENABLED<br>  value: &quot;true&quot;<br>- name: FABRIC_CA_SERVER_CSR_CN<br>  value: &quot;ca-org2&quot;<br>- name: FABRIC_CA_SERVER_CSR_HOSTS<br>  value: &quot;ca-org2&quot;</pre><p>Tls was activated, ca name ca-org2 was assigned, and csr host and cn ca-org2 were assigned.</p><pre>volumeMounts:<br>  - name: data<br>    mountPath: /etc/hyperledger/fabric-ca-server<br>    subPath: organizations/fabric-ca/org2</pre><p>organizations/fabric-ca/org2: path of ca server configuration file in fabric files</p><p>lets create an Fabric CA Org3 server deployment and service, ca-org3.yaml and ca-org3-svc.yaml.the yaml files is in the deploy/k8s/ca folder.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/c6194cb554cfb69b415ee745346d6c43/href\">https://medium.com/media/c6194cb554cfb69b415ee745346d6c43/href</a></iframe><pre>kind: Service<br>metadata:<br>  name: ca-org3</pre><p>ca-org3 is the kubernetes service name of the Org3 Fabric CA server.It must be added in the csr host configuration in the Fabric CA Configuration file.</p><pre>ports:<br>- protocol: TCP<br>  targetPort: 9054 <em><br>  </em>port: 9054</pre><p>targetPort is the container internal port.9054</p><p>port is the service port.9054</p><pre>--port&quot;, &quot;9054&quot;</pre><p>9054 is assigned as the ca server container port.</p><pre>- name: FABRIC_CA_SERVER_CA_NAME<br>  value: ca-org3<br>- name: FABRIC_CA_SERVER_TLS_ENABLED<br>  value: &quot;true&quot;<br>- name: FABRIC_CA_SERVER_CSR_CN<br>  value: &quot;ca-org3&quot;<br>- name: FABRIC_CA_SERVER_CSR_HOSTS<br>  value: &quot;ca-org3&quot;</pre><p>Tls was activated, ca name ca-org3 was assigned, and csr host and cn ca-org3 were assigned.</p><pre>volumeMounts:<br>  - name: data<br>    mountPath: /etc/hyperledger/fabric-ca-server<br>    subPath: organizations/fabric-ca/org3</pre><p>organizations/fabric-ca/org3: path of ca server configuration file in fabric files</p><p>Let’s define a job that creates the certificates for orderer,org1,org2 and org3.</p><p>Let’s connect to the kubernetes master node virtual machine with the <strong>vagrant ssh</strong> command.</p><pre>$ vagrant ssh k8smaster</pre><p>Let’s go to the directory where the kubernetes installation scripts are located.This directory is the same as the deploy/k8s folder in the project. With Vagrant, this directory is synchronized to the virtual machine.</p><pre>$ <em>cd</em> /vagrant/k8s</pre><p>Deploying the persistence volume for fabric files</p><pre>$ kubectl apply -f pv/fabricfiles-pv.yaml</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*ZD7joKRdD6roe6G3dSz1EQ.png\" /></figure><p>Deploying the persistence volume claim for fabric files</p><pre>$ kubectl apply -f pvc/fabricfiles-pvc.yaml</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*84R17p_E8SirPgdTQ5CDQQ.png\" /></figure><p>Deploying the fabric ca server</p><pre>$ <em>kubectl </em>apply -f ca/</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*kM3uuCXEnB830NaihhFq7g.png\" /></figure><p>Fabric ca server creation pending completion</p><pre>$ <em>kubectl </em>wait --for condition=available --timeout=300s deployment -l &quot;app in (ca-orderer,ca-org1,ca-org2,ca-org3)&quot;</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*xLAXkMaAPyLEGGifkmnwMg.png\" /></figure><p>Finally, let’s check whether our pods from the lens ide are running.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*s-mL0tBGbWIv83dVeFCr4A.png\" /></figure><p>All of fabric ca server pods are running.</p><p>My article ends here. In general,I explained Fabric CA Server introduction and installation of Fabric CA Server on Kubernetes.</p><p>See you in the next articles.</p><h3>Project Links</h3><p>Spring Boot Hlf Starter Project details and installation can be accessed via <a href=\"https://github.com/susimsek/spring-boot-hlf-starter\">this link</a>.</p><p>Asset Transfer Project details and installation can be accessed via <a href=\"https://github.com/susimsek/spring-boot-hlf-starter\">this link</a></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=7eb2fc75ba60\" width=\"1\" height=\"1\" alt=\"\">",
      "content:encodedSnippet": "Spring Boot Fullstack Blockchain Application With Hyperledger Fabric running on Kubernetes (Part 3) — Fabric CA Server\n\nHello everyone, through this article series we will the Hyperledger Fabric integration with Spring Boot.In this article, we will look into Fabric CA Server and installation of Fabric CA Server on Kubernetes.\nOther articles on Hyperledger Fabric integration with Spring Boot can be accessed from the links below.\nPart 1 — Introduction\nPart 2— Kubernetes Cluster Setup\nPart 3— Fabric CA Server\nPart 4 — Generating Certificates and Artifacts\nPart 5 — Kafka\nPart 6 — Orderer\nWhat is Fabric CA Server?\nFabric CA is a Certificate Authority (CA) for Hyperledger Fabric.\nIt provides features such as:\n\nregistration of identities, or connects to LDAP as the user registry\nissuance of Enrollment Certificates (ECerts)\ncertificate renewal and revocation\n\nThe diagram below illustrates how the Hyperledger Fabric CA server fits into the overall Hyperledger Fabric architecture.\n\nThere are two ways of interacting with a Hyperledger Fabric CA server: via the Hyperledger Fabric CA client or through one of the Fabric SDKs. All communication to the Hyperledger Fabric CA server is via REST APIs.\nThe Hyperledger Fabric CA client or SDK may connect to a server in a cluster of Hyperledger Fabric CA servers. This is illustrated in the top right section of the diagram. The client routes to an HA Proxy endpoint which load balances traffic to one of the fabric-ca-server cluster members.\nA server may contain multiple CAs. Each CA is either a root CA or an intermediate CA. Each intermediate CA has a parent CA which is either a root CA or another intermediate CA.\nConfiguration Fabric CA Server\n\nLet’s open the project we downloaded from this link and go to the directory where the ca is located.\n$ cd deploy/k8s\nConfiguration Settings\nThe Fabric CA provides 3 ways to configure settings on the Fabric CA server and client. The precedence order is:\n1. CLI flags\n2. Environment variables\n3. Configuration file\nWe will configure the fabric ca server with the configuration file.\nWe will set up 4 Fabric CAservers for orderer, org1, org2 and org3.It is necessary to define a configuration file for each Fabric CA server.\nThe Fabric CA configuration file defined in the project for the Orderer is as follows.the configuration file is in the deploy/k8s/fabricfiles/organizations/fabric-ca/ordererOrg folder.\nhttps://medium.com/media/29e4118b63e1c508812e25622f641b59/href\ncsr:\n   ...\n   hosts:\n     - localhost\n     - example.com\n     - ca-orderer\nAdded localhost,example.com,ca-orderer as csr host to the config file.\nca-orderer is the service name of fabric ca server in kubernetes.\nca:\n  # Name of this CA\n  name: OrdererCA\nOrdererCA is the CA name.\ncsr:\n   cn: ca-org1\n   names:\n      - C: US\n        ST: \"New York\"\n        L: \"New York\"\n        O: ca-org1\n        OU: ca-org1\nAll of the fields above pertain to the X.509 signing key and certificate which is generated by the fabric-ca-server init. This corresponds to the ca.certfile and ca.keyfile files in the server’s configuration file. The fields are as follows:\n\ncn is the Common Name\nO is the organization name\nOU is the organizational unit\nL is the location or city\nST is the state\nC is the country\n\nregistry:\n  ...\n  identities:\n     - name: admin\n       pass: adminpw\n       type: client\n       affiliation: \"\"\nit must be configured with at least one pre-registered bootstrap identity to enable you to register and enroll other identities.The -b option specifies the name and password for a bootstrap identity.\ndb:\n  type: sqlite3\n  datasource: fabric-ca-server.db\nThe default database is SQLite and the default database file is fabric-ca-server.db in the Fabric CA server’s home directory. Fabric CA server can also connect to PostgreSQL or MySQL databases.\nFabric CA supports the following database versions in a cluster setup:\n\nPostgreSQL: 9.5.5 or later\nMySQL: 5.7 or later\n\naffiliations:\n   org1:\n      - department1\n      - department2\n   org2:\n      - department1\nI think of affiliations as hierarchical tags. Each identity can be tagged (affiliated) to (with) one affiliation in the hierarchy. When an identity is associated with an affiliation, it is affiliated with that and all the child affiliations.Affiliations are currently used during registration and revocation.\nThe Fabric CA configuration file defined in the project for the Org1 is as follows.The configuration file is in the deploy/k8s/fabricfiles/organizations/fabric-ca/org1 folder.\nhttps://medium.com/media/295956ebcc3ae128e2627175f0876e64/href\ncsr:\n   ...\n   hosts:\n     - localhost\n     - example.com\n     - ca-org1\nAdded localhost,example.com,ca-org1 as csr host to the config file.\nca-org1 is the service name of fabric ca server in kubernetes.\nca:\n  # Name of this CA\n  name: Org1CA\nOrg1CA is the CA name.\ncsr:\n   cn: ca-org1\n   names:\n      - C: US\n        ST: \"New York\"\n        L: \"New York\"\n        O: ca-org1\n        OU: ca-org1\nAll of the fields above pertain to the X.509 signing key and certificate which is generated by the fabric-ca-server init.\naffiliations:\n   org1:\n      - department1\n      - department2\n   org2:\n      - department1\n   org3:\n      - department1\nThe affiliation definitions differ from the CA Orderer configuration file.Affiliation definitions have been added in Org2 and Org3.Database and bootstrap user configurations are the same as ca orderer.\nThe Fabric CA configuration file defined in the project for the Org2 is as follows.The configuration file is in the deploy/k8s/fabricfiles/organizations/fabric-ca/org2 folder.\nhttps://medium.com/media/214a2d3e86b8b7cded53e56907794e6b/href\nCsr settings and CA name are different from CA Org1 configuration. Other configurations are the same as ca org1.\ncsr:\n   cn: ca-org2\n   names:\n      - C: US\n        ST: \"New York\"\n        L: \"New York\"\n        O: ca-org2\n        OU: ca-org2\n   hosts:\n     - localhost\n     - example.com\n     - ca-org2\nca-org2 is the service name of fabric ca server in kubernetes.\nca:\n  # Name of this CA\n  name: Org2CA\nThe Fabric CA configuration file defined in the project for the Org3 is as follows.the configuration file is in the deploy/k8s/fabricfiles/organizations/fabric-ca/org3 folder.\nhttps://medium.com/media/aef09ba77b4e984307ac6bcdc4faf023/href\nCsr settings and CA name are different from CA Org1 configuration. Other configurations are the same as ca org1.\ncsr:\n   cn: ca-org3\n   names:\n      - C: US\n        ST: \"New York\"\n        L: \"New York\"\n        O: ca-org3\n        OU: ca-org3\n   hosts:\n     - localhost\n     - example.com\n     - ca-org3\nca-org3 is the service name of fabric ca server in kubernetes.\nca:\n  # Name of this CA\n  name: Org3CA\nAfter setting Fabric CA Server configurations for Orderer,Org1,Org2 and Org3, we can deploy CA Servers to kubernetes.\nInstallation of Fabric CA Server on Kubernetes\n\nLet’s create persistence volume and persistence volume claim to access fabric files from the pod,fabricfiles-pv.yaml and fabricfiles-pvc.yaml.\nThe fabricfiles-pv.yaml file is in the deploy/k8s/pv folder.\nThe fabricfiles-pvc.yaml file is in the deploy/k8s/pvc folder.\nhttps://medium.com/media/cedebf67a436edecb0cedf0dfc21e5d3/href\nnfs:\n  path: /srv/kubedata/fabricfiles\n  server: 192.168.12.9\n192.168.12.9 is the ip of the nfs server.\n/srv/kubedata/fabricfiles is the directory where the fabric files are located on the nfs server.\nlets create an Fabric Orderer CA server deployment and service, ca-orderer.yaml and ca-orderer-svc.yaml.The yaml files is in the deploy/k8s/ca folder.\nhttps://medium.com/media/64a53d8b9a94c102168ff620ddd6df47/href\nkind: Service\nmetadata:\n  name: ca-orderer\nca-orderer is the kubernetes service name of the Orderer Fabric CA server.It must be added in the csr host configuration in the Fabric CA Configuration file.\nports:\n- protocol: TCP\n  targetPort: 10054 \n  port: 10054 \ntargetPort is the container internal port.10054\nport is the service port.10054\n\"fabric-ca-server\",\n\"start\", \"-b\", \"admin:adminpw\", \"--port\", \"10054\", \"-d\"\nThe -b (bootstrap identity) option is required for initialization when LDAP is disabled. At least one bootstrap identity is required to start the Fabric CA server; this identity is the server administrator.\nadmin:adminpw \nserver admin information.Admin username is admin,Admin password is adminpw.\n--port\", \"10054\"\n10054 is assigned as the ca server container port.\nenv:\n  - name: FABRIC_CA_SERVER_CA_NAME\n    value: ca-orderer\n  - name: FABRIC_CA_SERVER_TLS_ENABLED\n    value: \"true\"\nTls aws active and ca name ca-orderer is assigned.\nvolumeMounts:\n  - name: data\n    mountPath: /etc/hyperledger/fabric-ca-server\n    subPath: organizations/fabric-ca/ordererOrg\norganizations/fabric-ca/ordererOrg: path of ca server configuration file in fabric files\nlets create an Fabric CA Org1 server deployment and service, ca-org1.yaml and ca-org1-svc.yaml.the yaml files are in the deploy/k8s/ca folder.\nhttps://medium.com/media/e0bae4d673e3b457a0647e232d5c98d0/href\nkind: Service\nmetadata:\n  name: ca-org1\nca-org1 is the kubernetes service name of the Org1 Fabric CA server.It must be added in the csr host configuration in the Fabric CA Configuration file.\nports:\n- protocol: TCP\n  targetPort: 7054 \n  port: 7054\ntargetPort is the container internal port.7054\nport is the service port.7054\n--port\", \"7054\"\n7054 is assigned as the ca server container port.\n- name: FABRIC_CA_SERVER_CA_NAME\n  value: ca-org1\n- name: FABRIC_CA_SERVER_TLS_ENABLED\n  value: \"true\"\n- name: FABRIC_CA_SERVER_CSR_CN\n  value: \"ca-org1\"\n- name: FABRIC_CA_SERVER_CSR_HOSTS\n  value: \"ca-org1\"\nTls was activated, ca name ca-org1 was assigned, and csr host and cn ca-org1 were assigned.\nvolumeMounts:\n  - name: data\n    mountPath: /etc/hyperledger/fabric-ca-server\n    subPath: organizations/fabric-ca/org1\norganizations/fabric-ca/org1: path of ca server configuration file in fabric files\nlets create an Fabric CA Org2 server deployment and service, ca-org2.yaml and ca-org2-svc.yaml.the yaml files are in the deploy/k8s/ca folder.\nhttps://medium.com/media/fa8787671f4500c5f0ec8e5e957976bc/href\nkind: Service\nmetadata:\n  name: ca-org2\nca-org2 is the kubernetes service name of the Org2 Fabric CA server.It must be added in the csr host configuration in the Fabric CA Configuration file.\nports:\n- protocol: TCP\n  targetPort: 8054 \n  port: 8054 \ntargetPort is the container internal port.8054\nport is the service port.8054\n--port\", \"8054\"\n8054 is assigned as the ca server container port.\n- name: FABRIC_CA_SERVER_CA_NAME\n  value: ca-org2\n- name: FABRIC_CA_SERVER_TLS_ENABLED\n  value: \"true\"\n- name: FABRIC_CA_SERVER_CSR_CN\n  value: \"ca-org2\"\n- name: FABRIC_CA_SERVER_CSR_HOSTS\n  value: \"ca-org2\"\nTls was activated, ca name ca-org2 was assigned, and csr host and cn ca-org2 were assigned.\nvolumeMounts:\n  - name: data\n    mountPath: /etc/hyperledger/fabric-ca-server\n    subPath: organizations/fabric-ca/org2\norganizations/fabric-ca/org2: path of ca server configuration file in fabric files\nlets create an Fabric CA Org3 server deployment and service, ca-org3.yaml and ca-org3-svc.yaml.the yaml files is in the deploy/k8s/ca folder.\nhttps://medium.com/media/c6194cb554cfb69b415ee745346d6c43/href\nkind: Service\nmetadata:\n  name: ca-org3\nca-org3 is the kubernetes service name of the Org3 Fabric CA server.It must be added in the csr host configuration in the Fabric CA Configuration file.\nports:\n- protocol: TCP\n  targetPort: 9054 \n  port: 9054\ntargetPort is the container internal port.9054\nport is the service port.9054\n--port\", \"9054\"\n9054 is assigned as the ca server container port.\n- name: FABRIC_CA_SERVER_CA_NAME\n  value: ca-org3\n- name: FABRIC_CA_SERVER_TLS_ENABLED\n  value: \"true\"\n- name: FABRIC_CA_SERVER_CSR_CN\n  value: \"ca-org3\"\n- name: FABRIC_CA_SERVER_CSR_HOSTS\n  value: \"ca-org3\"\nTls was activated, ca name ca-org3 was assigned, and csr host and cn ca-org3 were assigned.\nvolumeMounts:\n  - name: data\n    mountPath: /etc/hyperledger/fabric-ca-server\n    subPath: organizations/fabric-ca/org3\norganizations/fabric-ca/org3: path of ca server configuration file in fabric files\nLet’s define a job that creates the certificates for orderer,org1,org2 and org3.\nLet’s connect to the kubernetes master node virtual machine with the vagrant ssh command.\n$ vagrant ssh k8smaster\nLet’s go to the directory where the kubernetes installation scripts are located.This directory is the same as the deploy/k8s folder in the project. With Vagrant, this directory is synchronized to the virtual machine.\n$ cd /vagrant/k8s\nDeploying the persistence volume for fabric files\n$ kubectl apply -f pv/fabricfiles-pv.yaml\n\nDeploying the persistence volume claim for fabric files\n$ kubectl apply -f pvc/fabricfiles-pvc.yaml\n\nDeploying the fabric ca server\n$ kubectl apply -f ca/\n\nFabric ca server creation pending completion\n$ kubectl wait --for condition=available --timeout=300s deployment -l \"app in (ca-orderer,ca-org1,ca-org2,ca-org3)\"\n\nFinally, let’s check whether our pods from the lens ide are running.\n\nAll of fabric ca server pods are running.\nMy article ends here. In general,I explained Fabric CA Server introduction and installation of Fabric CA Server on Kubernetes.\nSee you in the next articles.\nProject Links\nSpring Boot Hlf Starter Project details and installation can be accessed via this link.\nAsset Transfer Project details and installation can be accessed via this link",
      "dc:creator": "Şuayb Şimşek",
      "guid": "https://medium.com/p/7eb2fc75ba60",
      "categories": ["spring-boot", "hyperledger-fabric", "blockchain", "fabric-ca-server", "kubernetes"],
      "isoDate": "2021-11-21T14:33:12.000Z"
    },
    {
      "creator": "Şuayb Şimşek",
      "title": "Spring Boot Fullstack Blockchain Application With Hyperledger Fabric running on Kubernetes (Part 2)",
      "link": "https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-ddce8d25858d?source=rss-bda589f2335a------2",
      "pubDate": "Sun, 31 Oct 2021 18:04:46 GMT",
      "content:encoded": "<h3>Spring Boot Fullstack Blockchain Application With Hyperledger Fabric running on Kubernetes (Part 2) — Kubernetes Cluster Setup</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*9wP-lyvH_YZXzgJHWQRfCw.png\" /></figure><p>Hello everyone, through this article series we will the Hyperledger Fabric integration with Spring Boot.In this article, we will look into Kubernetes Cluster<em>, Haproxy,Nfs server </em>introductions and installations using Vagrant.</p><p>Other articles on Hyperledger Fabric integration with Spring Boot can be accessed from the links below.</p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-1-c8f7c87d60eb\">Part 1 — Introduction</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-ddce8d25858d\">Part 2 — Kubernetes Cluster Setup</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-3-7eb2fc75ba60\">Part 3 — Fabric CA Server</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-4-d6583a57153c\">Part 4 — Generating Certificates and Artifacts</a></p><p><a href=\"https://suaybsimsek58.medium.com/1723502770e7\">Part 5 — Kafka</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-6-b82662ada0a4\">Part 6 — Orderer</a></p><h3>What is Kubernetes?</h3><p>Kubernetes is a production-grade open-source container orchestration tool developed by Google to help you manage the containerized/dockerized applications supporting multiple deployment environments like On-premise, cloud, or virtual machines.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/263/1*T9coezg1goi_0U8TbqZmVw.png\" /></figure><p>The name Kubernetes originates from Greek, meaning helmsman or pilot. K8s as an abbreviation results from counting the eight letters between the “K” and the “s”. Google open-sourced the Kubernetes project in 2014.</p><p>Project details of Kubernetes can be accessed via <a href=\"https://github.com/kubernetes/kubernetes\">this link</a></p><h3>What is Vagrant?</h3><p>Vagrant is a tool that allows us to create and manage virtual machines developed by HashiCorp. With Vagrant, we can easily configure the virtual machine and manage many machines easily.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/256/1*yiX9jOguPIJUxd3_GQ6aCQ.png\" /></figure><p>Vagrant automates the infrastructure of your virtual machines by using a single file Vagrantfile.</p><p>Project details of Vagrant can be accessed via <a href=\"https://github.com/hashicorp/vagrant\">this link</a></p><h3>Vagrant Installation</h3><p>For vagrant installation, virtualbox or hyperv must be installed on your local computer.We’ll be using the VirtualBox provider, which is the default provider for Vagrant.The Vagranfile I created supports hyperv and virtualbox providers.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/630/1*ZjDtrZjJRQREjFrsn_VaJw.png\" /></figure><p>You can download Vagrant from <a href=\"https://www.vagrantup.com/\">https://www.vagrantup.com/</a> and VirtualBox from <a href=\"https://www.virtualbox.org/wiki/Downloads\">https://www.virtualbox.org/wiki/Downloads</a> and install .</p><p>After installing you can confirm the <strong>vagrant version</strong> with installation.</p><h3>Installing the Guest Addition Plugin for Vagrant</h3><p>Guest Addition is essentially for being able to unleash Vagrant’s full potential, meaning it is important that it is installed and kept updated. However, ensuring your Vagrant boxes are always running the latest version of Guest Additions can be a time-consuming task, stealing away crucial cycles that could be put to better use.</p><p>We can install this plugin with the following command.</p><pre>vagrant plugin install vagrant-vbguest</pre><h3>Vagrantfile</h3><p>Vagrantfile is a file in which the necessary information (ram, cpu, vm_provider) is written for configuring and orchestrating virtual machines. Ruby language is used to make this configuration.</p><h3>Virtual Machine Installations Using Vagrant</h3><p>Let’s open the project we downloaded from <a href=\"https://github.com/susimsek/spring-boot-hlf-k8s-fullstack-blockchain-app\">this link</a> and go to the directory where the Vagrantfile is located.</p><pre>$ cd deploy</pre><p>We will set up 3 virtual machines for nfs server, ha proxy and kubernetes cluster using vagrant.</p><p>The hardware requirements and ips of the virtual machine are shown in the table below.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/5be99320a54d3a8150d624b011e732bc/href\">https://medium.com/media/5be99320a54d3a8150d624b011e732bc/href</a></iframe><p>Vagrant distribution of the Asset Transfer application, a minimum of 10 GB of RAM and 8 core CPU are required.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/d07a6992382747b7e404d5b6937f86fa/href\">https://medium.com/media/d07a6992382747b7e404d5b6937f86fa/href</a></iframe><p>The BOX_BASE variable in the Vagrantfile specifies the operating system of the vms. For Ubuntu Server 20.04 LTS, the value of this variable is ubuntu/focal64.</p><pre>BOX_BASE = “ubuntu/focal64”</pre><p>The NODES variable in the Vagrantfile sets the vms list and their paramaters(ram,cpu,ip,hostname). The type parameter can be haproxy, nfs, and k8s.Kubernetes cluster is currently setting up a single node.If the Kubernetes cluster is desired to consist of multiple master and worker nodes, this NODES variable can be assigned as follows.</p><p>For 3 Master,3 Worker Node Setup</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/87bf16dd4764acc71ea16c89348847ef/href\">https://medium.com/media/87bf16dd4764acc71ea16c89348847ef/href</a></iframe><p>Vagrantfile supports multiple master and worker mode installation. It also supports virtualbox and hyperv as virtual machine provider.</p><p>2. Now lets create virtual machines.</p><pre>$ vagrant up</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*1TALnl6zmDvf3wylDNu62Q.png\" /></figure><p>After the Vagrant installation is completed, that is, the virtual machines are created and the configurations we have specified in the Vagrantfile are made, we can connect to the virtual machine we have installed with the <strong>vagrant ssh</strong> command.</p><h3>Dynamic NFS Provisioning in Kubernetes</h3><p>Kubernetes containerized storage allows people to develop <strong>data containers on-demand</strong>. The confidentiality of the data contained is maintained by providing automatic provisioning.</p><p>The storage method is at sky-high demand sprawling out far and wide among various verticals. Numerous applications are coming up for providing contented access to people.</p><p>One of the ways Kubernetes allow applications to access storage is the standard Network File Service (NFS) protocol.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1000/0*tddU2bUZzEVAQf0t.png\" /></figure><h3>NFS Server Installation Using Vagrant</h3><p>Let’s connect to the nfs server virtual machine with the <strong>vagrant ssh</strong> command.</p><pre>$ vagrant ssh nfsserver</pre><p>Let’s go to the directory where the nfs server installation scripts are located.This directory is the same as the deploy/setup/nfs-server-setup folder in the project. With Vagrant, this directory is synchronized to the virtual machine.</p><pre>$ <em>cd</em> /vagrant/setup/nfs-server-setup</pre><p>Let’s give the permission to execute.</p><pre>$ <em>sudo</em> chmod u+x *.sh</pre><p>Let’s install Nfs Server.</p><pre>$ <em>sudo</em> ./install_nfs.sh</pre><p>After the nfs server installation is complete, let’s run the script that creates the necessary directories for fabric files.</p><pre>$ <em>./create_fabric_dir.sh</em></pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*1_Wqr52F0NtVVkFPIgG3qQ.png\" /></figure><p>After the directories are created, let’s copy the fabric files to these directories.</p><pre>$ <em>./copy_fabricfiles.sh</em></pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Z6y6omk328DFub1EZjLNhQ.png\" /></figure><p>The nfs server path is set to /srv/kubedata by default. If you want to change this path, you need to change the NFS_PATH variable in install_nfs.sh, NFS_DIR variable in copy_fabricfiles.sh and NFS_DIR variable in create_fabric_dir.sh.In the project, these scripts are in the deploy/setup/nfs-server-setup folder.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/b2931a092a01ca2fe6a0fd0e9e698a83/href\">https://medium.com/media/b2931a092a01ca2fe6a0fd0e9e698a83/href</a></iframe><p>Fabric dosyaları srv/kubedata/fabricfiles patihne kopyalanmıştır.Aşağı komut ile kontrol edelim.</p><pre>$ ls /srv/kubedata/fabricfiles/</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*mBqNBVkf6oXRr-owi9tuOw.png\" /></figure><p>Nfs server installation completed successfully. Let’s exit the terminal of the virtual machine.</p><pre>$ exit</pre><h3>External Load Balancer in Kubernetes</h3><p>HAProxy is a free, very fast and reliable solution offering high availability, load balancing, and proxying for TCP and HTTP-based applications. It is particularly suited for very high traffic web sites and powers quite a number of the world’s most visited ones. Over the years it has become the de-facto standard opensource load balancer, is now shipped with most mainstream Linux distributions.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/350/1*aRJP7GH---H3jf957JxzIg.png\" /></figure><p>We will use HAProxy as a Kubernetes external loadbalancer. Requests to haproxy from port 80 will be balanced to the nginx ingress controller on the worker nodes.In addition, requests to haproxy from port 6443 will be balanced between kubernetes master nodes.</p><h3>HAProxy Installation Using Vagrant</h3><p>Let’s connect to the haproxy virtual machine with the <strong>vagrant ssh</strong> command.</p><pre>$ vagrant ssh haproxy</pre><p>Let’s go to the directory where the haproxy installation scripts are located.This directory is the same as the deploy/setup/haproxy-setup folder in the project. With Vagrant, this directory is synchronized to the virtual machine.</p><pre>$ <em>cd</em> /vagrant/setup/haproxy-setup</pre><p>Let’s give the permission to execute.</p><pre>$ <em>sudo</em> chmod u+x *.sh</pre><p>Let’s install haproxy</p><pre>$ <em>sudo</em> ./install_haproxy.sh</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*zRODx-jdZd-yOyYEWr4Rvg.png\" /></figure><p>Haproxy installation completed successfully. Let’s exit the terminal of the virtual machine.</p><pre>$ exit</pre><h3>Kubernetes Installation Using Vagrant</h3><p>We will install a bare metal kubernetes cluster using Kubespray.Kubespray is a composition of <a href=\"https://docs.ansible.com/\">Ansible</a> playbooks, <a href=\"https://github.com/kubernetes-sigs/kubespray/blob/master/docs/ansible.md\">inventory</a>, provisioning tools, and domain knowledge for generic OS/Kubernetes clusters configuration management tasks.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/551/1*07j4DZQ_8Y8DcQHUDOBGdQ.png\" /></figure><p>Let’s connect to the kubernetes master node virtual machine with the <strong>vagrant ssh</strong> command.</p><pre>$ vagrant ssh k8smaster</pre><p>Let’s go to the directory where the kubernetes installation scripts are located.This directory is the same as the deploy/setup/kubernetes-setup folder in the project. With Vagrant, this directory is synchronized to the virtual machine.</p><pre>$ <em>cd</em> /vagrant/setup/kubernetes-setup</pre><p>Let’s give the permission to execute.</p><pre>$ <em>sudo</em> chmod u+x *.sh</pre><p>By default, kubernetes is installed with metallb, metric server,helm,weavenet,nfs provisioner,cert manager option.If you want to change this option, you need to change the variables(METALLB_ENABLED,METRIC_SERVER_ENABLED,etc) in install_kubespray.sh and the variables(CERT_MANAGER_ENABLED,ISTIO_ENABLED,etc) in install-prereqs.sh.In the project, these scripts are in the deploy/setup/kubernetes-setup folder.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/93c595019998c4845f73568279d40418/href\">https://medium.com/media/93c595019998c4845f73568279d40418/href</a></iframe><h3>What is MetalLB?</h3><p>MetalLB is a load-balancer implementation for bare metal <a href=\"https://kubernetes.io/\">Kubernetes</a> clusters, using standard routing protocols.It is installed by default.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/165/1*uGHr9r2Lk_rnvQQyuL9B4Q.png\" /></figure><h3>What is Helm?</h3><p>Helm is a tool for managing Charts. Charts are packages of pre-configured Kubernetes resources.It is installed by default.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/165/1*Nkod4KY-P42hQ4QJAMOwSg.png\" /></figure><h3>What is Nginx Ingress Controller?</h3><p>Ingress Nginx is an Ingress controller for Kubernetes using <a href=\"https://www.nginx.org/\">NGINX</a> as a reverse proxy and load balancer.It is installed by default.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/165/1*FwKurDFszP01i85PmvHinQ.png\" /></figure><h3>What is Istio?</h3><p>Istio, the most popular service mesh implementation, was developed on top of Kubernetes and has a different niche in the cloud native application ecosystem than Kubernetes.It is not installed by default.If you want to change this option, you need to change the ISTIO_ENABLED variable in install-prereqs.sh.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/165/1*6A9IYK5xuvkLszlNejR4nw.png\" /></figure><p>Let’s install kubernetes</p><pre>$ <em>./install-prereqs.sh</em></pre><p>Kubernetes installation completed successfully. Finally, let’s add the kubernetes cluster to Lens IDE.</p><h3>What is Lens?</h3><p>Lens — The Kubernetes IDE (“Lens IDE”) is a distribution of the OpenLens repository with Team Lens specific customizations released under a traditional <a href=\"https://k8slens.dev/licenses/eula\">EULA</a>.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/225/1*JAGCKJ7DtT6Jp1HwfdNqFA.png\" /></figure><p>Lens IDE provides the full situational awareness for everything that runs in Kubernetes. It’s lowering the barrier of entry for people just getting started and radically improving productivity for people with more experience.</p><p>Lens IDE a standalone application for MacOS, Windows and Linux operating systems. You can download it free of charge for Windows, MacOS, and Linux from <a href=\"https://k8slens.dev/\">Lens IDE website</a>.</p><h3>Add Kubernetes Cluster to Lens IDE</h3><p>Let’s copy the contents of the kubectl configuration file.The default kubectl configuration file is located at <strong>~/.</strong> <strong>kube/config</strong> and is referred to as the kubeconfig file.</p><pre>$ cat ${HOME}/.kube/config</pre><p>Let’s open the Lens IDE, let’s go to Clusters from the catalog. Let’s choose Add from kubeconfig.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*p9fi6IoBQ7Ejkst7dT6l6A.png\" /></figure><p>Let’s paste the contents of the kubectl configuration file into the window that opens.Let’s replace https://elb.kub:6443 with <a href=\"https://192.168.12.10:6443.\">https://192.168.12.10:6443.</a> Finally, click Add Cluster.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*66Z_BzpNP9xSmcU79O89ww.png\" /></figure><p>Kubernetes Cluster has been added successfully.let’s go to Clusters from the catalog. Let’s select kubernetes-admin@cluster.local and click connect.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*KOCWzb4mQgxYwIxU-hLIAg.png\" /></figure><p>Kubernetes successfully connected to the cluster. When we select Storage Class under Storage, there is a storage class called nfs-client.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*NDajHMjten2vXHPtmFZdHg.png\" /></figure><p>My article ends here. In general,I explained introduction and installation of kubernetes cluster and kubernetes tools, nfs server and haproxy using vagrant.</p><p>See you in the next articles.</p><h3>Project Links</h3><p>Spring Boot Hlf Starter Project details and installation can be accessed via <a href=\"https://github.com/susimsek/spring-boot-hlf-starter\">this link</a>.</p><p>Asset Transfer Project details and installation can be accessed via <a href=\"https://github.com/susimsek/spring-boot-hlf-starter\">this link</a></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=ddce8d25858d\" width=\"1\" height=\"1\" alt=\"\">",
      "content:encodedSnippet": "Spring Boot Fullstack Blockchain Application With Hyperledger Fabric running on Kubernetes (Part 2) — Kubernetes Cluster Setup\n\nHello everyone, through this article series we will the Hyperledger Fabric integration with Spring Boot.In this article, we will look into Kubernetes Cluster, Haproxy,Nfs server introductions and installations using Vagrant.\nOther articles on Hyperledger Fabric integration with Spring Boot can be accessed from the links below.\nPart 1 — Introduction\nPart 2 — Kubernetes Cluster Setup\nPart 3 — Fabric CA Server\nPart 4 — Generating Certificates and Artifacts\nPart 5 — Kafka\nPart 6 — Orderer\nWhat is Kubernetes?\nKubernetes is a production-grade open-source container orchestration tool developed by Google to help you manage the containerized/dockerized applications supporting multiple deployment environments like On-premise, cloud, or virtual machines.\n\nThe name Kubernetes originates from Greek, meaning helmsman or pilot. K8s as an abbreviation results from counting the eight letters between the “K” and the “s”. Google open-sourced the Kubernetes project in 2014.\nProject details of Kubernetes can be accessed via this link\nWhat is Vagrant?\nVagrant is a tool that allows us to create and manage virtual machines developed by HashiCorp. With Vagrant, we can easily configure the virtual machine and manage many machines easily.\n\nVagrant automates the infrastructure of your virtual machines by using a single file Vagrantfile.\nProject details of Vagrant can be accessed via this link\nVagrant Installation\nFor vagrant installation, virtualbox or hyperv must be installed on your local computer.We’ll be using the VirtualBox provider, which is the default provider for Vagrant.The Vagranfile I created supports hyperv and virtualbox providers.\n\nYou can download Vagrant from https://www.vagrantup.com/ and VirtualBox from https://www.virtualbox.org/wiki/Downloads and install .\nAfter installing you can confirm the vagrant version with installation.\nInstalling the Guest Addition Plugin for Vagrant\nGuest Addition is essentially for being able to unleash Vagrant’s full potential, meaning it is important that it is installed and kept updated. However, ensuring your Vagrant boxes are always running the latest version of Guest Additions can be a time-consuming task, stealing away crucial cycles that could be put to better use.\nWe can install this plugin with the following command.\nvagrant plugin install vagrant-vbguest\nVagrantfile\nVagrantfile is a file in which the necessary information (ram, cpu, vm_provider) is written for configuring and orchestrating virtual machines. Ruby language is used to make this configuration.\nVirtual Machine Installations Using Vagrant\nLet’s open the project we downloaded from this link and go to the directory where the Vagrantfile is located.\n$ cd deploy\nWe will set up 3 virtual machines for nfs server, ha proxy and kubernetes cluster using vagrant.\nThe hardware requirements and ips of the virtual machine are shown in the table below.\nhttps://medium.com/media/5be99320a54d3a8150d624b011e732bc/href\nVagrant distribution of the Asset Transfer application, a minimum of 10 GB of RAM and 8 core CPU are required.\nhttps://medium.com/media/d07a6992382747b7e404d5b6937f86fa/href\nThe BOX_BASE variable in the Vagrantfile specifies the operating system of the vms. For Ubuntu Server 20.04 LTS, the value of this variable is ubuntu/focal64.\nBOX_BASE = “ubuntu/focal64”\nThe NODES variable in the Vagrantfile sets the vms list and their paramaters(ram,cpu,ip,hostname). The type parameter can be haproxy, nfs, and k8s.Kubernetes cluster is currently setting up a single node.If the Kubernetes cluster is desired to consist of multiple master and worker nodes, this NODES variable can be assigned as follows.\nFor 3 Master,3 Worker Node Setup\nhttps://medium.com/media/87bf16dd4764acc71ea16c89348847ef/href\nVagrantfile supports multiple master and worker mode installation. It also supports virtualbox and hyperv as virtual machine provider.\n2. Now lets create virtual machines.\n$ vagrant up\n\nAfter the Vagrant installation is completed, that is, the virtual machines are created and the configurations we have specified in the Vagrantfile are made, we can connect to the virtual machine we have installed with the vagrant ssh command.\nDynamic NFS Provisioning in Kubernetes\nKubernetes containerized storage allows people to develop data containers on-demand. The confidentiality of the data contained is maintained by providing automatic provisioning.\nThe storage method is at sky-high demand sprawling out far and wide among various verticals. Numerous applications are coming up for providing contented access to people.\nOne of the ways Kubernetes allow applications to access storage is the standard Network File Service (NFS) protocol.\n\nNFS Server Installation Using Vagrant\nLet’s connect to the nfs server virtual machine with the vagrant ssh command.\n$ vagrant ssh nfsserver\nLet’s go to the directory where the nfs server installation scripts are located.This directory is the same as the deploy/setup/nfs-server-setup folder in the project. With Vagrant, this directory is synchronized to the virtual machine.\n$ cd /vagrant/setup/nfs-server-setup\nLet’s give the permission to execute.\n$ sudo chmod u+x *.sh\nLet’s install Nfs Server.\n$ sudo ./install_nfs.sh\nAfter the nfs server installation is complete, let’s run the script that creates the necessary directories for fabric files.\n$ ./create_fabric_dir.sh\n\nAfter the directories are created, let’s copy the fabric files to these directories.\n$ ./copy_fabricfiles.sh\n\nThe nfs server path is set to /srv/kubedata by default. If you want to change this path, you need to change the NFS_PATH variable in install_nfs.sh, NFS_DIR variable in copy_fabricfiles.sh and NFS_DIR variable in create_fabric_dir.sh.In the project, these scripts are in the deploy/setup/nfs-server-setup folder.\nhttps://medium.com/media/b2931a092a01ca2fe6a0fd0e9e698a83/href\nFabric dosyaları srv/kubedata/fabricfiles patihne kopyalanmıştır.Aşağı komut ile kontrol edelim.\n$ ls /srv/kubedata/fabricfiles/\n\nNfs server installation completed successfully. Let’s exit the terminal of the virtual machine.\n$ exit\nExternal Load Balancer in Kubernetes\nHAProxy is a free, very fast and reliable solution offering high availability, load balancing, and proxying for TCP and HTTP-based applications. It is particularly suited for very high traffic web sites and powers quite a number of the world’s most visited ones. Over the years it has become the de-facto standard opensource load balancer, is now shipped with most mainstream Linux distributions.\n\nWe will use HAProxy as a Kubernetes external loadbalancer. Requests to haproxy from port 80 will be balanced to the nginx ingress controller on the worker nodes.In addition, requests to haproxy from port 6443 will be balanced between kubernetes master nodes.\nHAProxy Installation Using Vagrant\nLet’s connect to the haproxy virtual machine with the vagrant ssh command.\n$ vagrant ssh haproxy\nLet’s go to the directory where the haproxy installation scripts are located.This directory is the same as the deploy/setup/haproxy-setup folder in the project. With Vagrant, this directory is synchronized to the virtual machine.\n$ cd /vagrant/setup/haproxy-setup\nLet’s give the permission to execute.\n$ sudo chmod u+x *.sh\nLet’s install haproxy\n$ sudo ./install_haproxy.sh\n\nHaproxy installation completed successfully. Let’s exit the terminal of the virtual machine.\n$ exit\nKubernetes Installation Using Vagrant\nWe will install a bare metal kubernetes cluster using Kubespray.Kubespray is a composition of Ansible playbooks, inventory, provisioning tools, and domain knowledge for generic OS/Kubernetes clusters configuration management tasks.\n\nLet’s connect to the kubernetes master node virtual machine with the vagrant ssh command.\n$ vagrant ssh k8smaster\nLet’s go to the directory where the kubernetes installation scripts are located.This directory is the same as the deploy/setup/kubernetes-setup folder in the project. With Vagrant, this directory is synchronized to the virtual machine.\n$ cd /vagrant/setup/kubernetes-setup\nLet’s give the permission to execute.\n$ sudo chmod u+x *.sh\nBy default, kubernetes is installed with metallb, metric server,helm,weavenet,nfs provisioner,cert manager option.If you want to change this option, you need to change the variables(METALLB_ENABLED,METRIC_SERVER_ENABLED,etc) in install_kubespray.sh and the variables(CERT_MANAGER_ENABLED,ISTIO_ENABLED,etc) in install-prereqs.sh.In the project, these scripts are in the deploy/setup/kubernetes-setup folder.\nhttps://medium.com/media/93c595019998c4845f73568279d40418/href\nWhat is MetalLB?\nMetalLB is a load-balancer implementation for bare metal Kubernetes clusters, using standard routing protocols.It is installed by default.\n\nWhat is Helm?\nHelm is a tool for managing Charts. Charts are packages of pre-configured Kubernetes resources.It is installed by default.\n\nWhat is Nginx Ingress Controller?\nIngress Nginx is an Ingress controller for Kubernetes using NGINX as a reverse proxy and load balancer.It is installed by default.\n\nWhat is Istio?\nIstio, the most popular service mesh implementation, was developed on top of Kubernetes and has a different niche in the cloud native application ecosystem than Kubernetes.It is not installed by default.If you want to change this option, you need to change the ISTIO_ENABLED variable in install-prereqs.sh.\n\nLet’s install kubernetes\n$ ./install-prereqs.sh\nKubernetes installation completed successfully. Finally, let’s add the kubernetes cluster to Lens IDE.\nWhat is Lens?\nLens — The Kubernetes IDE (“Lens IDE”) is a distribution of the OpenLens repository with Team Lens specific customizations released under a traditional EULA.\n\nLens IDE provides the full situational awareness for everything that runs in Kubernetes. It’s lowering the barrier of entry for people just getting started and radically improving productivity for people with more experience.\nLens IDE a standalone application for MacOS, Windows and Linux operating systems. You can download it free of charge for Windows, MacOS, and Linux from Lens IDE website.\nAdd Kubernetes Cluster to Lens IDE\nLet’s copy the contents of the kubectl configuration file.The default kubectl configuration file is located at ~/. kube/config and is referred to as the kubeconfig file.\n$ cat ${HOME}/.kube/config\nLet’s open the Lens IDE, let’s go to Clusters from the catalog. Let’s choose Add from kubeconfig.\n\nLet’s paste the contents of the kubectl configuration file into the window that opens.Let’s replace https://elb.kub:6443 with https://192.168.12.10:6443. Finally, click Add Cluster.\n\nKubernetes Cluster has been added successfully.let’s go to Clusters from the catalog. Let’s select kubernetes-admin@cluster.local and click connect.\n\nKubernetes successfully connected to the cluster. When we select Storage Class under Storage, there is a storage class called nfs-client.\n\nMy article ends here. In general,I explained introduction and installation of kubernetes cluster and kubernetes tools, nfs server and haproxy using vagrant.\nSee you in the next articles.\nProject Links\nSpring Boot Hlf Starter Project details and installation can be accessed via this link.\nAsset Transfer Project details and installation can be accessed via this link",
      "dc:creator": "Şuayb Şimşek",
      "guid": "https://medium.com/p/ddce8d25858d",
      "categories": ["blockchain", "kubernetes", "spring-boot", "vagrant", "hyperledger-fabric"],
      "isoDate": "2021-10-31T18:04:46.000Z"
    },
    {
      "creator": "Şuayb Şimşek",
      "title": "Spring Boot Fullstack Blockchain Application With Hyperledger Fabric running on Kubernetes (Part 1)",
      "link": "https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-1-c8f7c87d60eb?source=rss-bda589f2335a------2",
      "pubDate": "Sun, 17 Oct 2021 17:59:32 GMT",
      "content:encoded": "<h3>Spring Boot Fullstack Blockchain Application With Hyperledger Fabric running on Kubernetes (Part 1) — Introduction</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*R9q34HnrsGeQiExwr54NYA.png\" /></figure><p>Hello everyone, through this article series we will the Hyperledger Fabric integration with Spring Boot.In this article, we will look into <em>Spring Boot Hlf Starter i</em>ntroduction<em> , Asset Transfer Application i</em>ntroduction<em>,Blockchain and Hyperledger Fabric concept</em>.In other articles we will implement Asset Transfer Application step by step and deploy this app on Kubernetes.In the next articles,I will also explain Hyperledger explorer, grafana integration on Kubernetes.</p><p>Other articles on Hyperledger Fabric integration with Spring Boot can be accessed from the links below.</p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-1-c8f7c87d60eb\">Part 1 — Introduction</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-ddce8d25858d\">Part 2 — Kubernetes Cluster Setup</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-3-7eb2fc75ba60\">Part 3 — Fabric CA Server</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-4-d6583a57153c\">Part 4 — Generating Certificates and Artifacts</a></p><p><a href=\"https://suaybsimsek58.medium.com/1723502770e7\">Part 5 — Kafka</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-6-b82662ada0a4\">Part 6 — Orderer</a></p><h3>What is Asset Transfer App?</h3><p>Asset Transfer Application is a blockchain based fullstack application that allows you to create and transfer an asset by putting data on the ledger and retrieving it.The chaincode part of the application is written in Go, the backend part is written in Spring boot, and the frontend part is written in Angular.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*XI1aWY2GwvhnYSXyb2Oipg.png\" /></figure><p>Project details and installation of Asset Transfer Application can be accessed via<a href=\"https://github.com/susimsek/spring-boot-hlf-k8s-fullstack-blockchain-app\"> this link</a></p><h3>What is Spring Boot Hlf Starter?</h3><p>Spring Boot Hlf Starter library is a spring boot starter library I wrote.This library provides an easy way to get your Spring boot application using Hyperledger Fabric Gateway SDK v2.2 up and running quickly.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1000/0*E_7xA9se2QPU_EgU.png\" /></figure><p>Spring Native provides support for compiling Spring applications to native executables using the <a href=\"https://www.graalvm.org/\">GraalVM</a> <a href=\"https://www.graalvm.org/reference-manual/native-image/\">native-image</a> compiler.I’m thinking of adding spring native support in the next versions of the project.</p><p>Project details and installation can be accessed via <a href=\"https://github.com/susimsek/spring-boot-hlf-starter\">this link</a></p><h3>What is Blockchain?</h3><p>It was made with the work done by Stuart Haber and Scott Stornetta, who were in the cryptographic development phase of blockchain technology in the 1990s. Starting from the concept of the “hash tree”, which was classified in a way in the 1970s, two experts have managed to approach its definition with the modern blockchain, in full terms. Not Satoshi Nakamoto, but Haber and Stornetta.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Zmpq5aDS3HceMvlgxJep7Q.png\" /></figure><p>The first Blockchain in its current sense is as old as Bitcoin, the first cryptocurrency in history. When the calendars showed October 31, 2008, the white paper document called “Bitcoin: Peer-to-Peer Electronic Cash System” published by a person or group named Satoshi Nakamoto stated that the BTC infrastructure was completely based on blockchain technology.</p><h3>How Blockchain Works?</h3><p>Blockchain, which has a very wide working area, is used. Blockchain can be made from authorized and unofficial use cases, from usage areas, from contracts to notary transactions.</p><p>The most important features underlying the working principle of blockchain technology; it is anonymous, distributed, decentralized and public, yet unbreakable and unhackable. The fact that an information record chain is both accessible and unbreakable to everyone causes everyone, especially computer scientists, to approach this technology with admiration. There is a simple logic behind the fact that the records committed to the blocks are unbreakable and unchangeable: All blocks in this registry, which has billions of copies, must be changed in order to corrupt the blocks. It is almost impossible to make such an intervention.</p><h3>What is Ethereum?</h3><p>Ethereum is an open source distributed public blockchain network. It allows decentralized apps to be built on it with the help of Smart Contract functionality.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/700/0*t_TrUui-wwvBOrlx.png\" /></figure><p>Vitalik Buterin developed Ethereum as an extension to the original core blockchain concept. He improvised Bitcoin’s protocols to support applications beyond currency issuance. Its major breakthrough is the ability to easily write and deploy Smart Contracts. These are actually bits of code that are executed on the network. Hence, this platform could help developers to write programs for building decentralized organisations.</p><h3>What is Hyperledger Fabric?</h3><p>Hyperledger Fabric is a framework for developing Blockchain-based solutions for the enterprise. It is open-source and under the umbrella of the Linux Foundation.It is a private chain, thus super-helpful for enterprises since you don’t want to put your transactions for public display.The framework has a very sophisticated module architecture which allows thedeveloper to design the blockchain network with security, scalability, confidentiality and with high performance.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*z2REkO4RIp73p4wESu_0IA.png\" /></figure><h3>Hyperledger Fabric vs Ethereum</h3><p>The subject of this article is not etherium. so I won’t explain ethereum in detail. The differences between Ethereum and Hyperledger Fabric are explained in the table below.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/938/1*1MxxMQNC8E8LMTRA6URwKw.png\" /></figure><h3>What is DLT?</h3><p>Any Blockchain works on top of a DLT, or Distributed Ledger Technology. DLTs are protocols to store all transactions that take place in a network. In a Blockchain protocol, the network is <strong>decentralized</strong><em>, </em>which means that the transaction history is replicated across all the participating nodes.</p><h3>The Architecture of Fabric</h3><p>Fabric is a permissioned Blockchain, it has certain protocols you might not have heard of before, it’s time to take a look at them.</p><h3>Ledger</h3><p>In Hyperledger fabric ledger is a transaction history log that contains the state of each transaction committed to the ledger. The ledger consists of two parts World State, Blockchain.</p><p><strong>World State</strong> — It’s a database that contains the updated records for a transaction. The World state gets more comfortable for the programmer to check the record is stored accurately into the ledger or not.Hyperledger fabric supports LevelDB and CouchDB as the state database.I used CouchDB in Asset Transfer Application.</p><p><strong>Blockchain</strong> — is a set of sequential blocks containing the sequence of transactions carried out in the world state. Hyperledger Fabric uses an ordering service for transaction sequencing within the blocks.</p><h3>Transaction</h3><p>The role of a transaction is to store the state of an object.</p><h3>Blocks</h3><p>Blocks In Blockchain the first block is called the genesis block. This initial block doesn’t contain any user transaction but stores the channel configuration in the network.</p><h3>Peers</h3><p>Peers are a fundamental unit of any distributed model. Peers are essentially nodes, but in public blockchain networks like Bitcoin or Ethereum, all peers are equal. In a private Blockchain, all peers are <strong>not </strong>equal.</p><p>In Asset transfer application, each organization has only 1 peer.</p><h3>The Consensus Mechanism</h3><p>The point of Consensus is validating the correctness of a Block, and that it adheres to the policies set by the Chaincode. There are 2 types of Consensus Mechanisms in Fabric: <strong>Voting </strong>and <strong>Lottery</strong>. Voting is the more accepted method in enterprise right now. You, as a developer, must develop your Fabric-based enterprise application on the idea that there is partial trust in a network. I hope Voting and Lottery are clear from their nomenclature as to what they do, if not, refer to the official docs.</p><p>Consensus in Fabric is broken down in three phases:</p><ol><li><strong>Endorsement</strong>: The participants will have to endorse a transaction.</li><li><strong>Ordering</strong>: This phase will agree to the order of commitment to the ledger.</li><li><strong>Validation</strong>: Validates structure and order.</li></ol><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*qRymZnyZF7HqFeqd.jpeg\" /></figure><h3>Chaincode</h3><p>Chaincode is the software that defines the asset. Assets are basically a key-value pair, where Asset definitions enable the exchange of almost anything with a monetary value over the network. Chaincode is a program that runs in a secured Docker container. Chaincode is the Smart Contract of Fabric in the sense it runs the business logic of the network. You can write ChainCodes in Go, NodeJS, or Java.The chaincode part of Asset transfer application is written in Go.</p><p>Asset transfer application has a single chaincode called basic.</p><h3>Organization</h3><p>This is a set of groups of members under a single MSP. An organization can be related to a big multi corporation group or small coffee shop. The naming convention for organization MSP can be derived as Org1.MSP, here organization is “Org1”. And also an organization can have many MSPs based on its requirements.</p><p>Asset transfer application has 3 organizations.Org1,Org2,Org3.</p><h3>Membership Service Provider (MSP)</h3><p>For clients to participate in a private network, they need credentials to be authentic. MSPs are an abstract component that provides credentials to the clients. Clients use these credentials to authenticate their transactions, and peers use these credentials to authenticate transaction processing results (endorsements).</p><h3>Channels</h3><p>Hyperledger Fabric has a technique that allows the organization to join multiple blockchain networks through a channel. So, multiple organizations can communicate or participate in many networks implementing various channels.Asset transfer application has a single channel called mychannel.</p><h3>What is Hyperledger Explorer?</h3><p>Hyperledger Explorer is a simple, powerful, easy-to-use, well maintained, open source utility to browse activity on the underlying blockchain network. Users have the ability to configure and build Hyperledger Explorer on MacOS and Ubuntu.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*xtnKvzLqttYwzadI.png\" /></figure><p>In the next articles, I will explain the integration of Hyperledger Fabric with Hyperledger Explorer.</p><p>My article ends here. In general, I tried to give theoretical information about Hyperledger Fabric,Hyperledger Explorer and Blockchain.</p><p>See you in the next articles.</p><h3>Project Links</h3><p>Spring Boot Hlf Starter Project details and installation can be accessed via <a href=\"https://github.com/susimsek/spring-boot-hlf-starter\">this link</a>.</p><p>Asset Transfer Project details and installation can be accessed via <a href=\"https://github.com/susimsek/spring-boot-hlf-starter\">this link</a></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=c8f7c87d60eb\" width=\"1\" height=\"1\" alt=\"\">",
      "content:encodedSnippet": "Spring Boot Fullstack Blockchain Application With Hyperledger Fabric running on Kubernetes (Part 1) — Introduction\n\nHello everyone, through this article series we will the Hyperledger Fabric integration with Spring Boot.In this article, we will look into Spring Boot Hlf Starter introduction , Asset Transfer Application introduction,Blockchain and Hyperledger Fabric concept.In other articles we will implement Asset Transfer Application step by step and deploy this app on Kubernetes.In the next articles,I will also explain Hyperledger explorer, grafana integration on Kubernetes.\nOther articles on Hyperledger Fabric integration with Spring Boot can be accessed from the links below.\nPart 1 — Introduction\nPart 2 — Kubernetes Cluster Setup\nPart 3 — Fabric CA Server\nPart 4 — Generating Certificates and Artifacts\nPart 5 — Kafka\nPart 6 — Orderer\nWhat is Asset Transfer App?\nAsset Transfer Application is a blockchain based fullstack application that allows you to create and transfer an asset by putting data on the ledger and retrieving it.The chaincode part of the application is written in Go, the backend part is written in Spring boot, and the frontend part is written in Angular.\n\nProject details and installation of Asset Transfer Application can be accessed via this link\nWhat is Spring Boot Hlf Starter?\nSpring Boot Hlf Starter library is a spring boot starter library I wrote.This library provides an easy way to get your Spring boot application using Hyperledger Fabric Gateway SDK v2.2 up and running quickly.\n\nSpring Native provides support for compiling Spring applications to native executables using the GraalVM native-image compiler.I’m thinking of adding spring native support in the next versions of the project.\nProject details and installation can be accessed via this link\nWhat is Blockchain?\nIt was made with the work done by Stuart Haber and Scott Stornetta, who were in the cryptographic development phase of blockchain technology in the 1990s. Starting from the concept of the “hash tree”, which was classified in a way in the 1970s, two experts have managed to approach its definition with the modern blockchain, in full terms. Not Satoshi Nakamoto, but Haber and Stornetta.\n\nThe first Blockchain in its current sense is as old as Bitcoin, the first cryptocurrency in history. When the calendars showed October 31, 2008, the white paper document called “Bitcoin: Peer-to-Peer Electronic Cash System” published by a person or group named Satoshi Nakamoto stated that the BTC infrastructure was completely based on blockchain technology.\nHow Blockchain Works?\nBlockchain, which has a very wide working area, is used. Blockchain can be made from authorized and unofficial use cases, from usage areas, from contracts to notary transactions.\nThe most important features underlying the working principle of blockchain technology; it is anonymous, distributed, decentralized and public, yet unbreakable and unhackable. The fact that an information record chain is both accessible and unbreakable to everyone causes everyone, especially computer scientists, to approach this technology with admiration. There is a simple logic behind the fact that the records committed to the blocks are unbreakable and unchangeable: All blocks in this registry, which has billions of copies, must be changed in order to corrupt the blocks. It is almost impossible to make such an intervention.\nWhat is Ethereum?\nEthereum is an open source distributed public blockchain network. It allows decentralized apps to be built on it with the help of Smart Contract functionality.\n\nVitalik Buterin developed Ethereum as an extension to the original core blockchain concept. He improvised Bitcoin’s protocols to support applications beyond currency issuance. Its major breakthrough is the ability to easily write and deploy Smart Contracts. These are actually bits of code that are executed on the network. Hence, this platform could help developers to write programs for building decentralized organisations.\nWhat is Hyperledger Fabric?\nHyperledger Fabric is a framework for developing Blockchain-based solutions for the enterprise. It is open-source and under the umbrella of the Linux Foundation.It is a private chain, thus super-helpful for enterprises since you don’t want to put your transactions for public display.The framework has a very sophisticated module architecture which allows thedeveloper to design the blockchain network with security, scalability, confidentiality and with high performance.\n\nHyperledger Fabric vs Ethereum\nThe subject of this article is not etherium. so I won’t explain ethereum in detail. The differences between Ethereum and Hyperledger Fabric are explained in the table below.\n\nWhat is DLT?\nAny Blockchain works on top of a DLT, or Distributed Ledger Technology. DLTs are protocols to store all transactions that take place in a network. In a Blockchain protocol, the network is decentralized, which means that the transaction history is replicated across all the participating nodes.\nThe Architecture of Fabric\nFabric is a permissioned Blockchain, it has certain protocols you might not have heard of before, it’s time to take a look at them.\nLedger\nIn Hyperledger fabric ledger is a transaction history log that contains the state of each transaction committed to the ledger. The ledger consists of two parts World State, Blockchain.\nWorld State — It’s a database that contains the updated records for a transaction. The World state gets more comfortable for the programmer to check the record is stored accurately into the ledger or not.Hyperledger fabric supports LevelDB and CouchDB as the state database.I used CouchDB in Asset Transfer Application.\nBlockchain — is a set of sequential blocks containing the sequence of transactions carried out in the world state. Hyperledger Fabric uses an ordering service for transaction sequencing within the blocks.\nTransaction\nThe role of a transaction is to store the state of an object.\nBlocks\nBlocks In Blockchain the first block is called the genesis block. This initial block doesn’t contain any user transaction but stores the channel configuration in the network.\nPeers\nPeers are a fundamental unit of any distributed model. Peers are essentially nodes, but in public blockchain networks like Bitcoin or Ethereum, all peers are equal. In a private Blockchain, all peers are not equal.\nIn Asset transfer application, each organization has only 1 peer.\nThe Consensus Mechanism\nThe point of Consensus is validating the correctness of a Block, and that it adheres to the policies set by the Chaincode. There are 2 types of Consensus Mechanisms in Fabric: Voting and Lottery. Voting is the more accepted method in enterprise right now. You, as a developer, must develop your Fabric-based enterprise application on the idea that there is partial trust in a network. I hope Voting and Lottery are clear from their nomenclature as to what they do, if not, refer to the official docs.\nConsensus in Fabric is broken down in three phases:\n\nEndorsement: The participants will have to endorse a transaction.\nOrdering: This phase will agree to the order of commitment to the ledger.\nValidation: Validates structure and order.\n\nChaincode\nChaincode is the software that defines the asset. Assets are basically a key-value pair, where Asset definitions enable the exchange of almost anything with a monetary value over the network. Chaincode is a program that runs in a secured Docker container. Chaincode is the Smart Contract of Fabric in the sense it runs the business logic of the network. You can write ChainCodes in Go, NodeJS, or Java.The chaincode part of Asset transfer application is written in Go.\nAsset transfer application has a single chaincode called basic.\nOrganization\nThis is a set of groups of members under a single MSP. An organization can be related to a big multi corporation group or small coffee shop. The naming convention for organization MSP can be derived as Org1.MSP, here organization is “Org1”. And also an organization can have many MSPs based on its requirements.\nAsset transfer application has 3 organizations.Org1,Org2,Org3.\nMembership Service Provider (MSP)\nFor clients to participate in a private network, they need credentials to be authentic. MSPs are an abstract component that provides credentials to the clients. Clients use these credentials to authenticate their transactions, and peers use these credentials to authenticate transaction processing results (endorsements).\nChannels\nHyperledger Fabric has a technique that allows the organization to join multiple blockchain networks through a channel. So, multiple organizations can communicate or participate in many networks implementing various channels.Asset transfer application has a single channel called mychannel.\nWhat is Hyperledger Explorer?\nHyperledger Explorer is a simple, powerful, easy-to-use, well maintained, open source utility to browse activity on the underlying blockchain network. Users have the ability to configure and build Hyperledger Explorer on MacOS and Ubuntu.\n\nIn the next articles, I will explain the integration of Hyperledger Fabric with Hyperledger Explorer.\nMy article ends here. In general, I tried to give theoretical information about Hyperledger Fabric,Hyperledger Explorer and Blockchain.\nSee you in the next articles.\nProject Links\nSpring Boot Hlf Starter Project details and installation can be accessed via this link.\nAsset Transfer Project details and installation can be accessed via this link",
      "dc:creator": "Şuayb Şimşek",
      "guid": "https://medium.com/p/c8f7c87d60eb",
      "categories": ["hyperledger-fabric", "blockchain", "spring", "spring-boot", "hyperledger-explorer"],
      "isoDate": "2021-10-17T17:59:32.000Z"
    }
  ],
  "feedUrl": "https://medium.com/@suaybsimsek58/feed",
  "image": {
    "link": "https://medium.com/@suaybsimsek58?source=rss-bda589f2335a------2",
    "url": "https://cdn-images-1.medium.com/fit/c/150/150/1*lX-xeCmafVAFHZQp-zXGVQ.png",
    "title": "Stories by Şuayb Şimşek on Medium"
  },
  "paginationLinks": {
    "self": "https://medium.com/@suaybsimsek58/feed"
  },
  "title": "Stories by Şuayb Şimşek on Medium",
  "description": "Stories by Şuayb Şimşek on Medium",
  "webMaster": "yourfriends@medium.com",
  "generator": "Medium",
  "link": "https://medium.com/@suaybsimsek58?source=rss-bda589f2335a------2",
  "lastBuildDate": "Thu, 22 May 2025 00:06:45 GMT"
}
