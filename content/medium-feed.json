{
  "items": [
    {
      "creator": "Åuayb ÅimÅŸek",
      "title": "Spring Boot GraalVM Native Image",
      "link": "https://medium.com/yapi-kredi-teknoloji/spring-boot-graalvm-native-image-3703a3b9fd29?source=rss-bda589f2335a------2",
      "pubDate": "Tue, 22 Apr 2025 08:02:29 GMT",
      "content:encoded": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*07CXmp711t--05EF398hwQ.jpeg\" /></figure><h3>ğŸ“– Introduction</h3><p>Java applications often suffer from long startup times and high memory consumption, making modern software development more challenging. However, <strong>GraalVM Native Image</strong> offers a groundbreaking solution by significantly improving the performance of Spring Boot applications.</p><p>In this article, we will explore <a href=\"https://github.com/susimsek/spring-boot-graalvm-native-example\">my GitHub example project</a> and demonstrate how to develop <strong>high-performance and lightweight applications</strong> using <strong>Java 21, Spring Boot 3.4, and GraalVM Native Image</strong>.Â ğŸš€</p><h3>ğŸ“Œ What is GraalVM and Why Should You UseÂ It?</h3><p>GraalVM is a <strong>high-performance JIT (Just-In-Time) and AOT (Ahead-of-Time) compiler</strong> that enables Java applications to be compiled into nativeÂ code.</p><h3>Key Advantages ofÂ GraalVM:</h3><p>âœ… <strong>Instant startup time</strong>â€Šâ€”â€ŠRuns natively without requiring a JVM!<br>âœ… <strong>Lower memory consumption</strong>â€Šâ€”â€ŠUses significantly less RAM compared to the traditional JVM.<br>âœ… <strong>Smaller Docker containers</strong>â€Šâ€”â€ŠIdeal for cloud-native architectures.<br>âœ… <strong>Enhanced security</strong>â€Šâ€”â€ŠEliminates unnecessary runtime components, reducing the attackÂ surface.</p><h3>ğŸ“Œ JVM vs. GraalVM Native Image Comparison</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*o3bhQC_W4wL4IADTLfWb6w.png\" /></figure><h3>âš™ï¸ Getting Started with Spring Boot and GraalVM NativeÂ Image</h3><h3>1ï¸âƒ£ Set Up Your Development Environment</h3><p>Before you begin, ensure you have the following installed:</p><p>ğŸ”¹ <strong>Java 21</strong><br>ğŸ”¹ <strong>GraalVM 22.3+</strong> (with Native Image support)<br>ğŸ”¹ <strong>UPX</strong> (optional, for compressing the executable)</p><h3>2ï¸âƒ£ Running theÂ Project</h3><p>First, clone the repository fromÂ GitHub:</p><pre>git clone https://github.com/susimsek/spring-boot-graalvm-native-example.git<br> cd spring-boot-graalvm-native-example<br> ./mvnw spring-boot:run</pre><p>The application will be available at <a href=\"http://localhost:8080/\"><strong>http://localhost:8080</strong></a>.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*MbuoYAEuIaXN2aBb8a9oig.png\" /></figure><blockquote><strong><em>Test theÂ API:</em></strong></blockquote><blockquote><em>curl -X GET </em><a href=\"http://localhost:8080/api/v1/hello\"><em>http://localhost:8080/api/v1/hello</em></a></blockquote><blockquote><strong><em>Response:</em></strong></blockquote><blockquote><em>{ &quot;message&quot;: &quot;Hello, GraalVM Native Image!&quot;Â }</em></blockquote><h3>ğŸ— Building Your Spring Boot Application as a GraalVM NativeÂ Image</h3><p>To compile your Spring Boot project into a native binary, follow theseÂ steps:</p><h3>Compile a NativeÂ Image</h3><pre>./mvnw native:compile -B -ntp -Pnative,prod -DskipTests</pre><p>This will generate a <strong>target/native-executable</strong> binary.</p><p><strong>Optionally, compress the executable usingÂ UPX:</strong></p><pre>upx --ultra-brute --lzma target/native-executable</pre><p>UPX (Ultimate Packer for eXecutables) is a powerful tool that compresses executables to save disk space and reduce deployment size. <strong>Compressed binaries</strong> function identically to their uncompressed counterparts but occupy lessÂ storage.</p><p><strong>Advantages of UPX:</strong><br>âœ… <strong>Smaller file size</strong>â€Šâ€”â€ŠReduces executable size by 50â€“80%.<br>âœ… <strong>No performance loss</strong>â€Šâ€”â€ŠCompressed binaries execute as normal.<br>âœ… <strong>Efficient execution</strong>â€Šâ€”â€ŠMaintains CPU performance while conserving diskÂ space.</p><blockquote><strong><em>Example:</em></strong><em> If </em><em>target/native-executable is 140MB, using UPX can shrink it toÂ </em><strong><em>30-35MB</em></strong><em>.</em></blockquote><h3>ğŸ³ Deploying withÂ Docker</h3><p>A GraalVM-generated <strong>native executable</strong> can be easily containerized usingÂ Docker.</p><h3>Build the DockerÂ Image</h3><pre>docker build -t spring-boot-graalvm-samples .</pre><h3>Run the Docker Container</h3><pre>docker run -d -p 8080:8080 spring-boot-graalvm-samples</pre><h3>ğŸš€ Deploying to Kubernetes</h3><p>You can deploy your Spring Boot GraalVM application to Kubernetes using a HelmÂ Chart:</p><pre>helm install graalvm-native-app deploy/helm/graalvm-native-app</pre><p>To uninstall:</p><pre>helm uninstall graalvm-native-app</pre><h3>ğŸ›  Conclusion</h3><p>By leveraging Spring Boot and GraalVM Native Image, you can develop <strong>fast-starting, lightweight, and cloud-native</strong> applications.</p><p>This approach provides <strong>reduced memory consumption, improved startup performance, and enhanced security</strong>. If youâ€™re looking to build high-performance Spring Boot applications, try <a href=\"https://github.com/susimsek/spring-boot-graalvm-native-example\">cloning my GitHub repository</a> and get started today!Â ğŸš€</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=3703a3b9fd29\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/yapi-kredi-teknoloji/spring-boot-graalvm-native-image-3703a3b9fd29\">Spring Boot GraalVM Native Image</a> was originally published in <a href=\"https://medium.com/yapi-kredi-teknoloji\">YapÄ± Kredi Teknoloji</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
      "content:encodedSnippet": "ğŸ“– Introduction\nJava applications often suffer from long startup times and high memory consumption, making modern software development more challenging. However, GraalVM Native Image offers a groundbreaking solution by significantly improving the performance of Spring Boot applications.\nIn this article, we will explore my GitHub example project and demonstrate how to develop high-performance and lightweight applications using Java 21, Spring Boot 3.4, and GraalVM Native Image.Â ğŸš€\nğŸ“Œ What is GraalVM and Why Should You UseÂ It?\nGraalVM is a high-performance JIT (Just-In-Time) and AOT (Ahead-of-Time) compiler that enables Java applications to be compiled into nativeÂ code.\nKey Advantages ofÂ GraalVM:\nâœ… Instant startup timeâ€Šâ€”â€ŠRuns natively without requiring a JVM!\nâœ… Lower memory consumptionâ€Šâ€”â€ŠUses significantly less RAM compared to the traditional JVM.\nâœ… Smaller Docker containersâ€Šâ€”â€ŠIdeal for cloud-native architectures.\nâœ… Enhanced securityâ€Šâ€”â€ŠEliminates unnecessary runtime components, reducing the attackÂ surface.\nğŸ“Œ JVM vs. GraalVM Native Image Comparison\n\nâš™ï¸ Getting Started with Spring Boot and GraalVM NativeÂ Image\n1ï¸âƒ£ Set Up Your Development Environment\nBefore you begin, ensure you have the following installed:\nğŸ”¹ Java 21\nğŸ”¹ GraalVM 22.3+ (with Native Image support)\nğŸ”¹ UPX (optional, for compressing the executable)\n2ï¸âƒ£ Running theÂ Project\nFirst, clone the repository fromÂ GitHub:\ngit clone https://github.com/susimsek/spring-boot-graalvm-native-example.git\n cd spring-boot-graalvm-native-example\n ./mvnw spring-boot:run\nThe application will be available at http://localhost:8080.\n\nTest theÂ API:\ncurl -X GET http://localhost:8080/api/v1/hello\nResponse:\n{ \"message\": \"Hello, GraalVM Native Image!\"Â }\nğŸ— Building Your Spring Boot Application as a GraalVM NativeÂ Image\nTo compile your Spring Boot project into a native binary, follow theseÂ steps:\nCompile a NativeÂ Image\n./mvnw native:compile -B -ntp -Pnative,prod -DskipTests\nThis will generate a target/native-executable binary.\nOptionally, compress the executable usingÂ UPX:\nupx --ultra-brute --lzma target/native-executable\nUPX (Ultimate Packer for eXecutables) is a powerful tool that compresses executables to save disk space and reduce deployment size. Compressed binaries function identically to their uncompressed counterparts but occupy lessÂ storage.\nAdvantages of UPX:\nâœ… Smaller file sizeâ€Šâ€”â€ŠReduces executable size by 50â€“80%.\nâœ… No performance lossâ€Šâ€”â€ŠCompressed binaries execute as normal.\nâœ… Efficient executionâ€Šâ€”â€ŠMaintains CPU performance while conserving diskÂ space.\nExample: If target/native-executable is 140MB, using UPX can shrink it toÂ 30-35MB.\nğŸ³ Deploying withÂ Docker\nA GraalVM-generated native executable can be easily containerized usingÂ Docker.\nBuild the DockerÂ Image\ndocker build -t spring-boot-graalvm-samples .\nRun the Docker Container\ndocker run -d -p 8080:8080 spring-boot-graalvm-samples\nğŸš€ Deploying to Kubernetes\nYou can deploy your Spring Boot GraalVM application to Kubernetes using a HelmÂ Chart:\nhelm install graalvm-native-app deploy/helm/graalvm-native-app\nTo uninstall:\nhelm uninstall graalvm-native-app\nğŸ›  Conclusion\nBy leveraging Spring Boot and GraalVM Native Image, you can develop fast-starting, lightweight, and cloud-native applications.\nThis approach provides reduced memory consumption, improved startup performance, and enhanced security. If youâ€™re looking to build high-performance Spring Boot applications, try cloning my GitHub repository and get started today!Â ğŸš€\n\nSpring Boot GraalVM Native Image was originally published in YapÄ± Kredi Teknoloji on Medium, where people are continuing the conversation by highlighting and responding to this story.",
      "dc:creator": "Åuayb ÅimÅŸek",
      "guid": "https://medium.com/p/3703a3b9fd29",
      "categories": ["upx", "kubernetes", "spring-boot", "native-image", "graalvm"],
      "isoDate": "2025-04-22T08:02:29.000Z"
    },
    {
      "creator": "Åuayb ÅimÅŸek",
      "title": "React & Spring Boot Graphql Fullstack Microservice Application on Kubernetes",
      "link": "https://suaybsimsek58.medium.com/react-spring-boot-graphql-fullstack-microservice-application-on-kubernetes-eb227e1a748b?source=rss-bda589f2335a------2",
      "pubDate": "Sun, 28 Aug 2022 16:29:12 GMT",
      "content:encoded": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*PGprMpt9l9D_eUGYW1tzSA.png\" /></figure><p>Hello everyone, In this article, we will develop a fullstack graphql microservice application using React and Spring Boot and deploy this application on Kubernetes.</p><h3>GraphQL</h3><p>GraphQL is a query language and server-side runtime for application programming interfaces (APIs) that prioritizes giving clients exactly the data they request and noÂ more.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/240/1*kNO-rVTtHXdRaHZCTtpBLg.png\" /></figure><p>GraphQL is designed to make APIs fast, flexible, and developer-friendly. It can even be deployed within an integrated development environment (IDE) known as GraphiQL. As an alternative to REST, GraphQL lets developers construct requests that pull data from multiple data sources in a single APIÂ call.</p><h3>Apollo Federation</h3><p>Although Apollo Federation is a complex structure, we can briefly say that it is a technology that enables multiple graphql schemes to be gathered under a single gateway and accessed as a singleÂ scheme.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/216/1*uf7NRPwIF3HsxM8GikI_tQ.png\" /></figure><p>Apollo federation is an elegant approach to apply microservice pattern with GraphQL. Using federation you can easily split your schema into multiple subschemas and implement each subschema logic in itâ€™s ownÂ service.</p><p>In this article we gonna implement a simple product-review graph, so we will have 4 services:</p><ul><li>product service</li><li>review service</li><li>apollo gateway</li><li>auth service</li></ul><h3>Auth service</h3><p>We are using oauth2 authorization server that implemented using spring security authorization server framework to secure the application.we will not focus on this service in this article. OAuth authorization server is responsible for authenticating the users and issuing access tokens containing the user data and proper access policies.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*kgBXec8dhZgKxdCBsBo7SQ.png\" /></figure><p>The OAuth2 Authorization Server can be accessed from this link via kubernetes.</p><p><a href=\"http://auth.susimsek.github.io\">http://auth.susimsek.github.io</a></p><p>The OAuth2 Authorization Server can be accessed from this link via heroku. <a href=\"https://graphql-fullstack-auth-service.herokuapp.com\">https://graphql-fullstack-auth-service.herokuapp.com</a></p><h3>Apollo Gateway</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*LD3elemQEyFm114w2me_Kg.png\" /></figure><p>Letâ€™s define our Apollo Federation gateway.</p><pre>npm install @apollo/gateway apollo-server graphql --save</pre><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/cdd7e923df55696e20a3bc95c170ea3a/href\">https://medium.com/media/cdd7e923df55696e20a3bc95c170ea3a/href</a></iframe><p>We added cors configuration, Vault and Consul configuration to apollo gateway.And also added authorization header value to context to send authorization header to subgraph.We used manuel composition to compose a supergraph schema from a collection of subgraph schemas.manuel composition recommended for production environment by the apollo federation team.</p><h3>Product service</h3><p>First, we use spring graphql starter.Spring for GraphQL provides support for Spring applications built on <a href=\"https://www.graphql-java.com/\">GraphQL Java</a>.The project reached version 1.0 in MayÂ 2022.</p><pre>&lt;dependency&gt;<br>   &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;<br>   &lt;artifactId&gt;spring-boot-starter-graphql&lt;/artifactId&gt;<br>&lt;/dependency&gt;</pre><p>Then we will user spring webflux starter to provide reactive programming support.Reactive systems better utilize modern processors. Also, the inclusion of back-pressure in reactive programming ensures better resilience between decoupled components.</p><pre>&lt;dependency&gt;<br>   &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;<br>   &lt;artifactId&gt;spring-boot-starter-webflux&lt;/artifactId&gt;<br>&lt;/dependency&gt;</pre><p>Then letâ€™s define our productÂ schema</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/d06079dbc030bc0cd676de6b995a5750/href\">https://medium.com/media/d06079dbc030bc0cd676de6b995a5750/href</a></iframe><p>Take a note of that @key directive in <strong>Product </strong>type. It indicates that we defined a GraphQLÂ <strong>entity</strong>.</p><p>Letâ€™s create our controller to serve our <strong>product</strong>Â subgraph</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/02114e90722a69db30c19fb517345bd7/href\">https://medium.com/media/02114e90722a69db30c19fb517345bd7/href</a></iframe><h3>Review service</h3><p>Once we installed same dependencies as for product service we can define theÂ schema.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/29c9d0eea06f44efc24b991b3204469f/href\">https://medium.com/media/29c9d0eea06f44efc24b991b3204469f/href</a></iframe><p>As you can see, for now review subgraph doesnâ€™t have <strong>Query</strong> section at allâ€Šâ€”â€Šwe defined the <strong>Review </strong>type and what the most interestingâ€Šâ€”â€Šextended the <strong>Product</strong> type with a new reviews property that returns an array of productâ€™s reviews.</p><p>Letâ€™s create our controller to serve our <strong>review</strong>Â subgraph</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/8b0a57920343158c3fdb41b35e463c94/href\">https://medium.com/media/8b0a57920343158c3fdb41b35e463c94/href</a></iframe><h3>Getting everything together</h3><p>Now, letâ€™s launch and query. First start auth service and then our subgraphs. finally start the gateway. Apollo studio should be available on <a href=\"http://localhost:4000/\">http://localhost:4000/</a>.</p><p>Get access token from oauth2 server and assign it to Authorization header. Then, run the following query.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/e598349f411ef5ec672836987be175e9/href\">https://medium.com/media/e598349f411ef5ec672836987be175e9/href</a></iframe><p>The result shouldÂ be</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/d9e37d845c9bb35bcd41e3a509263e56/href\">https://medium.com/media/d9e37d845c9bb35bcd41e3a509263e56/href</a></iframe><h3>Frontend</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/982/1*hKmu4RbuCltBRBYNqcMjVg.png\" /></figure><p>We use GraphQL Code Generator is a CLI tool that generates code out of our GraphQLÂ schema.</p><p>Install the following npm packages required to run GraphQL Code Generator</p><pre>npm install @graphql-codegen/cli @graphql-codegen/introspection @graphql-codegen/typescript @graphql-codegen/typescript-operations @graphql-codegen/typescript-react-apollo --save-dev</pre><p>And also install the following npm packages generate types for TypeScript,Graphql andÂ Apollo.</p><pre>npm install graphql @apollo/client typescript --save</pre><p>Then add the codegen.yml file to your project. This file is a config for <strong>GraphQL-Codegen</strong>.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/c8e71b0de06219bb1578d0ca91ba84a5/href\">https://medium.com/media/c8e71b0de06219bb1578d0ca91ba84a5/href</a></iframe><p>The next step is to add scripts to package.json</p><pre>&quot;scripts&quot;: {<br>  &quot;generate&quot;: &quot;graphql-codegen --config codegen.yml&quot;,<br>  &quot;sonar&quot;: &quot;node sonarqube-scanner.js&quot;<br>}</pre><p>Youâ€™ll run this script (npm run codegen) every time you&#39;ve changed anything in your GraphQL API or in your GraphQL files to get the most up-to-date types generated.</p><p>Letâ€™s create the following GraphQL query to fetch productÂ reviews</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/3649ddb37ccdcebfdcf66e0dccc516eb/href\">https://medium.com/media/3649ddb37ccdcebfdcf66e0dccc516eb/href</a></iframe><p>Run the codegen to generate types andÂ hooks:</p><pre>npm run generate</pre><p>Use the generated hook inÂ react.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/044ee8961f47a563fb4abf51823748e8/href\">https://medium.com/media/044ee8961f47a563fb4abf51823748e8/href</a></iframe><p>Thatâ€™s it. Graphql codegen is amazing tool to generate reusable hooks in your GraphQL files.By using this approach we are ableÂ to;</p><ul><li>Avoid generating dynamic queries atÂ runtime.</li><li>Use graphql query language rather than a language-specific syntax like tagged templateÂ literals</li><li>Improve on the DX for we auto-generate reusable hooks with type-safety and IDE IntelliSense</li><li>Validate queries against ourÂ schema</li><li>Rebuild our code when we change ourÂ queries</li></ul><h3>Heroku</h3><p>I just deployed oauth2 authorization server fullstack application to heroku.You can access the oauth2 authorization server via the linksÂ below.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/148/0*yZKl06FmC9ALP_0-.png\" /></figure><p>The Oauth2 Authorization server can be accessed from thisÂ link.</p><p><a href=\"https://graphql-fullstack-auth-service.herokuapp.com\">http://graphql-fullstack-auth-service.herokuapp.com</a></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/990/1*0gMilg2yMdrvLWv255adjA.png\" /></figure><p>You can log in to the authorization server using that credential information or login to the authorization server withÂ gmail.</p><pre><em>username:</em> admin<br><em>password:</em> password</pre><h3>Deployment with DockerÂ Compose</h3><p>Docker Compose is a tool for defining and running multi-container Docker applications.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/367/1*ZFW8FyGjTuNkwHQgAZ76iA.png\" /></figure><p>You can deploy the app by running the following bashÂ command</p><pre><em>sudo</em> chmod +x deploy.sh<br><em>./deploy.sh</em> -d</pre><p>You can uninstall app the following bashÂ command</p><pre><em>./deploy.sh</em> -d -r</pre><p>The Fullstack GraphQL App be accessed with nginx from the linkÂ below.</p><p><a href=\"http://127.0.0.1\">http://127.0.0.1</a></p><p>The OAuth2 Authorization Server be accessed from the link below. (default nginxÂ ingress)</p><p><a href=\"http://127.0.0.1:9000\">http://127.0.0.1:9000</a></p><h3>Kubernetes Deployment withÂ Helm</h3><p><a href=\"https://www.helm.sh/\">Helm</a> is a package manager for Kubernetes that allows developers and operators to more easily package, configure, and deploy applications and services onto Kubernetes clusters.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/367/0*NBbRuV1NjGNvgARG.png\" /></figure><p>You can deploy the app by running the following bashÂ command</p><pre><em>sudo</em> chmod +x deploy.sh<br><em>./deploy.sh</em> -k</pre><p>You can uninstall app the following bashÂ command</p><pre><em>./deploy.sh</em> -k -r</pre><p>You can upgrade the App (if you have made any changes to the generated manifests) by running the following bashÂ command</p><pre><em>./deploy.sh</em> -u</pre><p>The Fullstack GraphQL App be accessed with ingress from the link below.(default nginxÂ ingress)</p><p><a href=\"https://gqlmsweb.susimsek.github.io/\">http://gqlmsweb.susimsek.github.io/</a></p><p>The OAuth2 Authorization Server be accessed with ingress from the link below. (default nginxÂ ingress)</p><p><a href=\"https://auth.susimsek.github.io/\">http://auth.susimsek.github.io/</a></p><h3>Summary</h3><p>In this article, we have focused on how to develop a fullstack microservice application graphql based on Kubernetes.</p><p>GraphQL is a very exciting new technology that can potentially revolutionize the way we develop Web APIs.With Apollo Federation, implementing federated GraphQL in the context of microservices becomes easier thanÂ ever.</p><p>Finally, the full implementation of this article can be found in <a href=\"https://github.com/susimsek/GraphqlMicroserviceFullstack\">the GitHubÂ project</a>.</p><p>My article ends here. See you in the next articles.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=eb227e1a748b\" width=\"1\" height=\"1\" alt=\"\">",
      "content:encodedSnippet": "Hello everyone, In this article, we will develop a fullstack graphql microservice application using React and Spring Boot and deploy this application on Kubernetes.\nGraphQL\nGraphQL is a query language and server-side runtime for application programming interfaces (APIs) that prioritizes giving clients exactly the data they request and noÂ more.\n\nGraphQL is designed to make APIs fast, flexible, and developer-friendly. It can even be deployed within an integrated development environment (IDE) known as GraphiQL. As an alternative to REST, GraphQL lets developers construct requests that pull data from multiple data sources in a single APIÂ call.\nApollo Federation\nAlthough Apollo Federation is a complex structure, we can briefly say that it is a technology that enables multiple graphql schemes to be gathered under a single gateway and accessed as a singleÂ scheme.\n\nApollo federation is an elegant approach to apply microservice pattern with GraphQL. Using federation you can easily split your schema into multiple subschemas and implement each subschema logic in itâ€™s ownÂ service.\nIn this article we gonna implement a simple product-review graph, so we will have 4 services:\n\nproduct service\nreview service\napollo gateway\nauth service\n\nAuth service\nWe are using oauth2 authorization server that implemented using spring security authorization server framework to secure the application.we will not focus on this service in this article. OAuth authorization server is responsible for authenticating the users and issuing access tokens containing the user data and proper access policies.\n\nThe OAuth2 Authorization Server can be accessed from this link via kubernetes.\nhttp://auth.susimsek.github.io\nThe OAuth2 Authorization Server can be accessed from this link via heroku. https://graphql-fullstack-auth-service.herokuapp.com\nApollo Gateway\n\nLetâ€™s define our Apollo Federation gateway.\nnpm install @apollo/gateway apollo-server graphql --save\nhttps://medium.com/media/cdd7e923df55696e20a3bc95c170ea3a/href\nWe added cors configuration, Vault and Consul configuration to apollo gateway.And also added authorization header value to context to send authorization header to subgraph.We used manuel composition to compose a supergraph schema from a collection of subgraph schemas.manuel composition recommended for production environment by the apollo federation team.\nProduct service\nFirst, we use spring graphql starter.Spring for GraphQL provides support for Spring applications built on GraphQL Java.The project reached version 1.0 in MayÂ 2022.\n<dependency>\n   <groupId>org.springframework.boot</groupId>\n   <artifactId>spring-boot-starter-graphql</artifactId>\n</dependency>\nThen we will user spring webflux starter to provide reactive programming support.Reactive systems better utilize modern processors. Also, the inclusion of back-pressure in reactive programming ensures better resilience between decoupled components.\n<dependency>\n   <groupId>org.springframework.boot</groupId>\n   <artifactId>spring-boot-starter-webflux</artifactId>\n</dependency>\nThen letâ€™s define our productÂ schema\nhttps://medium.com/media/d06079dbc030bc0cd676de6b995a5750/href\nTake a note of that @key directive in Product type. It indicates that we defined a GraphQLÂ entity.\nLetâ€™s create our controller to serve our productÂ subgraph\nhttps://medium.com/media/02114e90722a69db30c19fb517345bd7/href\nReview service\nOnce we installed same dependencies as for product service we can define theÂ schema.\nhttps://medium.com/media/29c9d0eea06f44efc24b991b3204469f/href\nAs you can see, for now review subgraph doesnâ€™t have Query section at allâ€Šâ€”â€Šwe defined the Review type and what the most interestingâ€Šâ€”â€Šextended the Product type with a new reviews property that returns an array of productâ€™s reviews.\nLetâ€™s create our controller to serve our reviewÂ subgraph\nhttps://medium.com/media/8b0a57920343158c3fdb41b35e463c94/href\nGetting everything together\nNow, letâ€™s launch and query. First start auth service and then our subgraphs. finally start the gateway. Apollo studio should be available on http://localhost:4000/.\nGet access token from oauth2 server and assign it to Authorization header. Then, run the following query.\nhttps://medium.com/media/e598349f411ef5ec672836987be175e9/href\nThe result shouldÂ be\nhttps://medium.com/media/d9e37d845c9bb35bcd41e3a509263e56/href\nFrontend\n\nWe use GraphQL Code Generator is a CLI tool that generates code out of our GraphQLÂ schema.\nInstall the following npm packages required to run GraphQL Code Generator\nnpm install @graphql-codegen/cli @graphql-codegen/introspection @graphql-codegen/typescript @graphql-codegen/typescript-operations @graphql-codegen/typescript-react-apollo --save-dev\nAnd also install the following npm packages generate types for TypeScript,Graphql andÂ Apollo.\nnpm install graphql @apollo/client typescript --save\nThen add the codegen.yml file to your project. This file is a config for GraphQL-Codegen.\nhttps://medium.com/media/c8e71b0de06219bb1578d0ca91ba84a5/href\nThe next step is to add scripts to package.json\n\"scripts\": {\n  \"generate\": \"graphql-codegen --config codegen.yml\",\n  \"sonar\": \"node sonarqube-scanner.js\"\n}\nYouâ€™ll run this script (npm run codegen) every time you've changed anything in your GraphQL API or in your GraphQL files to get the most up-to-date types generated.\nLetâ€™s create the following GraphQL query to fetch productÂ reviews\nhttps://medium.com/media/3649ddb37ccdcebfdcf66e0dccc516eb/href\nRun the codegen to generate types andÂ hooks:\nnpm run generate\nUse the generated hook inÂ react.\nhttps://medium.com/media/044ee8961f47a563fb4abf51823748e8/href\nThatâ€™s it. Graphql codegen is amazing tool to generate reusable hooks in your GraphQL files.By using this approach we are ableÂ to;\n\nAvoid generating dynamic queries atÂ runtime.\nUse graphql query language rather than a language-specific syntax like tagged templateÂ literals\nImprove on the DX for we auto-generate reusable hooks with type-safety and IDE IntelliSense\nValidate queries against ourÂ schema\nRebuild our code when we change ourÂ queries\n\nHeroku\nI just deployed oauth2 authorization server fullstack application to heroku.You can access the oauth2 authorization server via the linksÂ below.\n\nThe Oauth2 Authorization server can be accessed from thisÂ link.\nhttp://graphql-fullstack-auth-service.herokuapp.com\n\nYou can log in to the authorization server using that credential information or login to the authorization server withÂ gmail.\nusername: admin\npassword: password\nDeployment with DockerÂ Compose\nDocker Compose is a tool for defining and running multi-container Docker applications.\n\nYou can deploy the app by running the following bashÂ command\nsudo chmod +x deploy.sh\n./deploy.sh -d\nYou can uninstall app the following bashÂ command\n./deploy.sh -d -r\nThe Fullstack GraphQL App be accessed with nginx from the linkÂ below.\nhttp://127.0.0.1\nThe OAuth2 Authorization Server be accessed from the link below. (default nginxÂ ingress)\nhttp://127.0.0.1:9000\nKubernetes Deployment withÂ Helm\nHelm is a package manager for Kubernetes that allows developers and operators to more easily package, configure, and deploy applications and services onto Kubernetes clusters.\n\nYou can deploy the app by running the following bashÂ command\nsudo chmod +x deploy.sh\n./deploy.sh -k\nYou can uninstall app the following bashÂ command\n./deploy.sh -k -r\nYou can upgrade the App (if you have made any changes to the generated manifests) by running the following bashÂ command\n./deploy.sh -u\nThe Fullstack GraphQL App be accessed with ingress from the link below.(default nginxÂ ingress)\nhttp://gqlmsweb.susimsek.github.io/\nThe OAuth2 Authorization Server be accessed with ingress from the link below. (default nginxÂ ingress)\nhttp://auth.susimsek.github.io/\nSummary\nIn this article, we have focused on how to develop a fullstack microservice application graphql based on Kubernetes.\nGraphQL is a very exciting new technology that can potentially revolutionize the way we develop Web APIs.With Apollo Federation, implementing federated GraphQL in the context of microservices becomes easier thanÂ ever.\nFinally, the full implementation of this article can be found in the GitHubÂ project.\nMy article ends here. See you in the next articles.",
      "dc:creator": "Åuayb ÅimÅŸek",
      "guid": "https://medium.com/p/eb227e1a748b",
      "categories": ["microservices", "spring-boot", "graphql", "kubernetes", "react"],
      "isoDate": "2022-08-28T16:29:12.000Z"
    },
    {
      "creator": "Åuayb ÅimÅŸek",
      "title": "React & Spring Boot Hateoas Driven Fullstack Application on Kubernetes",
      "link": "https://suaybsimsek58.medium.com/react-spring-boot-hateoas-driven-fullstack-application-on-kubernetes-7ea33894d12b?source=rss-bda589f2335a------2",
      "pubDate": "Tue, 15 Mar 2022 19:22:26 GMT",
      "content:encoded": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*mbUH1UeaKybgUMKTP1ou5w.png\" /></figure><p>Hello everyone, In this article, we will develop a hateoas driven fullstack application using React and Spring Boot and deploy this application on Kubernetes.</p><h3>Richardson MaturityÂ Model</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/535/1*dK6PpMxmdt-HH7EXO20Sew.png\" /></figure><p>Leonard Richardson analyzed a hundred different web service designs and divided these designs into four categories. These categories are based on how much the web services are <a href=\"https://restfulapi.net/rest-architectural-constraints/\">REST compliant</a>.</p><p>This model of division of REST services to identify their maturity levelâ€Šâ€”â€Šis called Richardson MaturityÂ Model.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/266/1*4xcJomcTOcGeLWg4J7ghqA.png\" /></figure><p>Richardson used three main factors to decide the maturity of a service. These factorsÂ are</p><ol><li><a href=\"https://restfulapi.net/resource-naming/\">URI</a>,</li><li><a href=\"https://restfulapi.net/http-methods/\">HTTP Methods</a>,</li><li><a href=\"https://restfulapi.net/hateoas/\">HATEOAS (Hypermedia)</a></li></ol><p>The more a service employs these factorsâ€Šâ€”â€Šthe more mature it shall be considered.</p><h3>Maturity Levels</h3><p>In his analysis, Richardson described total 4 maturity levels as givenÂ below:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/468/1*enHtcgVqThY90MpbMFbc3Q.png\" /></figure><ul><li>Level Zero</li><li>Level One</li><li>Level Two</li><li>Level Three</li></ul><p>Note that Roy Fielding has already made it very clear that <a href=\"http://roy.gbiv.com/untangled/2008/rest-apis-must-be-hypertext-driven\">level 3 RMM is a pre-condition ofÂ REST</a>.</p><h3>Level Three</h3><p>Level three of maturity makes use of all three, i.e. URIs and HTTP, andÂ HATEOAS.</p><p>Level three is the most mature level of Richardsonâ€™s model, which encourages easy discoverability. This level makes it easy for the responses to be self-descriptive by usingÂ HATEOAS.</p><p>Level three services lead the service consumers through a trail of resources, causing application state transitions as aÂ result.</p><h3>Hateoas</h3><p>The term <strong>HATEOAS</strong> stands for the phrase <strong>H</strong>ypermedia <strong>A</strong>s <strong>T</strong>he <strong>E</strong>ngine <strong>O</strong>f <strong>A</strong>pplication <strong>S</strong>tate. To understand this further, we first need to understand the meaning of <strong>Hypermedia</strong>.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/153/1*1qTT497DQ-ie1B7LmkdI8Q.png\" /></figure><p>The single most important reason for HATEOAS is <strong>loose coupling</strong>. If a consumer of a REST service needs to hard-code all the resource URLs, then it is tightly coupled with your service implementation. Instead, if you return the URLs, it could use for the actions, then it is loosely coupled. There is no tight dependency on the URI structure, as it is specified and used from the response.</p><h3>HALâ€Šâ€”â€ŠHypertext Application Language</h3><p>When you design a RESTful service, there is a need to specify how to return data and links corresponding to a request. HAL is a simple format that gives an easy, consistent way to hyperlink between resources in your REST API. Here is anÂ example</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*7_IzGR9cvywMJ4NHbuQBaA.png\" /></figure><p>With HAL, you have a few categories of representations:</p><ul><li><strong>Links:</strong> Specified as a combination of</li><li>Targetâ€Šâ€”â€ŠGiven as aÂ URI</li><li>Relationâ€Šâ€”â€ŠAÂ name</li><li><strong>Embedded Resources:</strong> Other resources contained within a given RESTÂ resource</li><li><strong>State:</strong> The actual resourceÂ data</li></ul><p>If you happen to use the Spring Framework to develop your REST service, then Spring HATEOAS is a good engine to use for yourÂ service.</p><h3>Spring BootÂ HATEOAS</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/363/1*OrB6plZLWwdaapcgQeNKSw.png\" /></figure><p>First, letâ€™s add the Spring HATEOAS dependency to ourÂ <em>pom.xml</em></p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/1ee00ddc418508cf9c7e10d0fae339c4/href\">https://medium.com/media/1ee00ddc418508cf9c7e10d0fae339c4/href</a></iframe><p>We will use SpringDoc for api documentation. Springdoc supports hateoas.we simply add the <em>springdoc-openapi-ui and springdoc-openapi-hateoas ui </em>dependency to ourÂ <em>pom.xml</em>:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/331/1*dEdt4WAg0T8wJOSrhMidZg.png\" /></figure><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/407ef64f3c894855c9b06bc4c976321f/href\">https://medium.com/media/407ef64f3c894855c9b06bc4c976321f/href</a></iframe><p>Native images provide various advantages like an instant startup and reduced memory consumption.We will use Spring Native to compile and build native images using Buildpacks and GraalVMâ€™s native buildÂ tools.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/414/1*7nArJMkG1Q5NmiPOuSfY2w.png\" /></figure><p>Letâ€™s add the <a href=\"https://repo.spring.io/artifactory/release/org/springframework/experimental/spring-native/\"><em>spring-native</em></a> Maven dependency and required plugins,repos.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/c9771fcc2f34225ec626c2b3aa808767/href\">https://medium.com/media/c9771fcc2f34225ec626c2b3aa808767/href</a></iframe><p>Here, weâ€™ll use the tiny builder out of the various available builders like base and full to build a native image.Also, we enabled the upx by providing the <em>UPX</em> value to the BP_BINARY_COMPRESSION_METHOD environment variable.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/444/1*PcFg4nqcTsO14-6IKEx9hQ.png\" /></figure><p>UPX is an advanced file compressor that compresses executable files.When spring native or golang projects are compiled, since they produce executable programs directly, we can reduce the size of these programs by 50â€“70% withÂ upx.</p><p>weâ€™ll add property to use Java 17 for compilation</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/ef85335f9696a7cc3891288107d091f4/href\">https://medium.com/media/ef85335f9696a7cc3891288107d091f4/href</a></iframe><p><strong>Spring HATEOAS offers three abstractions for creating the URIâ€Šâ€”â€Š<em>RepresentationModel, Link, and WebMvcLinkBuilder</em></strong>. We can use these to create the metadata and associate it to the resource representation.</p><h3>Adding Hypermedia Support to aÂ Resource</h3><p>The CapabilityDto resource extends from the RepresentationModel class to inherit the add() method. So once we create a link, we can easily set that value to the resource representation without adding any new fields toÂ it.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/3c65184ea0664ed5c9d997738e105cc8/href\">https://medium.com/media/3c65184ea0664ed5c9d997738e105cc8/href</a></iframe><h3>Creating Links</h3><p>Spring Hateoas provides <strong>the <em>WebMvcLinkBuilder</em>â€Šâ€”â€Šwhich simplifies building URIs</strong> by avoiding hard-coded theÂ links.</p><p>The following snippet shows building the customer self-link using the <em>WebMvcLinkBuilder</em> class:</p><pre>linkTo(methodOn(CapabilityController.class).getCapability(resource.getContent().getId())).withSelfRel()</pre><p>Letâ€™s have aÂ look:</p><ul><li>the <em>linkTo()</em> method inspects the controller class and obtains its rootÂ mapping</li><li>the <em>slash()</em> method adds the <em>customerId</em> value as the path variable of theÂ link</li><li>finally, the <em>withSelfMethod()</em> qualifies the relation as a self-link</li></ul><h3>Assemblers</h3><p>itâ€™s not about assembly language, but about a special kind of class that converts our resource to RepresentationModel.</p><p>One of such assemblers is SimpleRepresentationModelAssembler. Its implementation goes asÂ follows:</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/805ddf1563097696c072a07e12c36749/href\">https://medium.com/media/805ddf1563097696c072a07e12c36749/href</a></iframe><p>In this case, our entity will be wrapped in an EntityModel (this class extends RepresentationModel) to which the links specified by us in the addLinks() will be added. Here we overwrite two addLinks() methods â€“ one for entire data collections and the other for single resources. Then, as part of the controller, it is enough to call the toModel() or toCollectionModel()<strong> </strong>method (addLinks() are template methods here), depending on whether we return a collection or a single representation.</p><p>Spring hateoas provides automatic pagination links, we must use <a href=\"https://docs.spring.io/spring-hateoas/docs/current/api/org/springframework/hateoas/PagedModel.html\">PagedModel</a> provided by spring hateoas module which helps in creating representations of pageable collections.</p><p>PagedResourcesAssembler accepts the JPA entities list or Dtos List, and convert it to PagedModel.</p><h3>Spring HATEOAS inÂ Action</h3><ul><li>Finally, PagedModel,EntityModel,CollectionModel is returned as API response from web controller.</li></ul><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/180021334c6e728aefdc76bf50c890db/href\">https://medium.com/media/180021334c6e728aefdc76bf50c890db/href</a></iframe><h3>Verify links</h3><p>Next, letâ€™s invoke the getAllCapabilitiesWithPagination<em>()</em> method</p><p>curl <a href=\"http://localhost:8080/api/paged-capabilities\">http://localhost:8080/api/paged-capabilities</a></p><p>And examine theÂ result:</p><pre>{<br>  &quot;_embedded&quot;: {<br>    &quot;capabilities&quot;: [<br>      {<br>        &quot;id&quot;: 1,<br>        &quot;techStack&quot;: &quot;ReactJS&quot;,<br>        &quot;numOfDevelopers&quot;: 70,<br>        &quot;numOfAvailableDevelopers&quot;: 20,<br>        &quot;_links&quot;: {<br>          &quot;self&quot;: {<br>            &quot;href&quot;: &quot;http://localhost:8080/api/capabilities/1&quot;<br>          },<br>          &quot;capabilities&quot;: {<br>            &quot;href&quot;: &quot;http://localhost:8080/api/capabilities&quot;<br>          }<br>        }<br>      }<br>    ]<br>  },<br>  &quot;_links&quot;: {<br>    &quot;first&quot;: {<br>      &quot;href&quot;: &quot;http://localhost:8080/api/paged-capabilities?page=0&amp;size=1&quot;<br>    },<br>    &quot;self&quot;: {<br>      &quot;href&quot;: &quot;http://localhost:8080/api/paged-capabilities?page=0&amp;size=1&quot;<br>    },<br>    &quot;next&quot;: {<br>      &quot;href&quot;: &quot;http://localhost:8080/api/paged-capabilities?page=1&amp;size=1&quot;<br>    },<br>    &quot;last&quot;: {<br>      &quot;href&quot;: &quot;http://localhost:8080/api/paged-capabilities?page=12&amp;size=1&quot;<br>    },<br>    &quot;capabilities&quot;: {<br>      &quot;href&quot;: &quot;http://localhost:8080/api/capabilities&quot;<br>    }<br>  },<br>  &quot;page&quot;: {<br>    &quot;size&quot;: 1,<br>    &quot;totalElements&quot;: 13,<br>    &quot;totalPages&quot;: 13,<br>    &quot;number&quot;: 0<br>  }<br>}</pre><h3>Spring Hateoas Conclusion</h3><p>We saw hateoas driven rest api that adding HATEOAS links using spring boot hateoas module is very much easy and requires very less time and effort. In return, it increases the discoverability and usefulness of APIs by manyÂ folds.</p><h3>React Hateoas</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/395/1*qhOfS0A0lU8GcFAD456Avg.png\" /></figure><p>/api/paged-capabilities api returns the links associated with the capability resource.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*2k-Oj7CVjEsw8DbMrGTYug.png\" /></figure><p>Without HATEOAS, the client needs to know the URIs beforehand to call the correct endpoints. Instead of hardcoding the URIs in our component, we can refer to the links contained within our capability resource</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/5de3ff8de2584da5982b901b3dcb3d1c/href\">https://medium.com/media/5de3ff8de2584da5982b901b3dcb3d1c/href</a></iframe><p>Weâ€™ll start by writing a thunk function that makes an api call to our /api/paged-accounts endpoint to request an page of capality object, and then dispatch an action containing that array andÂ links.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/e7604c41be65117e6e0da73715be1acb/href\">https://medium.com/media/e7604c41be65117e6e0da73715be1acb/href</a></iframe><p>In the code below, we created a capabilityReducer.GET_CAPABILITIES action will return the page, payload andÂ links.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/592510203994c4acffeaff1332aee752/href\">https://medium.com/media/592510203994c4acffeaff1332aee752/href</a></iframe><p>AddCapability component gets the post Link from the redux store and passes this link as a parameter to the addCapabilityHandler.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/664b463d6d8569e31b97031ae7ecae69/href\">https://medium.com/media/664b463d6d8569e31b97031ae7ecae69/href</a></iframe><p>addCapabilityHandler executes <strong>addCapability()</strong> asynchronous HTTP requestÂ .Then, It dispatches ADD_CAPABILITY action to update the reduxÂ store.</p><h3>Paging</h3><p>Spring hateoas provides PagedModel to enable automatic pagination links.</p><p>We can retrieve a list of capabilities from GET /api/paged-capabilities, which features basic pagination:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*m8dI61JhYEHcJNikd_M96Q.png\" /></figure><p>The outer links array in the response payload includes references to the first, previous, current (via &quot;rel&quot;Â : &quot;self&quot;), next, and last resources pages</p><h3>React Hateoas Conclusion</h3><p>100% HATEOAS IS compatible with React &amp; Flux, HATEOAS is compatible with Angular, HATEOAS is compatible with JQuery and even vanillaÂ JS.</p><p>HATEOAS doesnâ€™t not impose any technical or implementation requirements on a consuming client.</p><p>HATEOAS is in fact simply a concept to which you can design your API (you can use one of several standards though likeÂ <a href=\"http://stateless.co/hal_specification.html\">HAL</a>)</p><p>Basically if you can call an API then you can implement a HATEOASÂ client.</p><p>So how to getÂ there:</p><ul><li>Step 1, how would you normally do an API call in React? Do it the sameÂ way.</li><li>Step 2, interrogate response.</li><li>Step 3, based on response, respond in the UI appropriately.</li></ul><h3>Heroku</h3><p>I deployed to heroku the hateoas fullstack application.You can access the web ui of the application and the swagger documentation via the linksÂ below.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/148/1*VEUaD79frAwYs4GYpDRskQ.png\" /></figure><p>The Hateoas Fullstack application can be accessed from thisÂ link.</p><p><a href=\"https://hateoas-fullstack-ui.herokuapp.com/\">https://hateoas-fullstack-ui.herokuapp.com</a></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*a2-2OYYPhZ1TvjlS.png\" /></figure><p>The swagger ui can be accessed from this link.<br><a href=\"https://hateoas-fullstack-api.herokuapp.com/swagger-ui.html\">https://hateoas-fullstack-api.herokuapp.com/swagger-ui.html</a></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*vTkE65V0Kq1hyI6z.png\" /></figure><h3>Kubernetes Deployment withÂ Helm</h3><p><a href=\"https://www.helm.sh/\">Helm</a> is a package manager for Kubernetes that allows developers and operators to more easily package, configure, and deploy applications and services onto Kubernetes clusters.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/367/1*17xVcNLzYsJABpViO9_SGg.png\" /></figure><p>You can deploy hateoas fullstack app by running the following bashÂ command</p><pre>./helm-apply.sh</pre><p>You can upgrade hateoas fullstack apps (if you have made any changes to the generated manifests) by running the following bashÂ command</p><pre>./helm-upgrade.sh</pre><p>The Hateoas Fullstack application can be accessed with ingress from the linkÂ below.</p><p><a href=\"http://hateoas-fullstack-ui.github.io\">http://hateoas-fullstack-ui.github.io</a></p><p>Finally, the full implementation of this article can be found in <a href=\"https://github.com/susimsek/HateoasFullstackApp\">the GitHubÂ project</a>.</p><p>My article ends here. See you in the next articles.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=7ea33894d12b\" width=\"1\" height=\"1\" alt=\"\">",
      "content:encodedSnippet": "Hello everyone, In this article, we will develop a hateoas driven fullstack application using React and Spring Boot and deploy this application on Kubernetes.\nRichardson MaturityÂ Model\n\nLeonard Richardson analyzed a hundred different web service designs and divided these designs into four categories. These categories are based on how much the web services are REST compliant.\nThis model of division of REST services to identify their maturity levelâ€Šâ€”â€Šis called Richardson MaturityÂ Model.\n\nRichardson used three main factors to decide the maturity of a service. These factorsÂ are\n\nURI,\nHTTP Methods,\nHATEOAS (Hypermedia)\n\nThe more a service employs these factorsâ€Šâ€”â€Šthe more mature it shall be considered.\nMaturity Levels\nIn his analysis, Richardson described total 4 maturity levels as givenÂ below:\n\nLevel Zero\nLevel One\nLevel Two\nLevel Three\n\nNote that Roy Fielding has already made it very clear that level 3 RMM is a pre-condition ofÂ REST.\nLevel Three\nLevel three of maturity makes use of all three, i.e. URIs and HTTP, andÂ HATEOAS.\nLevel three is the most mature level of Richardsonâ€™s model, which encourages easy discoverability. This level makes it easy for the responses to be self-descriptive by usingÂ HATEOAS.\nLevel three services lead the service consumers through a trail of resources, causing application state transitions as aÂ result.\nHateoas\nThe term HATEOAS stands for the phrase Hypermedia As The Engine Of Application State. To understand this further, we first need to understand the meaning of Hypermedia.\n\nThe single most important reason for HATEOAS is loose coupling. If a consumer of a REST service needs to hard-code all the resource URLs, then it is tightly coupled with your service implementation. Instead, if you return the URLs, it could use for the actions, then it is loosely coupled. There is no tight dependency on the URI structure, as it is specified and used from the response.\nHALâ€Šâ€”â€ŠHypertext Application Language\nWhen you design a RESTful service, there is a need to specify how to return data and links corresponding to a request. HAL is a simple format that gives an easy, consistent way to hyperlink between resources in your REST API. Here is anÂ example\n\nWith HAL, you have a few categories of representations:\n\nLinks: Specified as a combination of\nTargetâ€Šâ€”â€ŠGiven as aÂ URI\nRelationâ€Šâ€”â€ŠAÂ name\nEmbedded Resources: Other resources contained within a given RESTÂ resource\nState: The actual resourceÂ data\n\nIf you happen to use the Spring Framework to develop your REST service, then Spring HATEOAS is a good engine to use for yourÂ service.\nSpring BootÂ HATEOAS\n\nFirst, letâ€™s add the Spring HATEOAS dependency to ourÂ pom.xml\nhttps://medium.com/media/1ee00ddc418508cf9c7e10d0fae339c4/href\nWe will use SpringDoc for api documentation. Springdoc supports hateoas.we simply add the springdoc-openapi-ui and springdoc-openapi-hateoas ui dependency to ourÂ pom.xml:\nhttps://medium.com/media/407ef64f3c894855c9b06bc4c976321f/href\nNative images provide various advantages like an instant startup and reduced memory consumption.We will use Spring Native to compile and build native images using Buildpacks and GraalVMâ€™s native buildÂ tools.\n\nLetâ€™s add the spring-native Maven dependency and required plugins,repos.\nhttps://medium.com/media/c9771fcc2f34225ec626c2b3aa808767/href\nHere, weâ€™ll use the tiny builder out of the various available builders like base and full to build a native image.Also, we enabled the upx by providing the UPX value to the BP_BINARY_COMPRESSION_METHOD environment variable.\n\nUPX is an advanced file compressor that compresses executable files.When spring native or golang projects are compiled, since they produce executable programs directly, we can reduce the size of these programs by 50â€“70% withÂ upx.\nweâ€™ll add property to use Java 17 for compilation\nhttps://medium.com/media/ef85335f9696a7cc3891288107d091f4/href\nSpring HATEOAS offers three abstractions for creating the URIâ€Šâ€”â€ŠRepresentationModel, Link, and WebMvcLinkBuilder. We can use these to create the metadata and associate it to the resource representation.\nAdding Hypermedia Support to aÂ Resource\nThe CapabilityDto resource extends from the RepresentationModel class to inherit the add() method. So once we create a link, we can easily set that value to the resource representation without adding any new fields toÂ it.\nhttps://medium.com/media/3c65184ea0664ed5c9d997738e105cc8/href\nCreating Links\nSpring Hateoas provides the WebMvcLinkBuilderâ€Šâ€”â€Šwhich simplifies building URIs by avoiding hard-coded theÂ links.\nThe following snippet shows building the customer self-link using the WebMvcLinkBuilder class:\nlinkTo(methodOn(CapabilityController.class).getCapability(resource.getContent().getId())).withSelfRel()\nLetâ€™s have aÂ look:\n\nthe linkTo() method inspects the controller class and obtains its rootÂ mapping\nthe slash() method adds the customerId value as the path variable of theÂ link\nfinally, the withSelfMethod() qualifies the relation as a self-link\n\nAssemblers\nitâ€™s not about assembly language, but about a special kind of class that converts our resource to RepresentationModel.\nOne of such assemblers is SimpleRepresentationModelAssembler. Its implementation goes asÂ follows:\nhttps://medium.com/media/805ddf1563097696c072a07e12c36749/href\nIn this case, our entity will be wrapped in an EntityModel (this class extends RepresentationModel) to which the links specified by us in the addLinks() will be added. Here we overwrite two addLinks() methods â€“ one for entire data collections and the other for single resources. Then, as part of the controller, it is enough to call the toModel() or toCollectionModel() method (addLinks() are template methods here), depending on whether we return a collection or a single representation.\nSpring hateoas provides automatic pagination links, we must use PagedModel provided by spring hateoas module which helps in creating representations of pageable collections.\nPagedResourcesAssembler accepts the JPA entities list or Dtos List, and convert it to PagedModel.\nSpring HATEOAS inÂ Action\n\nFinally, PagedModel,EntityModel,CollectionModel is returned as API response from web controller.\nhttps://medium.com/media/180021334c6e728aefdc76bf50c890db/href\nVerify links\nNext, letâ€™s invoke the getAllCapabilitiesWithPagination() method\ncurl http://localhost:8080/api/paged-capabilities\nAnd examine theÂ result:\n{\n  \"_embedded\": {\n    \"capabilities\": [\n      {\n        \"id\": 1,\n        \"techStack\": \"ReactJS\",\n        \"numOfDevelopers\": 70,\n        \"numOfAvailableDevelopers\": 20,\n        \"_links\": {\n          \"self\": {\n            \"href\": \"http://localhost:8080/api/capabilities/1\"\n          },\n          \"capabilities\": {\n            \"href\": \"http://localhost:8080/api/capabilities\"\n          }\n        }\n      }\n    ]\n  },\n  \"_links\": {\n    \"first\": {\n      \"href\": \"http://localhost:8080/api/paged-capabilities?page=0&size=1\"\n    },\n    \"self\": {\n      \"href\": \"http://localhost:8080/api/paged-capabilities?page=0&size=1\"\n    },\n    \"next\": {\n      \"href\": \"http://localhost:8080/api/paged-capabilities?page=1&size=1\"\n    },\n    \"last\": {\n      \"href\": \"http://localhost:8080/api/paged-capabilities?page=12&size=1\"\n    },\n    \"capabilities\": {\n      \"href\": \"http://localhost:8080/api/capabilities\"\n    }\n  },\n  \"page\": {\n    \"size\": 1,\n    \"totalElements\": 13,\n    \"totalPages\": 13,\n    \"number\": 0\n  }\n}\nSpring Hateoas Conclusion\nWe saw hateoas driven rest api that adding HATEOAS links using spring boot hateoas module is very much easy and requires very less time and effort. In return, it increases the discoverability and usefulness of APIs by manyÂ folds.\nReact Hateoas\n\n/api/paged-capabilities api returns the links associated with the capability resource.\n\nWithout HATEOAS, the client needs to know the URIs beforehand to call the correct endpoints. Instead of hardcoding the URIs in our component, we can refer to the links contained within our capability resource\nhttps://medium.com/media/5de3ff8de2584da5982b901b3dcb3d1c/href\nWeâ€™ll start by writing a thunk function that makes an api call to our /api/paged-accounts endpoint to request an page of capality object, and then dispatch an action containing that array andÂ links.\nhttps://medium.com/media/e7604c41be65117e6e0da73715be1acb/href\nIn the code below, we created a capabilityReducer.GET_CAPABILITIES action will return the page, payload andÂ links.\nhttps://medium.com/media/592510203994c4acffeaff1332aee752/href\nAddCapability component gets the post Link from the redux store and passes this link as a parameter to the addCapabilityHandler.\nhttps://medium.com/media/664b463d6d8569e31b97031ae7ecae69/href\naddCapabilityHandler executes addCapability() asynchronous HTTP requestÂ .Then, It dispatches ADD_CAPABILITY action to update the reduxÂ store.\nPaging\nSpring hateoas provides PagedModel to enable automatic pagination links.\nWe can retrieve a list of capabilities from GET /api/paged-capabilities, which features basic pagination:\n\nThe outer links array in the response payload includes references to the first, previous, current (via \"rel\"Â : \"self\"), next, and last resources pages\nReact Hateoas Conclusion\n100% HATEOAS IS compatible with React & Flux, HATEOAS is compatible with Angular, HATEOAS is compatible with JQuery and even vanillaÂ JS.\nHATEOAS doesnâ€™t not impose any technical or implementation requirements on a consuming client.\nHATEOAS is in fact simply a concept to which you can design your API (you can use one of several standards though likeÂ HAL)\nBasically if you can call an API then you can implement a HATEOASÂ client.\nSo how to getÂ there:\n\nStep 1, how would you normally do an API call in React? Do it the sameÂ way.\nStep 2, interrogate response.\nStep 3, based on response, respond in the UI appropriately.\n\nHeroku\nI deployed to heroku the hateoas fullstack application.You can access the web ui of the application and the swagger documentation via the linksÂ below.\n\nThe Hateoas Fullstack application can be accessed from thisÂ link.\nhttps://hateoas-fullstack-ui.herokuapp.com\n\nThe swagger ui can be accessed from this link.\nhttps://hateoas-fullstack-api.herokuapp.com/swagger-ui.html\n\nKubernetes Deployment withÂ Helm\nHelm is a package manager for Kubernetes that allows developers and operators to more easily package, configure, and deploy applications and services onto Kubernetes clusters.\n\nYou can deploy hateoas fullstack app by running the following bashÂ command\n./helm-apply.sh\nYou can upgrade hateoas fullstack apps (if you have made any changes to the generated manifests) by running the following bashÂ command\n./helm-upgrade.sh\nThe Hateoas Fullstack application can be accessed with ingress from the linkÂ below.\nhttp://hateoas-fullstack-ui.github.io\nFinally, the full implementation of this article can be found in the GitHubÂ project.\nMy article ends here. See you in the next articles.",
      "dc:creator": "Åuayb ÅimÅŸek",
      "guid": "https://medium.com/p/7ea33894d12b",
      "categories": ["kubernetes", "spring-boot", "hateoas", "react", "spring-native"],
      "isoDate": "2022-03-15T19:22:26.000Z"
    },
    {
      "creator": "Åuayb ÅimÅŸek",
      "title": "Spring Boot Fullstack Blockchain Application With Hyperledger Fabric running on Kubernetes (Part 6)",
      "link": "https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-6-b82662ada0a4?source=rss-bda589f2335a------2",
      "pubDate": "Mon, 10 Jan 2022 20:01:06 GMT",
      "content:encoded": "<h3>Spring Boot Fullstack Blockchain Application With Hyperledger Fabric running on Kubernetes (Part 6)â€Šâ€”â€ŠOrderer</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*268MMjpMXhKeCJKmo74BWA.png\" /></figure><p>Hello everyone, through this article series we will the Hyperledger Fabric integration with Spring Boot.In this article, we will look into orderer.Also,I will also explain how to deploy orderer service on Kubernetes.</p><p>Other articles on Hyperledger Fabric integration with Spring Boot can be accessed from the linksÂ below.</p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-1-c8f7c87d60eb\">Part 1â€Šâ€”â€ŠIntroduction</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-ddce8d25858d\">Part 2â€Šâ€”â€ŠKubernetes ClusterÂ Setup</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-3-7eb2fc75ba60\">Part 3â€Šâ€”â€ŠFabric CAÂ Server</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-4-d6583a57153c\">Part 4â€Šâ€”â€ŠGenerating Certificates and Artifacts</a></p><p><a href=\"https://suaybsimsek58.medium.com/1723502770e7\">Part 5â€Šâ€”â€ŠKafka</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-6-b82662ada0a4\">Part 6â€”Â Orderer</a></p><h3>Orderer</h3><p>The Orderer is responsible for packaging transactions into Blocks, and distribute them to Anchor Peers across theÂ network.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*V9-6AwxNQQFzGDs8.png\" /></figure><p>The transaction flow of Fabric have the steps Proposal, Packaging and Validation. The orderer is responsible for Packaging and involved in the Validation step for distribution of new blocks on theÂ network.</p><p>Ordering service provides a shared communication channel to clients and peers, offering a broadcast service for messages containing transactions. Clients connect to the channel and may broadcast messages on the channel which are then delivered to all peers. The channel supports atomic delivery of all messages, that is, message communication with total-order delivery and (implementation specific) reliability. In other words, the channel outputs the same messages to all connected peers and outputs them to all peers in the same logicalÂ order.</p><p>Ordering service is not capable of transaction validations, itâ€™s primary goal to provide total order for transactions published, cut blocks with ordered transactions.</p><h3>Ordering Service Implementations</h3><p>While every ordering service currently available handles transactions and configuration updates the same way, there are nevertheless several different implementations for achieving consensus on the strict ordering of transactions between ordering serviceÂ nodes</p><h3>Raft</h3><p>New as of v1.4.1, Raft is a crash fault tolerant (CFT) ordering service based on an implementation of <a href=\"https://raft.github.io/raft.pdf\">Raft protocol</a> inÂ <a href=\"https://coreos.com/etcd/\">etcd</a></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*hD4gWyOV642eUcMH.png\" /></figure><p>Raft follows a â€œleader and followerâ€ model, where a leader node is elected (per channel) and its decisions are replicated by the followers.</p><p>Raft ordering services should be easier to set up and manage than Kafka-based ordering services, and their design allows different organizations to contribute nodes to a distributed orderingÂ service.</p><h3>Kafka</h3><p>Similar to Raft-based ordering, Apache Kafka is a CFT implementation that uses a â€œleader and followerâ€ node configuration.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*lwPYdYLUhGgofsc6.png\" /></figure><p>Kafka utilizes a ZooKeeper ensemble for management purposes.Asset transfer projesinde kafka kullanÄ±yoruz.We will useÂ kafka.</p><h3><strong>Solo</strong> (deprecated inÂ v2.x)</h3><p>The Solo implementation of the ordering service is intended for test only and consists only of a single ordering node.It has been deprecated and may be removed entirely in a futureÂ release.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/680/0*FhYVUBpcoh46CoKv.png\" /></figure><p>Existing users of Solo should move to a single node Raft network for equivalent function.</p><h3>Installation of Orderer on Kubernetes</h3><p>For the asset transfer project, we will set up kafka and zookeeper to run 5 pods on kubernetes.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/629/1*FJ7d03WfEvpA-jpNz_bbwg.png\" /></figure><p>Letâ€™s open the project we downloaded from <a href=\"https://github.com/susimsek/spring-boot-hlf-k8s-fullstack-blockchain-app\">this link</a> and go to the directory where the k8s isÂ located.</p><pre>$ cd deploy/k8s</pre><h3>Kafka and Zookeeper Deployment</h3><p>Letâ€™s create a deployment for eachÂ orderer.</p><p>The yaml files that create it are in the following directory in the projectÂ below.</p><p>deploy/k8s/orderer/orderer.yaml</p><p>deploy/k8s/orderer/orderer2.yaml</p><p>deploy/k8s/orderer/orderer3.yaml</p><p>deploy/k8s/orderer/orderer4.yaml</p><p>deploy/k8s/orderer/orderer5.yaml</p><pre>apiVersion: apps/v1<br>kind: Deployment<br>metadata:<br>  name: orderer<br>  labels: <br>    app: orderer</pre><ul><li>The Deployment for orderer 1, namedÂ orderer.</li></ul><pre>apiVersion: apps/v1<br>kind: Deployment<br>metadata:<br>  name: orderer5<br>  labels: <br>    app: orderer5</pre><ul><li>The Deployment for orderer 5, named orderer5.In other orderers, this value is assigned to orderer2,orderer3,orderer4.</li></ul><pre>spec:<br>  selector:<br>    matchLabels:<br>      app: orderer<br>  replicas: 1</pre><ul><li>The Deployment, has a Spec that indicates that 1 replicas of the orderer container will be launched in unique Pods.This value is assigned the same for eachÂ orderer.</li></ul><pre>volumes:<br>  - name: fabricfiles<br>    persistentVolumeClaim:<br>      claimName: fabricfiles-pvc</pre><ul><li>The volumeClaimTemplates will provide stable storage using <a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/\">PersistentVolumes</a> provisioned by a PersistentVolume Provisioner.It should be the same as the pvc metadata name.This value is assigned the same for eachÂ orderer.</li></ul><pre>volumeMounts:<br>  - name:  fabricfiles<br>    mountPath: /organizations<br>    subPath: organizations <em># </em>required for certificates.<em><br>  </em>- name: fabricfiles<br>    mountPath: /system-genesis-block<br>    subPath: system-genesis-block <em># </em>It needs a genesis block.<em><br>  </em>- name: fabricfiles<br>    mountPath: /var/hyperledger/production/orderer<br>    subPath: state/orderer <em># orderer persistence data</em></pre><p>The Orderer container mounts the PV at /organizations for certificates.</p><p>The Orderer container mounts the PV at /system-genesis-block for genesis block.<em> </em>It needs a genesisÂ block.</p><p>The Orderer container mounts the PV at /var/hyperledger/production/orderer for orderer persistence data.<em> </em>This data will be kept in state/orderer sub directory on nfsÂ server.</p><pre>- name: fabricfiles<br>  mountPath: /var/hyperledger/production/orderer<br>  subPath: state/orderer5</pre><p>The persistence data of other orderers are also kept in subdirectories such as state/orderer2, state/orderer5 on the nfsÂ server.</p><pre>livenessProbe:<br>  httpGet:<br>    port: 9444<br>    path: /healthz<br>  initialDelaySeconds: 60<br>  timeoutSeconds: 5<br>  failureThreshold: 6<br>readinessProbe:<br>    httpGet:<br>      port: 9444<br>      path: /healthz<br>    initialDelaySeconds: 5<br>    timeoutSeconds: 3<br>    periodSeconds: 5</pre><p>when a container get in the ready state, kubernetes starts to route traffic to the relavent pod. But the pod in the container may not be ready to accept traffic. Therefore, we need to specify â€œ<em>liveness</em>â€ and â€œ<em>readiness</em>â€ probes for applications in order kubernetes to do this process more efficiently.</p><p>Kubelet will check whether the container is alive and healthy by sending requests to the /healthz path on port 9443 and expect a success resultÂ code.</p><pre>spec:<br>  containers:<br>  - name: orderer<br>    image: hyperledger/fabric-orderer:2.3<br>    imagePullPolicy: IfNotPresent</pre><p>2.3 is assigned as Hyperledger fabric orderer docker imageÂ version.</p><p>Set imagePullPolicy to IfNotPresent or Never and <a href=\"http://kubernetes.io/docs/user-guide/images/#pre-pulling-images\"><em>pre-pull</em></a>: Pull manually images on each cluster node so the latest is cached, then do a kubectl rolling-update or similar to restartÂ Pods.</p><p>The following environment variables are assigned forÂ orderer.</p><pre>env:<br>  - name: CONFIGTX_ORDERER_ADDRESSES<br>    value: &quot;orderer:7050&quot;<br>  - name: ORDERER_GENERAL_LISTENADDRESS<br>    value: &quot;0.0.0.0&quot;<br>  - name: ORDERER_GENERAL_LISTENPORT<br>    value: &quot;7050&quot;<br>  - name: ORDERER_GENERAL_LOGLEVEL<br>    value: debug<br>  - name: ORDERER_GENERAL_LOCALMSPDIR<br>    value: /organizations/ordererOrganizations/example.com/orderers/orderer.example.com/msp<br>  - name: ORDERER_GENERAL_LOCALMSPID<br>    value: OrdererMSP<br>  - name: ORDERER_GENERAL_GENESISMETHOD<br>    value: file<br>  - name: ORDERER_GENERAL_GENESISFILE<br>    value: /system-genesis-block/genesis.block<br>  - name: ORDERER_GENERAL_TLS_ENABLED<br>    value: &quot;true&quot;<br>  - name: ORDERER_GENERAL_TLS_PRIVATEKEY<br>    value: /organizations/ordererOrganizations/example.com/orderers/orderer.example.com/tls/server.key<br>  - name: ORDERER_GENERAL_TLS_CERTIFICATE<br>    value: /organizations/ordererOrganizations/example.com/orderers/orderer.example.com/tls/server.crt<br>  - name: ORDERER_GENERAL_TLS_ROOTCAS<br>    value: /organizations/ordererOrganizations/example.com/orderers/orderer.example.com/tls/ca.crt<br>  - name:  ORDERER_GENERAL_CLUSTER_CLIENTPRIVATEKEY<br>    value: /organizations/ordererOrganizations/example.com/orderers/orderer.example.com/tls/server.key<br>  - name:  ORDERER_GENERAL_CLUSTER_CLIENTCERTIFICATE<br>    value: /organizations/ordererOrganizations/example.com/orderers/orderer.example.com/tls/server.crt<br>  - name: ORDERER_OPERATIONS_LISTENADDRESS <em># metric endpoint<br>    </em>value: 0.0.0.0:9444<br>  - name: ORDERER_METRICS_PROVIDER<br>    value: prometheus<br>  - name: CONFIGTX_ORDERER_ORDERERTYPE<br>    value: kafka<br>  - name: CONFIGTX_ORDERER_KAFKA_BROKERS<br>    value: &quot;broker-0.broker:9092,broker-1.broker:9092&quot;<br>  - name: ORDERER_KAFKA_RETRY_SHORTINTERVAL<br>    value: 1s<br>  - name: ORDERER_KAFKA_RETRY_SHORTTOTAL<br>    value: 30s<br>  - name: ORDERER_KAFKA_VERBOSE<br>    value: &quot;true&quot;</pre><p>ORDERER_GENERAL_GENESISFILE: path to genesis fileÂ path</p><p>ORDERER_GENERAL_LOCALMSPID: ID to load the MSP definition</p><p>ORDERER_GENERAL_LOCALMSPDIR: MSPDir is the filesystem path which contains the MSP configuration</p><p>ORDERER_GENERAL_TLS_ENABLED: enable TLS with client authentication.</p><p>ORDERER_GENERAL_TLS_PRIVATEKEY: fully qualified path of the file that contains the server privateÂ key</p><p>ORDERER_GENERAL_TLS_CERTIFICATE: fully qualified path of the file that contains the server certificate</p><p>ORDERER_GENERAL_TLS_ROOTCASÂ : fully qualified path of the file that contains the certificate chain of the CA that issued TLS server certificate</p><p>ORDERER_GENERAL_GENESISMETHOD: <em>file</em> is used when you want provide the genesis block as file to the container</p><p>ORDERER_GENERAL_TLS_CLIENTROOTCAS: fully qualified path of the file that contains the certificate chain of the CA that issued TLS server certificate</p><p>CONFIGTX_ORDERER_ORDERERTYPE: The orderer implementation to start.Available types are solo,kafka and etcdraft.</p><p>ORDERER_KAFKA_RETRY_SHORTINTERVAL: The order node may fail to connect kafka_ kafka_ RETRY_ Shortentreval is the interval betweenÂ retries.</p><p>ORDERER_KAFKA_RETRY_SHORTTOTAL: Total number ofÂ retries.</p><p>CONFIGTX_ORDERER_KAFKA_BROKERS:Instructs Orderer how to get in touch withÂ Kafka.</p><p>ORDERER_GENERAL_LISTENPORT: This value is the port that the orderer listensÂ to.</p><p>ORDERER_KAFKA_RETRY_SHORTINTERVAL:orderer node may fail to connect to kafka, This value is the retry interval.</p><p>broker-0.broker: kafka serviceÂ name</p><p>9092:kafka serviceÂ port</p><p>orderer metrics will be dump on the belowÂ port.</p><pre>- name: ORDERER_OPERATIONS_LISTENADDRESS <em># metric endpoint<br>    </em>value: 0.0.0.0:9444<br>- name: ORDERER_METRICS_PROVIDER<br>   value: prometheus</pre><h3>Orderer Service</h3><p>Letâ€™s create a service forÂ orderer.</p><p>The yaml files that create it are in the following directory in the projectÂ below.</p><p>deploy/orderer/order-svc.yaml</p><p>deploy/orderer/order2-svc.yaml</p><p>deploy/orderer/order3-svc.yaml</p><p>deploy/orderer/order4-svc.yaml</p><p>deploy/orderer/order5-svc.yaml</p><pre>apiVersion: v1<br>kind: Service<br>metadata:<br>  name: orderer<br>  labels: <br>    app: orderer</pre><p>This specification creates a new Service object named â€œordererâ€.The Service for orderer 5, named orderer5.In other orderers, this value is assigned to orderer2,orderer3,orderer4.</p><pre>ports:<br>- name: grpc<br>  protocol: TCP<br>  targetPort: 7050 <em><br>  </em>port: 7050</pre><p>targetPort: container port.7050 is assigned.</p><p>port: kubernetes service port.7050 is assigned.</p><pre>apiVersion: v1<br>kind: Service<br>metadata:<br>  name: orderer-metrics<br>  labels:<br>    app: orderer<br>    metrics-service: &quot;true&quot;</pre><p>A new service has been created object named â€œorderer-metricsâ€ to retrieve orderer metric information.The metric service for orderer 5, named orderer5-metrics.</p><pre>spec:<br>  type: ClusterIP<br>  selector:<br>    app: orderer<br>  ports:<br>  - name: &quot;orderer-metrics&quot;<br>    targetPort: 9444 <em># container metric port<br>    </em>port: 9444</pre><p>The target port 9444 is the container metric portit has an open portÂ 9444.</p><p>Service maps port 9444 of the container to the nodeâ€™s external IP:Port for all containers with the labels app:orderer.</p><p>My article ends here. In general,I introduced orderer and explained the deployment of orderer on Kubernetes.</p><p>See you in the next articles.</p><h3>Deploy Orderer on Kubernetes</h3><p>Letâ€™s connect to the kubernetes master node virtual machine with the <strong>vagrant ssh</strong>Â command.</p><pre>$ vagrant ssh k8smaster</pre><p>Letâ€™s go to the directory where the kubernetes installation scripts are located.This directory is the same as the deploy/k8s folder in the project. With Vagrant, this directory is synchronized to the virtualÂ machine.</p><pre>$ <em>cd</em> /vagrant/k8s</pre><p>Deploying the deployments,services forÂ orderer.</p><pre>$ <em>kubectl </em>apply -f orderer/</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*M8DUxLJAZ4IeN-KWy1T2nA.png\" /></figure><p>Orderer creation pending completion.</p><pre>$ kubectl wait --for condition=available --timeout=300s deployment -l &quot;app in (orderer,orderer2,orderer3,orderer4,orderer5)&quot;</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*OpDvUmXh9jGdKJjJNl_FlA.png\" /></figure><p>Orderer created successfully.</p><p>Finally, letâ€™s check the conditions of the pods we run from the lensÂ ide.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*CfG5t_Z4l7qE6WRxnhlY2g.png\" /></figure><p>Finally, letâ€™s check the conditions of the pods we run from the lensÂ ide.</p><p>The state of orderer pods appear to beÂ running.</p><p>My article ends here. In general,I introduced Orderer and explained the deployment of these tools on Kubernetes.</p><p>See you in the next articles.</p><h3>Project Links</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*_SkaFgOe_jFbbYOo.png\" /></figure><p>Spring Boot Hlf Starter Project details and installation can be accessed via <a href=\"https://github.com/susimsek/spring-boot-hlf-starter\">thisÂ link</a>.</p><p>Asset Transfer Project details and installation can be accessed via <a href=\"https://github.com/susimsek/spring-boot-hlf-starter\">thisÂ link</a></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=b82662ada0a4\" width=\"1\" height=\"1\" alt=\"\">",
      "content:encodedSnippet": "Spring Boot Fullstack Blockchain Application With Hyperledger Fabric running on Kubernetes (Part 6)â€Šâ€”â€ŠOrderer\n\nHello everyone, through this article series we will the Hyperledger Fabric integration with Spring Boot.In this article, we will look into orderer.Also,I will also explain how to deploy orderer service on Kubernetes.\nOther articles on Hyperledger Fabric integration with Spring Boot can be accessed from the linksÂ below.\nPart 1â€Šâ€”â€ŠIntroduction\nPart 2â€Šâ€”â€ŠKubernetes ClusterÂ Setup\nPart 3â€Šâ€”â€ŠFabric CAÂ Server\nPart 4â€Šâ€”â€ŠGenerating Certificates and Artifacts\nPart 5â€Šâ€”â€ŠKafka\nPart 6â€”Â Orderer\nOrderer\nThe Orderer is responsible for packaging transactions into Blocks, and distribute them to Anchor Peers across theÂ network.\n\nThe transaction flow of Fabric have the steps Proposal, Packaging and Validation. The orderer is responsible for Packaging and involved in the Validation step for distribution of new blocks on theÂ network.\nOrdering service provides a shared communication channel to clients and peers, offering a broadcast service for messages containing transactions. Clients connect to the channel and may broadcast messages on the channel which are then delivered to all peers. The channel supports atomic delivery of all messages, that is, message communication with total-order delivery and (implementation specific) reliability. In other words, the channel outputs the same messages to all connected peers and outputs them to all peers in the same logicalÂ order.\nOrdering service is not capable of transaction validations, itâ€™s primary goal to provide total order for transactions published, cut blocks with ordered transactions.\nOrdering Service Implementations\nWhile every ordering service currently available handles transactions and configuration updates the same way, there are nevertheless several different implementations for achieving consensus on the strict ordering of transactions between ordering serviceÂ nodes\nRaft\nNew as of v1.4.1, Raft is a crash fault tolerant (CFT) ordering service based on an implementation of Raft protocol inÂ etcd\n\nRaft follows a â€œleader and followerâ€ model, where a leader node is elected (per channel) and its decisions are replicated by the followers.\nRaft ordering services should be easier to set up and manage than Kafka-based ordering services, and their design allows different organizations to contribute nodes to a distributed orderingÂ service.\nKafka\nSimilar to Raft-based ordering, Apache Kafka is a CFT implementation that uses a â€œleader and followerâ€ node configuration.\n\nKafka utilizes a ZooKeeper ensemble for management purposes.Asset transfer projesinde kafka kullanÄ±yoruz.We will useÂ kafka.\nSolo (deprecated inÂ v2.x)\nThe Solo implementation of the ordering service is intended for test only and consists only of a single ordering node.It has been deprecated and may be removed entirely in a futureÂ release.\n\nExisting users of Solo should move to a single node Raft network for equivalent function.\nInstallation of Orderer on Kubernetes\nFor the asset transfer project, we will set up kafka and zookeeper to run 5 pods on kubernetes.\n\nLetâ€™s open the project we downloaded from this link and go to the directory where the k8s isÂ located.\n$ cd deploy/k8s\nKafka and Zookeeper Deployment\nLetâ€™s create a deployment for eachÂ orderer.\nThe yaml files that create it are in the following directory in the projectÂ below.\ndeploy/k8s/orderer/orderer.yaml\ndeploy/k8s/orderer/orderer2.yaml\ndeploy/k8s/orderer/orderer3.yaml\ndeploy/k8s/orderer/orderer4.yaml\ndeploy/k8s/orderer/orderer5.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: orderer\n  labels: \n    app: orderer\n\nThe Deployment for orderer 1, namedÂ orderer.\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: orderer5\n  labels: \n    app: orderer5\n\nThe Deployment for orderer 5, named orderer5.In other orderers, this value is assigned to orderer2,orderer3,orderer4.\n\nspec:\n  selector:\n    matchLabels:\n      app: orderer\n  replicas: 1\n\nThe Deployment, has a Spec that indicates that 1 replicas of the orderer container will be launched in unique Pods.This value is assigned the same for eachÂ orderer.\n\nvolumes:\n  - name: fabricfiles\n    persistentVolumeClaim:\n      claimName: fabricfiles-pvc\n\nThe volumeClaimTemplates will provide stable storage using PersistentVolumes provisioned by a PersistentVolume Provisioner.It should be the same as the pvc metadata name.This value is assigned the same for eachÂ orderer.\n\nvolumeMounts:\n  - name:  fabricfiles\n    mountPath: /organizations\n    subPath: organizations # required for certificates.\n  - name: fabricfiles\n    mountPath: /system-genesis-block\n    subPath: system-genesis-block # It needs a genesis block.\n  - name: fabricfiles\n    mountPath: /var/hyperledger/production/orderer\n    subPath: state/orderer # orderer persistence data\nThe Orderer container mounts the PV at /organizations for certificates.\nThe Orderer container mounts the PV at /system-genesis-block for genesis block. It needs a genesisÂ block.\nThe Orderer container mounts the PV at /var/hyperledger/production/orderer for orderer persistence data. This data will be kept in state/orderer sub directory on nfsÂ server.\n- name: fabricfiles\n  mountPath: /var/hyperledger/production/orderer\n  subPath: state/orderer5\nThe persistence data of other orderers are also kept in subdirectories such as state/orderer2, state/orderer5 on the nfsÂ server.\nlivenessProbe:\n  httpGet:\n    port: 9444\n    path: /healthz\n  initialDelaySeconds: 60\n  timeoutSeconds: 5\n  failureThreshold: 6\nreadinessProbe:\n    httpGet:\n      port: 9444\n      path: /healthz\n    initialDelaySeconds: 5\n    timeoutSeconds: 3\n    periodSeconds: 5\nwhen a container get in the ready state, kubernetes starts to route traffic to the relavent pod. But the pod in the container may not be ready to accept traffic. Therefore, we need to specify â€œlivenessâ€ and â€œreadinessâ€ probes for applications in order kubernetes to do this process more efficiently.\nKubelet will check whether the container is alive and healthy by sending requests to the /healthz path on port 9443 and expect a success resultÂ code.\nspec:\n  containers:\n  - name: orderer\n    image: hyperledger/fabric-orderer:2.3\n    imagePullPolicy: IfNotPresent\n2.3 is assigned as Hyperledger fabric orderer docker imageÂ version.\nSet imagePullPolicy to IfNotPresent or Never and pre-pull: Pull manually images on each cluster node so the latest is cached, then do a kubectl rolling-update or similar to restartÂ Pods.\nThe following environment variables are assigned forÂ orderer.\nenv:\n  - name: CONFIGTX_ORDERER_ADDRESSES\n    value: \"orderer:7050\"\n  - name: ORDERER_GENERAL_LISTENADDRESS\n    value: \"0.0.0.0\"\n  - name: ORDERER_GENERAL_LISTENPORT\n    value: \"7050\"\n  - name: ORDERER_GENERAL_LOGLEVEL\n    value: debug\n  - name: ORDERER_GENERAL_LOCALMSPDIR\n    value: /organizations/ordererOrganizations/example.com/orderers/orderer.example.com/msp\n  - name: ORDERER_GENERAL_LOCALMSPID\n    value: OrdererMSP\n  - name: ORDERER_GENERAL_GENESISMETHOD\n    value: file\n  - name: ORDERER_GENERAL_GENESISFILE\n    value: /system-genesis-block/genesis.block\n  - name: ORDERER_GENERAL_TLS_ENABLED\n    value: \"true\"\n  - name: ORDERER_GENERAL_TLS_PRIVATEKEY\n    value: /organizations/ordererOrganizations/example.com/orderers/orderer.example.com/tls/server.key\n  - name: ORDERER_GENERAL_TLS_CERTIFICATE\n    value: /organizations/ordererOrganizations/example.com/orderers/orderer.example.com/tls/server.crt\n  - name: ORDERER_GENERAL_TLS_ROOTCAS\n    value: /organizations/ordererOrganizations/example.com/orderers/orderer.example.com/tls/ca.crt\n  - name:  ORDERER_GENERAL_CLUSTER_CLIENTPRIVATEKEY\n    value: /organizations/ordererOrganizations/example.com/orderers/orderer.example.com/tls/server.key\n  - name:  ORDERER_GENERAL_CLUSTER_CLIENTCERTIFICATE\n    value: /organizations/ordererOrganizations/example.com/orderers/orderer.example.com/tls/server.crt\n  - name: ORDERER_OPERATIONS_LISTENADDRESS # metric endpoint\n    value: 0.0.0.0:9444\n  - name: ORDERER_METRICS_PROVIDER\n    value: prometheus\n  - name: CONFIGTX_ORDERER_ORDERERTYPE\n    value: kafka\n  - name: CONFIGTX_ORDERER_KAFKA_BROKERS\n    value: \"broker-0.broker:9092,broker-1.broker:9092\"\n  - name: ORDERER_KAFKA_RETRY_SHORTINTERVAL\n    value: 1s\n  - name: ORDERER_KAFKA_RETRY_SHORTTOTAL\n    value: 30s\n  - name: ORDERER_KAFKA_VERBOSE\n    value: \"true\"\nORDERER_GENERAL_GENESISFILE: path to genesis fileÂ path\nORDERER_GENERAL_LOCALMSPID: ID to load the MSP definition\nORDERER_GENERAL_LOCALMSPDIR: MSPDir is the filesystem path which contains the MSP configuration\nORDERER_GENERAL_TLS_ENABLED: enable TLS with client authentication.\nORDERER_GENERAL_TLS_PRIVATEKEY: fully qualified path of the file that contains the server privateÂ key\nORDERER_GENERAL_TLS_CERTIFICATE: fully qualified path of the file that contains the server certificate\nORDERER_GENERAL_TLS_ROOTCASÂ : fully qualified path of the file that contains the certificate chain of the CA that issued TLS server certificate\nORDERER_GENERAL_GENESISMETHOD: file is used when you want provide the genesis block as file to the container\nORDERER_GENERAL_TLS_CLIENTROOTCAS: fully qualified path of the file that contains the certificate chain of the CA that issued TLS server certificate\nCONFIGTX_ORDERER_ORDERERTYPE: The orderer implementation to start.Available types are solo,kafka and etcdraft.\nORDERER_KAFKA_RETRY_SHORTINTERVAL: The order node may fail to connect kafka_ kafka_ RETRY_ Shortentreval is the interval betweenÂ retries.\nORDERER_KAFKA_RETRY_SHORTTOTAL: Total number ofÂ retries.\nCONFIGTX_ORDERER_KAFKA_BROKERS:Instructs Orderer how to get in touch withÂ Kafka.\nORDERER_GENERAL_LISTENPORT: This value is the port that the orderer listensÂ to.\nORDERER_KAFKA_RETRY_SHORTINTERVAL:orderer node may fail to connect to kafka, This value is the retry interval.\nbroker-0.broker: kafka serviceÂ name\n9092:kafka serviceÂ port\norderer metrics will be dump on the belowÂ port.\n- name: ORDERER_OPERATIONS_LISTENADDRESS # metric endpoint\n    value: 0.0.0.0:9444\n- name: ORDERER_METRICS_PROVIDER\n   value: prometheus\nOrderer Service\nLetâ€™s create a service forÂ orderer.\nThe yaml files that create it are in the following directory in the projectÂ below.\ndeploy/orderer/order-svc.yaml\ndeploy/orderer/order2-svc.yaml\ndeploy/orderer/order3-svc.yaml\ndeploy/orderer/order4-svc.yaml\ndeploy/orderer/order5-svc.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: orderer\n  labels: \n    app: orderer\nThis specification creates a new Service object named â€œordererâ€.The Service for orderer 5, named orderer5.In other orderers, this value is assigned to orderer2,orderer3,orderer4.\nports:\n- name: grpc\n  protocol: TCP\n  targetPort: 7050 \n  port: 7050\ntargetPort: container port.7050 is assigned.\nport: kubernetes service port.7050 is assigned.\napiVersion: v1\nkind: Service\nmetadata:\n  name: orderer-metrics\n  labels:\n    app: orderer\n    metrics-service: \"true\"\nA new service has been created object named â€œorderer-metricsâ€ to retrieve orderer metric information.The metric service for orderer 5, named orderer5-metrics.\nspec:\n  type: ClusterIP\n  selector:\n    app: orderer\n  ports:\n  - name: \"orderer-metrics\"\n    targetPort: 9444 # container metric port\n    port: 9444\nThe target port 9444 is the container metric portit has an open portÂ 9444.\nService maps port 9444 of the container to the nodeâ€™s external IP:Port for all containers with the labels app:orderer.\nMy article ends here. In general,I introduced orderer and explained the deployment of orderer on Kubernetes.\nSee you in the next articles.\nDeploy Orderer on Kubernetes\nLetâ€™s connect to the kubernetes master node virtual machine with the vagrant sshÂ command.\n$ vagrant ssh k8smaster\nLetâ€™s go to the directory where the kubernetes installation scripts are located.This directory is the same as the deploy/k8s folder in the project. With Vagrant, this directory is synchronized to the virtualÂ machine.\n$ cd /vagrant/k8s\nDeploying the deployments,services forÂ orderer.\n$ kubectl apply -f orderer/\n\nOrderer creation pending completion.\n$ kubectl wait --for condition=available --timeout=300s deployment -l \"app in (orderer,orderer2,orderer3,orderer4,orderer5)\"\n\nOrderer created successfully.\nFinally, letâ€™s check the conditions of the pods we run from the lensÂ ide.\n\nFinally, letâ€™s check the conditions of the pods we run from the lensÂ ide.\nThe state of orderer pods appear to beÂ running.\nMy article ends here. In general,I introduced Orderer and explained the deployment of these tools on Kubernetes.\nSee you in the next articles.\nProject Links\n\nSpring Boot Hlf Starter Project details and installation can be accessed via thisÂ link.\nAsset Transfer Project details and installation can be accessed via thisÂ link",
      "dc:creator": "Åuayb ÅimÅŸek",
      "guid": "https://medium.com/p/b82662ada0a4",
      "categories": ["orderer", "blockchain", "kubernetes", "spring-boot", "hyperledger-fabric"],
      "isoDate": "2022-01-10T20:01:06.000Z"
    },
    {
      "creator": "Åuayb ÅimÅŸek",
      "title": "Spring Boot Fullstack Blockchain Application With Hyperledger Fabric running on Kubernetes (Part 5)",
      "link": "https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-5-1723502770e7?source=rss-bda589f2335a------2",
      "pubDate": "Mon, 27 Dec 2021 19:30:57 GMT",
      "content:encoded": "<h3>Spring Boot Fullstack Blockchain Application With Hyperledger Fabric running on Kubernetes (Part 5)â€Šâ€”â€ŠKafka</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*cN6xIURs0Xub9a7A38II-g.png\" /></figure><p>Hello everyone, through this article series we will the Hyperledger Fabric integration with Spring Boot.In this article, we will look into kafka.Also,I will also explain how to deploy Kafka on Kubernetes.</p><p>Other articles on Hyperledger Fabric integration with Spring Boot can be accessed from the linksÂ below.</p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-1-c8f7c87d60eb\">Part 1â€Šâ€”â€ŠIntroduction</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-ddce8d25858d\">Part 2â€Šâ€”â€ŠKubernetes ClusterÂ Setup</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-3-7eb2fc75ba60\">Part 3â€Šâ€”â€ŠFabric CAÂ Server</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-4-d6583a57153c\">Part 4â€Šâ€”â€ŠGenerating Certificates and Artifacts</a></p><p><a href=\"https://suaybsimsek58.medium.com/1723502770e7\">Part 5â€”Â Kafka</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-6-b82662ada0a4\">Part 6â€Šâ€”â€ŠOrderer</a></p><p>First of all, I will briefly explainÂ Kafka.</p><h3>Kafka</h3><p>Apache Kafka is an open source framework for instant storage and analysis of big data, developed by LinkedIn and now part ofÂ Apache.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/410/1*Jnw877FtV8fUpyFvwQj4Xw.png\" /></figure><p>It uses the messaging system (queue) to quickly store and analyze bigÂ data.</p><p>Hyperledger Fabric ordering service nodes (OSNs) use your Kafka cluster and provide an ordering service to your blockchain network.</p><p>Letâ€™s examine the Topic, Producer and Consumer concepts inÂ Kafka.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/700/0*f1aFyQ6YwoWmudVZ.png\" /></figure><h3>Topic</h3><p>Topics is a user-nameable area where data (messages) are stored. Topics are divided into Partitions and the number of Partitions they are stored in can be determined by theÂ user.</p><h3>Producer</h3><p>Producers, on the other hand, are Publisher when we look at the Publish-Subscribe structure of Apache Kafka, which can send messages to these Topics. They can send data, ie messages, into the Topics and can be linked to more than one topic at the sameÂ time.</p><h3>Consumer</h3><p>Consumers, on the other hand, are Subscribers who consume the messages sent by the Producers to the Topics, as we can understand from the name. More than one Producer can send messages to a topic (Topic), and more than one Consumer can be included in a topic and read the data sent to the Topic. After a consumer reads these messages sent by the producers, this data is not deleted from theÂ Topic.</p><h3>Zookeeper</h3><p>Apache ZooKeeper is an open source Apache project that allows clusters to distribute information such as configuration, naming, and group services over large clusters.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/453/1*yoaG23tcn2RTH7JIYRCH_A.png\" /></figure><p>ZooKeeper uses a key-value store in a hierarchical fashion. Used for high availability environments. Apache ZooKeeper is written in Java and licensed under the Apache License 2.0. It is used by some big companies like Rackspace, Yahoo, eBay andÂ Reddit.</p><p>Zookeeper keeps track of status of the Kafka cluster nodes and it also keeps track of Kafka topics, partitions etc.</p><h3>Installation of Kafka on Kubernetes</h3><p>For the asset transfer project, we will set up kafka and zookeeper to run 2 pods on kubernetes.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/629/1*hlkxZN68vIdm_U-1iptB5A.png\" /></figure><p>Letâ€™s open the project we downloaded from <a href=\"https://github.com/susimsek/spring-boot-hlf-k8s-fullstack-blockchain-app\">this link</a> and go to the directory where the k8s isÂ located.</p><pre>$ cd deploy/k8s</pre><h3>Kafka and Zookeeper Persistence Volume</h3><p>First of all, letâ€™s create a persistence volume for kafka and zookeper. The yaml files that create it are in the following directory in the projectÂ below.</p><p>deploy/k8s/pv/kafka-pv.yaml</p><p>deploy/k8s/pv/zookeeper-pv.yaml</p><p>For Kafka;</p><pre>nfs:<br>  path: /srv/kubedata/fabricfiles/broker/kafka1<br>  server: 192.168.12.9</pre><p>It specifies the path where the permanent data of Kafka is kept on the nfs server and the ip of the nfsÂ server.</p><p>For Zookeper</p><pre>nfs:<br>  path: /srv/kubedata/fabricfiles/broker/zookeeper0<br>  server: 192.168.12.9</pre><p>It specifies the path where the permanent data of Zookoper is kept on the nfs server and the ip of the nfsÂ server.</p><pre>spec:<br>  storageClassName: default<br>  volumeMode: Filesystem<br>  capacity:<br>    storage: 1Gi</pre><p>Storage capacity is limited and may vary depending on the node on which a pod runs: network-attached storage might not be accessible by all nodes, or storage is local to a node to begin with.It is assigned asÂ 1gb.</p><pre>metadata:<br>  name: kafka1-pv<br>  labels:<br>    app: kafka<br>    podindex: &quot;0&quot;</pre><p>A podindex label was assigned to attach persistence volume with the relevant persistence volume claims.podindex: â€œ0â€ represents the persistent data of the firstÂ kafka.</p><pre>metadata:<br>  name: kafka1-pv<br>  labels:<br>    app: kafka<br>    podindex: &quot;1&quot;</pre><p>podindex: â€œ1â€ represents the persistent data of the secondÂ kafka.</p><pre>metadata:<br>  name: zookeeper1-pv<br>  labels:<br>    app: zookeeper<br>    podindex: &quot;0&quot;</pre><p>podindex: â€œ0â€ represents the persistent data of the first zookeeper.podindex: â€œ1â€ represents the persistent data of the second zookeeper.</p><h3>Kafka and Zookeeper Persistence VolumeÂ Claim</h3><p>Letâ€™s create a persistence volume claim for kafka and zookeper. The yaml files that create it are in the following directory in the projectÂ below.</p><p>deploy/k8s/pvc/kafka-pvc.yaml</p><p>deploy/k8s/pvc/zookeeper-pvc.yaml</p><pre>selector:<br>  matchLabels:<br>    app: kafka<br>    podindex: &quot;0&quot;</pre><p>Claims can specify a <a href=\"https://v1-20.docs.kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors\">label selector</a> to further filter the set of volumes. Only the volumes whose labels match the selector can be bound to the claim.kafka persistence volume must have â€˜podindex: â€œ0â€â€™Â label.</p><pre>selector:<br>  matchLabels:<br>    app: zookeeper<br>    podindex: &quot;0&quot;</pre><p>Same way,zookeeper persistence volume must have podindex: â€œ0â€Â label.</p><pre>resources:<br>  requests:<br>    storage: 1Gi</pre><p>Claims, like Pods, can request specific quantities of a resource. In this case, the request is for storage.It is assigned asÂ 1gb.</p><h3>Kafka and Zookeeper StatefulSet</h3><p>Letâ€™s create a statefulset for kafka and zookeper. StatefulSet is the workload API object used to manage stateful applications.Manages the deployment and scaling of a set of <a href=\"https://kubernetes.io/docs/concepts/workloads/pods/\">Pods</a>, <em>and provides guarantees about the ordering and uniqueness</em> of theseÂ Pods.</p><p>The yaml files that create it are in the following directory in the projectÂ below.</p><p>deploy/k8s/kafka/kafka.yaml</p><p>deploy/k8s/kafka/zookeeper.yaml</p><pre>apiVersion: apps/v1<br>kind: StatefulSet<br>metadata:<br>  name: broker<br>  labels:<br>    app: kafka</pre><ul><li>The StatefulSet, namedÂ broker.</li></ul><pre>apiVersion: apps/v1<br>kind: StatefulSet<br>metadata:<br>  name: zoo<br>  labels:<br>    app: zookeeper</pre><ul><li>The StatefulSet, namedÂ zoo.</li></ul><pre>spec:<br>  selector:<br>    matchLabels:<br>      app: kafka<br>  serviceName: broker<br>  replicas: 2</pre><ul><li>The StatefulSet, has a Spec that indicates that 2 replicas of the kafka container will be launched in uniqueÂ Pods.</li></ul><pre>selector:<br>  matchLabels:<br>    app: zookeeper<br>serviceName: zoo<br>replicas: 2</pre><p>Same way,The StatefulSet, has a Spec that indicates that 2 replicas of the zookeeper container will be launched in uniqueÂ Pods.</p><pre>volumeClaimTemplates:<br>- metadata:<br>    name: kafka</pre><ul><li>The volumeClaimTemplates will provide stable storage using <a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/\">PersistentVolumes</a> provisioned by a PersistentVolume Provisioner.It should be the same as the pvc metadataÂ name.</li></ul><pre>volumeClaimTemplates:<br>- metadata:<br>    name: zookeeper</pre><p>Same way,it should be the same as the pvc metadataÂ name.</p><p>The following environment variables are assigned forÂ Kafka.</p><pre>env:<br>  - name: KAFKA_MESSAGE_MAX_BYTES<br>    value: &quot;102760448&quot;<br>  - name: KAFKA_REPLICA_FETCH_MAX_BYTES<br>    value: &quot;102760448&quot;<br>  - name: KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE<br>    value: &quot;false&quot;<br>  - name: KAFKA_DEFAULT_REPLICATION_FACTOR<br>    value: &quot;2&quot;<br>  - name: KAFKA_MIN_INSYNC_REPLICAS<br>    value: &quot;2&quot;<br>  - name: KAFKA_ZOOKEEPER_CONNECT<br>    value: zoo-0.zoo:2181,zoo-1.zoo:2181<br>  - name: KAFKA_PORT<br>    value: &quot;9092&quot;<br>  - name: GODEBUG<br>    value: netdns=go   <br>  - name: KAFKA_ZOOKEEPER_CONNECTION_TIMEOUT_MS<br>    value: &quot;30000&quot;<br>  - name: KAFKA_LOG_DIRS<br>    value: /opt/kafka/data</pre><p>KAFKA_MESSAGE_MAX_BYTES: Maximum transmit messageÂ size.</p><p>KAFKA_REPLICA_FETCH_MAX_BYTES:Initial maximum number of bytes per topic+partition to request when fetching messages from theÂ broker.</p><p>KAFKA_MIN_INSYNC_REPLICAS:is used when there is a problem in the topic, maybe one of the partitions is not in-sync, or offline. When this is the case the cluster will send an ack when KAFKA_MIN_INSYNC_REPLICAS is satisfied. So 2 replicas, with KAFKA_MIN_INSYNC_REPLICAS=2 will still be able toÂ write.</p><p>KAFKA_ZOOKEEPER_CONNECT:Instructs Kafka how to get in touch with ZooKeeper.</p><p>zoo: zookeeper serviceÂ name</p><p>2181:zookeeper serviceÂ port</p><p>KAFKA_ZOOKEEPER_CONNECTION_TIMEOUT_MS:The max time that the client waits while establishing a connection to zookeeper</p><p>KAFKA_DEFAULT_REPLICATION_FACTOR:The replication factor for the tier metadata topic (set higher to ensure availability).</p><p>The following environment variables are assigned for Zookeeper.</p><pre>env:<br>  - name: ZOO_SERVERS<br>    value: server.0=zoo-0.zoo.default.svc.cluster.local:2888:3888 server.1=zoo-1.zoo.default.svc.cluster.local:2888:3888<br>  - name: ZOO_4LW_COMMANDS_WHITELIST<br>    value: srvr, mntr, ruok<br>  - name: ZOO_MAX_SESSION_TIMEOUT<br>    value: &quot;40000&quot;<br>  - name: ZOO_TICK_TIME<br>    value: &quot;2000&quot;</pre><p>ZOO_SERVERS: This variable allows you to specify a list of machines of the Zookeeper ensemble. Each entry should be specified as such: server.id=&lt;address1&gt;:&lt;port1&gt;:&lt;port2&gt;</p><p>ZOO_4LW_COMMANDS_WHITELIST:A list of comma separated Four Letter Words commands that user wants to use. A valid Four Letter Words command must be put in this list else ZooKeeper server will not enable the command.Defaults toÂ srvr</p><p>ZOO_MAX_SESSION_TIMEOUT:Maximum session timeout in milliseconds that the server will allow the client to negotiate</p><p>ZOO_TICK_TIME:Basic time unit in milliseconds used by Apache ZooKeeper for heartbeats</p><pre>ports:<br>  - name: broker<br>    containerPort: 9092</pre><p>Kafka listens one port: 9092 is the default port used byÂ Kafka.</p><pre>ports:<br>  - name: client<br>    containerPort: 2181<br>  - name: peer<br>    containerPort: 2888<br>  - name: leader-election<br>    containerPort: 3888</pre><p>Zookeeper listens on three ports: 2181 for client connections; 2888 for follower connections, if they are the leader; and 3888 for other server connections during the leader election phaseÂ .</p><h3>Kafka and Zookeeper Service</h3><p>Letâ€™s create a service for kafka and zookeper.</p><p>The yaml files that create it are in the following directory in the projectÂ below.</p><p>deploy/k8s/kafka/kafka-svc.yaml</p><p>deploy/k8s/kafka/zookeeper-svc.yaml</p><pre>apiVersion: v1<br>kind: Service<br>metadata:<br>  name: kafka<br>  labels:<br>    app: kafka</pre><p>This specification creates a new Service object namedÂ â€œkafkaâ€.</p><pre>apiVersion: v1<br>kind: Service<br>metadata:<br>  name: broker</pre><p>This specification creates a new headless Service object named â€œbrokerâ€.A headless service is a service with a service IP but instead of load-balancing it will return the IPs of our associated Pods. This allows us to interact directly with the Pods instead of a proxy.Required for configuring the Kafka in clusterÂ mode.</p><pre>ports:<br>- name: &quot;broker&quot;<br>  targetPort: 9092<em><br>  </em>port: 9092</pre><p>targetPort: container port.The default port used by Kafka isÂ 9092.</p><p>port: kubernetes serviceÂ port.</p><pre>apiVersion: v1<br>kind: Service<br>metadata:<br>  name: zookeeper<br>  labels:<br>    app: zookeeper</pre><p>This specification creates a new Service object named â€œzookeeperâ€.</p><pre>apiVersion: v1<br>kind: Service<br>metadata:<br>  name: zoo<br>spec:<br>  type: ClusterIP<br>  clusterIP: None<br>  selector:<br>    app: zookeeper</pre><p>This specification creates a new headless Service object named â€œzooâ€.Required for configuring the Zookeeper in clusterÂ mode.</p><pre>ports:<br>- name: &quot;peer&quot;<br>  targetPort: 2888 <em><br>  </em>port: 2888<br>- name: &quot;leader-election&quot;<br>  targetPort: 3888 <em><br>  </em>port: 3888</pre><p>The zookeeperâ€™s peer and leader election default ports are exposed via headlessÂ service.</p><pre>ports:<br>  - name: client<br>    protocol: TCP<br>    targetPort: 2181<br>    port: 2181</pre><p>The zookeeperâ€™s client default port are exposed via zookeeper service.</p><h3>Deploy Kafka and Zookeeper on Kubernetes</h3><p>Letâ€™s connect to the kubernetes master node virtual machine with the <strong>vagrant ssh</strong>Â command.</p><pre>$ vagrant ssh k8smaster</pre><p>Letâ€™s go to the directory where the kubernetes installation scripts are located.This directory is the same as the deploy/k8s folder in the project. With Vagrant, this directory is synchronized to the virtualÂ machine.</p><pre>$ <em>cd</em> /vagrant/k8s</pre><p>Deploying the persistence volume for zookeeper andÂ kafka.</p><pre>$ kubectl apply -f pv/kafka-pv.yaml</pre><pre>$ kubectl apply -f pv/zookeeper-pv.yaml</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*xHAW5fDsvHsG55DDktp9gA.png\" /></figure><p>After the pv is created, letâ€™s create theÂ pvc.</p><p>Deploying the persistence volume claim for zookeeper andÂ kafka.</p><pre>$ kubectl apply -f pvc/kafka-pvc.yaml</pre><pre>$ kubectl apply -f pvc/zookeeper-pvc.yaml</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*KWsQuG3-CrFZWJ2WBenD2w.png\" /></figure><p>Deploying the statefulset for zookeeper andÂ kafka.</p><pre>$ <em>kubectl </em>apply -f kafka/</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*2bRMmiS3KuWcxpq2luty4A.png\" /></figure><p>Kafka and zookeeper creation pending completion.</p><pre>$ kubectl wait --for condition=ready --timeout=300s pod -l &quot;app in (zookeeper,kafka)&quot;</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*doF0DyLwE320Aofr0d1T6g.png\" /></figure><p>Kafka and zookeeper created successfully.</p><p>Finally, letâ€™s check the conditions of the pods we run from the lensÂ ide.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*6ezjpOsVvrcGxFiZ8kjnZw.png\" /></figure><p>The state of Kafka and zookeeper pods appear to beÂ running.</p><p>My article ends here. In general,I introduced Kafka and Zookeeper and explained the deployment of these tools on Kubernetes.</p><p>See you in the next articles.</p><h3>Project Links</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*_SkaFgOe_jFbbYOo.png\" /></figure><p>Spring Boot Hlf Starter Project details and installation can be accessed via <a href=\"https://github.com/susimsek/spring-boot-hlf-starter\">thisÂ link</a>.</p><p>Asset Transfer Project details and installation can be accessed via <a href=\"https://github.com/susimsek/spring-boot-hlf-starter\">thisÂ link</a></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=1723502770e7\" width=\"1\" height=\"1\" alt=\"\">",
      "content:encodedSnippet": "Spring Boot Fullstack Blockchain Application With Hyperledger Fabric running on Kubernetes (Part 5)â€Šâ€”â€ŠKafka\n\nHello everyone, through this article series we will the Hyperledger Fabric integration with Spring Boot.In this article, we will look into kafka.Also,I will also explain how to deploy Kafka on Kubernetes.\nOther articles on Hyperledger Fabric integration with Spring Boot can be accessed from the linksÂ below.\nPart 1â€Šâ€”â€ŠIntroduction\nPart 2â€Šâ€”â€ŠKubernetes ClusterÂ Setup\nPart 3â€Šâ€”â€ŠFabric CAÂ Server\nPart 4â€Šâ€”â€ŠGenerating Certificates and Artifacts\nPart 5â€”Â Kafka\nPart 6â€Šâ€”â€ŠOrderer\nFirst of all, I will briefly explainÂ Kafka.\nKafka\nApache Kafka is an open source framework for instant storage and analysis of big data, developed by LinkedIn and now part ofÂ Apache.\n\nIt uses the messaging system (queue) to quickly store and analyze bigÂ data.\nHyperledger Fabric ordering service nodes (OSNs) use your Kafka cluster and provide an ordering service to your blockchain network.\nLetâ€™s examine the Topic, Producer and Consumer concepts inÂ Kafka.\n\nTopic\nTopics is a user-nameable area where data (messages) are stored. Topics are divided into Partitions and the number of Partitions they are stored in can be determined by theÂ user.\nProducer\nProducers, on the other hand, are Publisher when we look at the Publish-Subscribe structure of Apache Kafka, which can send messages to these Topics. They can send data, ie messages, into the Topics and can be linked to more than one topic at the sameÂ time.\nConsumer\nConsumers, on the other hand, are Subscribers who consume the messages sent by the Producers to the Topics, as we can understand from the name. More than one Producer can send messages to a topic (Topic), and more than one Consumer can be included in a topic and read the data sent to the Topic. After a consumer reads these messages sent by the producers, this data is not deleted from theÂ Topic.\nZookeeper\nApache ZooKeeper is an open source Apache project that allows clusters to distribute information such as configuration, naming, and group services over large clusters.\n\nZooKeeper uses a key-value store in a hierarchical fashion. Used for high availability environments. Apache ZooKeeper is written in Java and licensed under the Apache License 2.0. It is used by some big companies like Rackspace, Yahoo, eBay andÂ Reddit.\nZookeeper keeps track of status of the Kafka cluster nodes and it also keeps track of Kafka topics, partitions etc.\nInstallation of Kafka on Kubernetes\nFor the asset transfer project, we will set up kafka and zookeeper to run 2 pods on kubernetes.\n\nLetâ€™s open the project we downloaded from this link and go to the directory where the k8s isÂ located.\n$ cd deploy/k8s\nKafka and Zookeeper Persistence Volume\nFirst of all, letâ€™s create a persistence volume for kafka and zookeper. The yaml files that create it are in the following directory in the projectÂ below.\ndeploy/k8s/pv/kafka-pv.yaml\ndeploy/k8s/pv/zookeeper-pv.yaml\nFor Kafka;\nnfs:\n  path: /srv/kubedata/fabricfiles/broker/kafka1\n  server: 192.168.12.9\nIt specifies the path where the permanent data of Kafka is kept on the nfs server and the ip of the nfsÂ server.\nFor Zookeper\nnfs:\n  path: /srv/kubedata/fabricfiles/broker/zookeeper0\n  server: 192.168.12.9\nIt specifies the path where the permanent data of Zookoper is kept on the nfs server and the ip of the nfsÂ server.\nspec:\n  storageClassName: default\n  volumeMode: Filesystem\n  capacity:\n    storage: 1Gi\nStorage capacity is limited and may vary depending on the node on which a pod runs: network-attached storage might not be accessible by all nodes, or storage is local to a node to begin with.It is assigned asÂ 1gb.\nmetadata:\n  name: kafka1-pv\n  labels:\n    app: kafka\n    podindex: \"0\"\nA podindex label was assigned to attach persistence volume with the relevant persistence volume claims.podindex: â€œ0â€ represents the persistent data of the firstÂ kafka.\nmetadata:\n  name: kafka1-pv\n  labels:\n    app: kafka\n    podindex: \"1\"\npodindex: â€œ1â€ represents the persistent data of the secondÂ kafka.\nmetadata:\n  name: zookeeper1-pv\n  labels:\n    app: zookeeper\n    podindex: \"0\"\npodindex: â€œ0â€ represents the persistent data of the first zookeeper.podindex: â€œ1â€ represents the persistent data of the second zookeeper.\nKafka and Zookeeper Persistence VolumeÂ Claim\nLetâ€™s create a persistence volume claim for kafka and zookeper. The yaml files that create it are in the following directory in the projectÂ below.\ndeploy/k8s/pvc/kafka-pvc.yaml\ndeploy/k8s/pvc/zookeeper-pvc.yaml\nselector:\n  matchLabels:\n    app: kafka\n    podindex: \"0\"\nClaims can specify a label selector to further filter the set of volumes. Only the volumes whose labels match the selector can be bound to the claim.kafka persistence volume must have â€˜podindex: â€œ0â€â€™Â label.\nselector:\n  matchLabels:\n    app: zookeeper\n    podindex: \"0\"\nSame way,zookeeper persistence volume must have podindex: â€œ0â€Â label.\nresources:\n  requests:\n    storage: 1Gi\nClaims, like Pods, can request specific quantities of a resource. In this case, the request is for storage.It is assigned asÂ 1gb.\nKafka and Zookeeper StatefulSet\nLetâ€™s create a statefulset for kafka and zookeper. StatefulSet is the workload API object used to manage stateful applications.Manages the deployment and scaling of a set of Pods, and provides guarantees about the ordering and uniqueness of theseÂ Pods.\nThe yaml files that create it are in the following directory in the projectÂ below.\ndeploy/k8s/kafka/kafka.yaml\ndeploy/k8s/kafka/zookeeper.yaml\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: broker\n  labels:\n    app: kafka\n\nThe StatefulSet, namedÂ broker.\n\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: zoo\n  labels:\n    app: zookeeper\n\nThe StatefulSet, namedÂ zoo.\n\nspec:\n  selector:\n    matchLabels:\n      app: kafka\n  serviceName: broker\n  replicas: 2\n\nThe StatefulSet, has a Spec that indicates that 2 replicas of the kafka container will be launched in uniqueÂ Pods.\n\nselector:\n  matchLabels:\n    app: zookeeper\nserviceName: zoo\nreplicas: 2\nSame way,The StatefulSet, has a Spec that indicates that 2 replicas of the zookeeper container will be launched in uniqueÂ Pods.\nvolumeClaimTemplates:\n- metadata:\n    name: kafka\n\nThe volumeClaimTemplates will provide stable storage using PersistentVolumes provisioned by a PersistentVolume Provisioner.It should be the same as the pvc metadataÂ name.\n\nvolumeClaimTemplates:\n- metadata:\n    name: zookeeper\nSame way,it should be the same as the pvc metadataÂ name.\nThe following environment variables are assigned forÂ Kafka.\nenv:\n  - name: KAFKA_MESSAGE_MAX_BYTES\n    value: \"102760448\"\n  - name: KAFKA_REPLICA_FETCH_MAX_BYTES\n    value: \"102760448\"\n  - name: KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE\n    value: \"false\"\n  - name: KAFKA_DEFAULT_REPLICATION_FACTOR\n    value: \"2\"\n  - name: KAFKA_MIN_INSYNC_REPLICAS\n    value: \"2\"\n  - name: KAFKA_ZOOKEEPER_CONNECT\n    value: zoo-0.zoo:2181,zoo-1.zoo:2181\n  - name: KAFKA_PORT\n    value: \"9092\"\n  - name: GODEBUG\n    value: netdns=go   \n  - name: KAFKA_ZOOKEEPER_CONNECTION_TIMEOUT_MS\n    value: \"30000\"\n  - name: KAFKA_LOG_DIRS\n    value: /opt/kafka/data\nKAFKA_MESSAGE_MAX_BYTES: Maximum transmit messageÂ size.\nKAFKA_REPLICA_FETCH_MAX_BYTES:Initial maximum number of bytes per topic+partition to request when fetching messages from theÂ broker.\nKAFKA_MIN_INSYNC_REPLICAS:is used when there is a problem in the topic, maybe one of the partitions is not in-sync, or offline. When this is the case the cluster will send an ack when KAFKA_MIN_INSYNC_REPLICAS is satisfied. So 2 replicas, with KAFKA_MIN_INSYNC_REPLICAS=2 will still be able toÂ write.\nKAFKA_ZOOKEEPER_CONNECT:Instructs Kafka how to get in touch with ZooKeeper.\nzoo: zookeeper serviceÂ name\n2181:zookeeper serviceÂ port\nKAFKA_ZOOKEEPER_CONNECTION_TIMEOUT_MS:The max time that the client waits while establishing a connection to zookeeper\nKAFKA_DEFAULT_REPLICATION_FACTOR:The replication factor for the tier metadata topic (set higher to ensure availability).\nThe following environment variables are assigned for Zookeeper.\nenv:\n  - name: ZOO_SERVERS\n    value: server.0=zoo-0.zoo.default.svc.cluster.local:2888:3888 server.1=zoo-1.zoo.default.svc.cluster.local:2888:3888\n  - name: ZOO_4LW_COMMANDS_WHITELIST\n    value: srvr, mntr, ruok\n  - name: ZOO_MAX_SESSION_TIMEOUT\n    value: \"40000\"\n  - name: ZOO_TICK_TIME\n    value: \"2000\"\nZOO_SERVERS: This variable allows you to specify a list of machines of the Zookeeper ensemble. Each entry should be specified as such: server.id=<address1>:<port1>:<port2>\nZOO_4LW_COMMANDS_WHITELIST:A list of comma separated Four Letter Words commands that user wants to use. A valid Four Letter Words command must be put in this list else ZooKeeper server will not enable the command.Defaults toÂ srvr\nZOO_MAX_SESSION_TIMEOUT:Maximum session timeout in milliseconds that the server will allow the client to negotiate\nZOO_TICK_TIME:Basic time unit in milliseconds used by Apache ZooKeeper for heartbeats\nports:\n  - name: broker\n    containerPort: 9092\nKafka listens one port: 9092 is the default port used byÂ Kafka.\nports:\n  - name: client\n    containerPort: 2181\n  - name: peer\n    containerPort: 2888\n  - name: leader-election\n    containerPort: 3888\nZookeeper listens on three ports: 2181 for client connections; 2888 for follower connections, if they are the leader; and 3888 for other server connections during the leader election phaseÂ .\nKafka and Zookeeper Service\nLetâ€™s create a service for kafka and zookeper.\nThe yaml files that create it are in the following directory in the projectÂ below.\ndeploy/k8s/kafka/kafka-svc.yaml\ndeploy/k8s/kafka/zookeeper-svc.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: kafka\n  labels:\n    app: kafka\nThis specification creates a new Service object namedÂ â€œkafkaâ€.\napiVersion: v1\nkind: Service\nmetadata:\n  name: broker\nThis specification creates a new headless Service object named â€œbrokerâ€.A headless service is a service with a service IP but instead of load-balancing it will return the IPs of our associated Pods. This allows us to interact directly with the Pods instead of a proxy.Required for configuring the Kafka in clusterÂ mode.\nports:\n- name: \"broker\"\n  targetPort: 9092\n  port: 9092\ntargetPort: container port.The default port used by Kafka isÂ 9092.\nport: kubernetes serviceÂ port.\napiVersion: v1\nkind: Service\nmetadata:\n  name: zookeeper\n  labels:\n    app: zookeeper\nThis specification creates a new Service object named â€œzookeeperâ€.\napiVersion: v1\nkind: Service\nmetadata:\n  name: zoo\nspec:\n  type: ClusterIP\n  clusterIP: None\n  selector:\n    app: zookeeper\nThis specification creates a new headless Service object named â€œzooâ€.Required for configuring the Zookeeper in clusterÂ mode.\nports:\n- name: \"peer\"\n  targetPort: 2888 \n  port: 2888\n- name: \"leader-election\"\n  targetPort: 3888 \n  port: 3888\nThe zookeeperâ€™s peer and leader election default ports are exposed via headlessÂ service.\nports:\n  - name: client\n    protocol: TCP\n    targetPort: 2181\n    port: 2181\nThe zookeeperâ€™s client default port are exposed via zookeeper service.\nDeploy Kafka and Zookeeper on Kubernetes\nLetâ€™s connect to the kubernetes master node virtual machine with the vagrant sshÂ command.\n$ vagrant ssh k8smaster\nLetâ€™s go to the directory where the kubernetes installation scripts are located.This directory is the same as the deploy/k8s folder in the project. With Vagrant, this directory is synchronized to the virtualÂ machine.\n$ cd /vagrant/k8s\nDeploying the persistence volume for zookeeper andÂ kafka.\n$ kubectl apply -f pv/kafka-pv.yaml\n$ kubectl apply -f pv/zookeeper-pv.yaml\n\nAfter the pv is created, letâ€™s create theÂ pvc.\nDeploying the persistence volume claim for zookeeper andÂ kafka.\n$ kubectl apply -f pvc/kafka-pvc.yaml\n$ kubectl apply -f pvc/zookeeper-pvc.yaml\n\nDeploying the statefulset for zookeeper andÂ kafka.\n$ kubectl apply -f kafka/\n\nKafka and zookeeper creation pending completion.\n$ kubectl wait --for condition=ready --timeout=300s pod -l \"app in (zookeeper,kafka)\"\n\nKafka and zookeeper created successfully.\nFinally, letâ€™s check the conditions of the pods we run from the lensÂ ide.\n\nThe state of Kafka and zookeeper pods appear to beÂ running.\nMy article ends here. In general,I introduced Kafka and Zookeeper and explained the deployment of these tools on Kubernetes.\nSee you in the next articles.\nProject Links\n\nSpring Boot Hlf Starter Project details and installation can be accessed via thisÂ link.\nAsset Transfer Project details and installation can be accessed via thisÂ link",
      "dc:creator": "Åuayb ÅimÅŸek",
      "guid": "https://medium.com/p/1723502770e7",
      "categories": ["hyperledger-fabric", "kubernetes", "blockchain", "spring-boot", "kafka"],
      "isoDate": "2021-12-27T19:30:57.000Z"
    },
    {
      "creator": "Åuayb ÅimÅŸek",
      "title": "Reducing Spring Native and Golang Docker Image Size by 70% with Upx",
      "link": "https://suaybsimsek58.medium.com/reducing-spring-native-and-golang-docker-image-size-by-70-with-upx-daf84e4f9227?source=rss-bda589f2335a------2",
      "pubDate": "Mon, 20 Dec 2021 18:03:04 GMT",
      "content:encoded": "<h3>Reducing Docker Image Size by 70% in Spring Native and Golang Projects withÂ Upx</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*ssImHh3iXkYz014MF6ukrw.png\" /></figure><p>Hi, In this article, I will explain how we can reduce the docker image size by 70% using upx in spring native and golang projects.</p><h3>What is UPX(Ultimate Executable Packer)</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/200/0*BOTRiDRcC6oCosKm.png\" /></figure><p>UPX is an advanced file compressor that compresses executable files. UPX typically reduces the file size of programs and DLLs by about 50â€“70%, reducing programsâ€™ disk space, network load times, download times, and other distribution and storageÂ costs.</p><p>When spring native or golang projects are compiled, since they produce executable programs directly, we can reduce the size of these programs by 50â€“70% withÂ upx.</p><h3>Reducing Docker Image Size with UPX in GolangÂ Projects</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/964/1*ezPXDsdB-rSHM5IP98NB8Q.png\" /></figure><p>The docker image size of the web service I developed using the echo framework written in Golang was 40 MB. I reduced the image size to 5.05 MB by making some adjustments to the dockerfile.</p><p>The Dockerfile is asÂ follows.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/e2ebffced551beb90031272f4d96ba94/href\">https://medium.com/media/e2ebffced551beb90031272f4d96ba94/href</a></iframe><p><strong>ARG upx_version=3.96Â </strong>: Upx version to be installed. Currently the most updatedÂ version.</p><p><strong>CGO_ENABLED=0Â : </strong>you have a statically linked binary file. so it works without any external dependencies. In the scratch docker image, it makes our application run.</p><p><strong>GOOS=linux</strong>: the compiler compiles the application to the linux operating system.</p><p><strong>GOARCH=amd64:</strong> it tells the compiler that it must conform to the amd64 processor architecture.</p><p><strong>ldflags=â€-s -wâ€Â </strong>: removes debug information from the binaryÂ file.</p><p><strong>RUN upxâ€Šâ€”â€Šultra-brute -qq server &amp;&amp; \\upx -t server</strong>Â : we compressed the executable usingÂ upx.</p><p><strong>FROM scratch</strong>: zero size out of the box docker image. This means that the resulting image size is equal to the binaryÂ size.</p><p><strong>COPYâ€Šâ€”â€Šfrom=builder /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/ca-certificates.crtÂ </strong>: adds root certificates to struct dockerÂ images.</p><p><strong>CMD [â€œ./serverâ€]</strong>Â : runs the application.</p><p>Letâ€™s create docker images of the application.</p><pre>$ docker build -t app:1.0.0 .</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*r-bH6u4EiAWtSvYbA0vO6w.png\" /></figure><p>The docker image size of the application is 5.05Â mb</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*LhmwhueJXVvwrd4wmfFTlw.png\" /></figure><h3>Reducing Docker Image Size with UPX in Spring NativeÂ Projects</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/964/1*zw3ZV4Pa--Ae9pQGsOG9Uw.png\" /></figure><p>We can compile Spring Boot application as native with Spring native.When I compiled the application as native, the docker image size of the web service was 169 MB. I made some adjustments in the spring-boot-maven-plugin and reduced the image size to 53.4Â MB.</p><p>The pom.xml is asÂ follows.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/32c7eb2c8d61f0ee337ae09be1553a23/href\">https://medium.com/media/32c7eb2c8d61f0ee337ae09be1553a23/href</a></iframe><p><strong>&lt;java.version&gt;17&lt;/java.version&gt;</strong>Â : the application was compiled according to JavaÂ 17</p><p><strong>&lt;spring-native.version&gt;0.11.0-RC1&lt;/spring-native.version&gt;</strong>Â : the current most up-to-date spring native version is compatible with spring bootÂ 2.6.1.</p><p><strong>&lt;name&gt;docker.io/suayb/native-app:1.0.0&lt;/name&gt;Â : t</strong>he tag of the docker image to beÂ created.</p><p><strong>&lt;BP_NATIVE_IMAGE&gt;true&lt;/BP_NATIVE_IMAGE&gt;</strong>Â :the native image build pack is enabled. In Spring Native 0.11, the Liberica Native Image Kit (NIK) is the native image compiler distribution used by default with Buildpacks.</p><p><strong>&lt;BP_BINARY_COMPRESSION_METHOD&gt;upx&lt;/BP_BINARY_COMPRESSION_METHOD&gt;</strong>: it creates a compressed executable using upx. Required to be able to reduce the docker imageÂ size.</p><p><strong>â€” enable-httpsâ€Šâ€”â€Šenable-http</strong>Â : http and https are enabled as url protocols.</p><p><strong>&lt;publish&gt;false&lt;/publish&gt;</strong>Â : does not publish the created image in the docker registry.</p><p><strong>&lt;pullPolicy&gt;IF_NOT_PRESENT&lt;/pullPolicy&gt;</strong>Â : if the builder docker images do not exist locally, it pulls. This value is ALWAYS byÂ default.</p><p>Letâ€™s create docker images of the application.</p><pre>$ <em>mvn</em> spring-boot:build-image</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*rLSQ5-kywTOGYQIICjtxDQ.png\" /></figure><p>The docker image size of the application is 53.04Â mb.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*9HVwcS-SSdc3kwSCd7yH6Q.png\" /></figure><p>My article ends here. As seen with Upx, we can reduce docker image sizes by 70%. In Docker, we prefer small size image sizes because of the disposability principle. Because large containers take a long time to standÂ up.</p><p>Image size doesnâ€™t affect your programâ€™s performance, but it can speed up many processes.</p><p>See you in the next articles.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=daf84e4f9227\" width=\"1\" height=\"1\" alt=\"\">",
      "content:encodedSnippet": "Reducing Docker Image Size by 70% in Spring Native and Golang Projects withÂ Upx\n\nHi, In this article, I will explain how we can reduce the docker image size by 70% using upx in spring native and golang projects.\nWhat is UPX(Ultimate Executable Packer)\n\nUPX is an advanced file compressor that compresses executable files. UPX typically reduces the file size of programs and DLLs by about 50â€“70%, reducing programsâ€™ disk space, network load times, download times, and other distribution and storageÂ costs.\nWhen spring native or golang projects are compiled, since they produce executable programs directly, we can reduce the size of these programs by 50â€“70% withÂ upx.\nReducing Docker Image Size with UPX in GolangÂ Projects\n\nThe docker image size of the web service I developed using the echo framework written in Golang was 40 MB. I reduced the image size to 5.05 MB by making some adjustments to the dockerfile.\nThe Dockerfile is asÂ follows.\nhttps://medium.com/media/e2ebffced551beb90031272f4d96ba94/href\nARG upx_version=3.96Â : Upx version to be installed. Currently the most updatedÂ version.\nCGO_ENABLED=0Â : you have a statically linked binary file. so it works without any external dependencies. In the scratch docker image, it makes our application run.\nGOOS=linux: the compiler compiles the application to the linux operating system.\nGOARCH=amd64: it tells the compiler that it must conform to the amd64 processor architecture.\nldflags=â€-s -wâ€Â : removes debug information from the binaryÂ file.\nRUN upxâ€Šâ€”â€Šultra-brute -qq server && \\upx -t serverÂ : we compressed the executable usingÂ upx.\nFROM scratch: zero size out of the box docker image. This means that the resulting image size is equal to the binaryÂ size.\nCOPYâ€Šâ€”â€Šfrom=builder /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/ca-certificates.crtÂ : adds root certificates to struct dockerÂ images.\nCMD [â€œ./serverâ€]Â : runs the application.\nLetâ€™s create docker images of the application.\n$ docker build -t app:1.0.0 .\n\nThe docker image size of the application is 5.05Â mb\n\nReducing Docker Image Size with UPX in Spring NativeÂ Projects\n\nWe can compile Spring Boot application as native with Spring native.When I compiled the application as native, the docker image size of the web service was 169 MB. I made some adjustments in the spring-boot-maven-plugin and reduced the image size to 53.4Â MB.\nThe pom.xml is asÂ follows.\nhttps://medium.com/media/32c7eb2c8d61f0ee337ae09be1553a23/href\n<java.version>17</java.version>Â : the application was compiled according to JavaÂ 17\n<spring-native.version>0.11.0-RC1</spring-native.version>Â : the current most up-to-date spring native version is compatible with spring bootÂ 2.6.1.\n<name>docker.io/suayb/native-app:1.0.0</name>Â : the tag of the docker image to beÂ created.\n<BP_NATIVE_IMAGE>true</BP_NATIVE_IMAGE>Â :the native image build pack is enabled. In Spring Native 0.11, the Liberica Native Image Kit (NIK) is the native image compiler distribution used by default with Buildpacks.\n<BP_BINARY_COMPRESSION_METHOD>upx</BP_BINARY_COMPRESSION_METHOD>: it creates a compressed executable using upx. Required to be able to reduce the docker imageÂ size.\nâ€” enable-httpsâ€Šâ€”â€Šenable-httpÂ : http and https are enabled as url protocols.\n<publish>false</publish>Â : does not publish the created image in the docker registry.\n<pullPolicy>IF_NOT_PRESENT</pullPolicy>Â : if the builder docker images do not exist locally, it pulls. This value is ALWAYS byÂ default.\nLetâ€™s create docker images of the application.\n$ mvn spring-boot:build-image\n\nThe docker image size of the application is 53.04Â mb.\n\nMy article ends here. As seen with Upx, we can reduce docker image sizes by 70%. In Docker, we prefer small size image sizes because of the disposability principle. Because large containers take a long time to standÂ up.\nImage size doesnâ€™t affect your programâ€™s performance, but it can speed up many processes.\nSee you in the next articles.",
      "dc:creator": "Åuayb ÅimÅŸek",
      "guid": "https://medium.com/p/daf84e4f9227",
      "categories": ["upx", "golang", "docker", "spring-boot", "spring-native"],
      "isoDate": "2021-12-20T18:03:04.000Z"
    },
    {
      "creator": "Åuayb ÅimÅŸek",
      "title": "Spring Boot Fullstack Blockchain Application With Hyperledger Fabric running on Kubernetes (Part 4)",
      "link": "https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-4-d6583a57153c?source=rss-bda589f2335a------2",
      "pubDate": "Sat, 27 Nov 2021 13:47:38 GMT",
      "content:encoded": "<h3>Spring Boot Fullstack Blockchain Application With Hyperledger Fabric running on Kubernetes (Part 4)â€Šâ€”â€ŠGenerating Certificates and Artifacts</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*RynSwoXhv_P64QKrH4q2ZQ.png\" /></figure><p>Hello everyone, through this article series we will the Hyperledger Fabric integration with Spring Boot.In this article, we will look into generating certificates,genesis block and channel transaction.Also,we will deploy job on kubernetes for this certificates and artifacts generating.</p><p>Other articles on Hyperledger Fabric integration with Spring Boot can be accessed from the linksÂ below.</p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-1-c8f7c87d60eb\">Part 1â€Šâ€”â€ŠIntroduction</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-ddce8d25858d\">Part 2â€Šâ€”â€ŠKubernetes ClusterÂ Setup</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-3-7eb2fc75ba60\">Part 3â€Šâ€”â€ŠFabric CAÂ Server</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-4-d6583a57153c\">Part 4â€” Generating Certificates and Artifacts</a></p><p><a href=\"https://suaybsimsek58.medium.com/1723502770e7\">Part 5â€Šâ€”â€ŠKafka</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-6-b82662ada0a4\">Part 6â€Šâ€”â€ŠOrderer</a></p><p>First, letâ€™s explain some concepts that are often repeated in theÂ article.</p><h3>Anchor Peer</h3><p>Used by gossip to make sure peers in different organizations know about eachÂ other.</p><p>When a configuration block that contains an update to the anchor peers is committed, peers reach out to the anchor peers and learn from them about all of the peers known to the anchor peer(s). Once at least one peer from each organization has contacted an anchor peer, the anchor peer learns about every peer in theÂ channel.</p><h3>ACL</h3><p>An ACL, or Access Control List, associates access to specific peer resources (such as system chaincode APIs or event services) to a <a href=\"https://hyperledger-fabric.readthedocs.io/en/release-2.2/glossary.html#policy\">Policy</a> (which specifies how many and what types of organizations or roles are required).</p><p>The ACL is part of a channelâ€™s configuration.</p><p>A set of default ACLs is provided in the configtx.yaml file which is used by configtxgen to build channel configurations.</p><h3>Block</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/325/0*IJw0E3lqVitY8LeI.png\" /></figure><p>A block contains an ordered set of transactions.</p><p>It is cryptographically linked to the preceding block, and in turn it is linked to be subsequent blocks.</p><p>The first block in such a chain of blocks is called the <strong>genesisÂ block</strong>.</p><p>Blocks are created by the ordering service, and then validated and committed byÂ peers.</p><h3>Channel</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/325/0*prvgWW1lLE62irrC.png\" /></figure><p>A channel is a private blockchain overlay which allows for data isolation and confidentiality.</p><p>A channel-specific ledger is shared across the peers in the channel, and transacting parties must be authenticated to a channel in order to interact withÂ it.</p><h3><strong>Configuration Settings</strong></h3><p>Letâ€™s open the project we downloaded from <a href=\"https://github.com/susimsek/spring-boot-hlf-k8s-fullstack-blockchain-app\">this link</a> and go to the directory where the k8s isÂ located.</p><pre>$ cd deploy/k8s</pre><p>We need the configtx.yaml file to create generate genesis block and channel transaction.The yaml file is in deploy/k8s/fabricfiles/configtx/configtx.yaml.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/695e23290dcec3efa57a53c40d5e80e7/href\">https://medium.com/media/695e23290dcec3efa57a53c40d5e80e7/href</a></iframe><pre>- &amp;OrdererOrg<br><br>    <em># ID to load the MSP definition as<br>    </em>ID: OrdererMSP</pre><p>The ID field is required to load the Orderer MSP definitions.Likewise, this parameter is mandatory in Org1,Org2 andÂ Org3.</p><pre>MSPDir: ../organizations/ordererOrganizations/example.com/msp</pre><p>MSPDir is the filesystem path which contains the MSP configuration.This parameter is mandatory in Org1,Org2 andÂ Org3.</p><pre>OrdererEndpoints:<br>    - orderer:7050</pre><p>For OrdererOrg, it is the service name and service port information of the orderer on kubernetes.</p><pre>AnchorPeers:<br>    <em># AnchorPeers defines the location of peers which can be used<br>    # for cross org gossip communication.  Note, this value is only<br>    # encoded in the genesis block in the Application section context<br>    </em>- Host: peer0-org1<br>      Port: 7051</pre><p>Anchor Peer defines the location of peers which can be used.It must be defined for each organization.it is the service name and service port definition of the peer to be used as the anchor peer for Org1 on kubernetes.The anchor peer must be defined in Org2 andÂ Org3.</p><pre>Orderer: &amp;OrdererDefaults<br><br>    <em># Orderer Type: The orderer implementation to start<br>    </em>OrdererType: kafka</pre><p>With this configuration,Hyperledger Fabric ordering service nodes (OSNs) use your Kafka cluster and provide an ordering service to your blockchain network.</p><pre>Addresses:<br>    - orderer:7050<br>    - orderer2:7050<br>    - orderer3:7050<br>    - orderer4:7050<br>    - orderer5:7050</pre><p>The service name and service port of orderers on kubernetes are defined. 5 orderers are running on blockchain network.</p><pre>Kafka:<br>    <em># Brokers: A list of Kafka brokers to which the orderer connects<br>    # NOTE: Use IP:port notation<br>    </em>Brokers:<br>        - broker-0.broker:9092<br>        - broker-1.broker:9092</pre><p>The service name and service port of the kafka brokers on kubernetes to which the orderer will be connected are defined.We will set up Kafka cluster in this project. Kafka will run 2 instances.</p><pre>TwoOrgsOrdererGenesis:<br>    ...<br>            Organizations:<br>                - *Org1<br>                - *Org2<br>                - *Org3</pre><p>The TwoOrgsOrdererGenesis profile needs to be defined to create a genesis block.3 organizations are defined in this configuration.</p><pre>TwoOrgsChannel:<br>    ...<br>        Organizations:<br>            - *Org1<br>            - *Org2<br>            - *Org3</pre><p>The TwoOrgsChannel profile needs to be defined to create the channel transaction.3 organizations are defined in this configuration.</p><p>The generating certificate scripts defined in the project for the Orderer is as follows.The script file is in the deploy/k8s/fabricfiles/scripts/orderer-certs.shÂ .</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/f341500919f04e7f641fd98534490d46/href\">https://medium.com/media/f341500919f04e7f641fd98534490d46/href</a></iframe><p>.Certificate Authorities are used to generate the identities assigned to admins, nodes, and users (client applications).This script creates tls certificate, msp for each of the orderers and register and enroll identities withÂ CA.</p><p>The generating certificate scripts defined in the project for the Org1,Org2 and Org3 are as follows.The scripts files are in the deploy/k8s/fabricfiles/scripts folder.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/dc3a5f7fcd52849a0b95a067bb9fd876/href\">https://medium.com/media/dc3a5f7fcd52849a0b95a067bb9fd876/href</a></iframe><p>This scripts create tls certificate, msp for each of the organization and register and enroll identities withÂ CA.</p><p>The generating artifact scripts defined in the project are as follows.The scripts files are in the deploy/k8s/fabricfiles/scripts folder.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/57b25494ed8dd23a88d3376b60ed5d86/href\">https://medium.com/media/57b25494ed8dd23a88d3376b60ed5d86/href</a></iframe><pre><em>configtxgen </em>-profile TwoOrgsOrdererGenesis</pre><p>The TwoOrgsOrdererGenesis profile must be defined in the configtx.yaml file. It is required to create a GenesisÂ block.</p><pre>-channelID system-channel</pre><p>To create a Genesis block, the channel id must be system-channel.</p><pre><em>configtxgen </em>-profile TwoOrgsChannel</pre><p>The TwoOrgsChannel profile must be defined in the configtx.yaml file. It is required to create channel transaction.</p><pre>CHANNEL_NAME:=&quot;mychannel&quot;</pre><p>Channel is a private â€œsubnetâ€ of communication between two or more specific network members, for the purpose of conducting private and confidential transactions.Channel id information is required to create a channel transaction.The channel id is assigned â€œmychannelâ€.</p><pre><em>for </em>orgmsp <em>in </em>Org1MSP Org2MSP Org3MSP; <em>do</em><br><em>set </em>-x<br><em>configtxgen </em>-profile TwoOrgsChannel -outputAnchorPeersUpdate ./channel-artifacts/<em>$</em>{orgmsp}anchors.tx -channelID $CHANNEL_NAME -asOrg <em>$</em>{orgmsp}<em><br>done</em></pre><p>Generate anchor peer update transaction for each organization.</p><p>Lets create an certificate creation job, create-certs.yamlÂ .the yaml file is in the deploy/k8s/job folder.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/74694427ddbd7822ba8a6d87c17cece6/href\">https://medium.com/media/74694427ddbd7822ba8a6d87c17cece6/href</a></iframe><pre>- |<br>  ./scripts/orderer-certs.sh &amp;&amp;<br>  ./scripts/org1-certs.sh &amp;&amp;<br>  ./scripts/org2-certs.sh &amp;&amp;<br>  ./scripts/org3-certs.sh</pre><p>Scripts to run in job are defined to create certificates.</p><pre>volumeMounts:<br>  - name: fabricfiles<br>    mountPath: /organizations <em><br>    </em>subPath: organizations <em><br>  </em>- name: fabricfiles<br>    mountPath: /scripts<br>    subPath: scripts </pre><p>Required to access scripts folder and organizations folder on nfs server.Certificates are created in the organizations folder.</p><pre>volumes:<br>  - name: fabricfiles<br>    persistentVolumeClaim:<br>      claimName: fabricfiles-pvc</pre><p>Lets create an artifact creation job, create-certs.yamlÂ .the yaml file is in the deploy/k8s/job folder.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/216e5a46deced1fb1eefa08659529323/href\">https://medium.com/media/216e5a46deced1fb1eefa08659529323/href</a></iframe><p>It allows us to mount the fabric files on the nfs server to the container.</p><pre>- |<br>  ./scripts/createGenesis.sh &amp;&amp;<br>  ./scripts/createChannel.sh</pre><p>Scripts to run in job are defined to create artifacts.</p><pre>volumeMounts:<br>  - name: fabricfiles<br>    mountPath: /organizations<br>    subPath: organizations <em><br>  </em>- name: fabricfiles<br>    mountPath: /configtx <br>    subPath: configtx <em><br>  </em>- name: fabricfiles<br>    mountPath: /system-genesis-block<br>    subPath: system-genesis-block <em><br>  </em>- name: fabricfiles<br>    mountPath: /channel-artifacts<br>    subPath: channel-artifacts <em><br>  </em>- name: fabricfiles<br>    mountPath: /scripts<br>    subPath: scripts</pre><p>Required to access scripts folder,configtx folder and organizations folder on nfs server.System Genesis Block is created in the system-genesis-block folder.Channel artifacts are created in the channel-artifacts folder.</p><h3>Installation of Jobs on Kubernetes</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*KdOJx0kdABrPqhpIs6ff6A.png\" /></figure><p>Letâ€™s connect to the kubernetes master node virtual machine with the <strong>vagrant ssh</strong>Â command.</p><pre>$ vagrant ssh k8smaster</pre><p>Letâ€™s go to the directory where the kubernetes installation scripts are located.This directory is the same as the deploy/k8s folder in the project. With Vagrant, this directory is synchronized to the virtualÂ machine.</p><pre>$ <em>cd</em> /vagrant/k8s</pre><p>Deploying the certificate creating job for peers and orderers.</p><pre>$ kubectl apply -f job/create-certs.yaml</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*HBF19SzxO3yNL8juyCRzFg.png\" /></figure><p>Certificate creation job pending completion</p><pre>$ <em>kubectl </em>wait --for=condition=complete --timeout=300s job create-certs</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*ATKOqimZy7MD1EytVbin1g.png\" /></figure><p>After the job is completed, ordererOrganizations and peerOrganizations were created under the organizations folder on the nfs server.Letâ€™s check if these folders areÂ created.</p><p>Letâ€™s open a new terminal and connect to the nfs server virtualÂ machine.</p><pre>$ vagrant ssh nfsserver</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*TMqxU9UpwVleDXVyGWxVRw.png\" /></figure><p>Letâ€™s list the folders under the organizations folder.</p><pre>$ ls /srv/kubedata/fabricfiles/organizations</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*vTl1bsXtsSCbE1t5BfPD9A.png\" /></figure><p>As you can see, the ordererOrganizations and peerOrganizations folders have been created.Certificate creation completed successfully.</p><p>Lets come back to kubernetes master node terminal.</p><p>Deploying the certificate artifacts job.</p><pre>$ kubectl apply -f job/create-artifacts.yaml</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*57cm7hRPa8_eUIlG-H0Z2g.png\" /></figure><p>Artifact creation job pending completion.</p><pre>$ <em>kubectl </em>wait --for=condition=complete --timeout=300s job create-artifacts</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*pX99koczynryuODs3veFDg.png\" /></figure><p>After the job is completed, system-genesis-block and channel-artifacts were created under the fabricfiles folder on the nfs server.Letâ€™s check if these folders areÂ created.</p><p>Lets come back to nfs server virtual machine terminal.</p><p>Letâ€™s list the folders under the fabricfiles folder.</p><pre>$ ls /srv/kubedata/fabricfiles</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*lmNDYgzoM2z85WmCGinfwA.png\" /></figure><p>As you can see, the system-genesis-block and channel-artifacts folders have been created.Artifacts creation completed successfully.</p><p>Finally, letâ€™s check the conditions of the jobs we run from the lensÂ ide.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*8IgeihJwiV22mm0qB0yGwA.png\" /></figure><p>The jobs we run have been completed.</p><p>My article ends here. In general,I explained generating certificates for peers,orderer and generating genesis block and generating channel transactions on Kubernetes.</p><p>See you in the next articles.</p><h3>Project Links</h3><p>Spring Boot Hlf Starter Project details and installation can be accessed via <a href=\"https://github.com/susimsek/spring-boot-hlf-starter\">thisÂ link</a>.</p><p>Asset Transfer Project details and installation can be accessed via <a href=\"https://github.com/susimsek/spring-boot-hlf-starter\">thisÂ link</a></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=d6583a57153c\" width=\"1\" height=\"1\" alt=\"\">",
      "content:encodedSnippet": "Spring Boot Fullstack Blockchain Application With Hyperledger Fabric running on Kubernetes (Part 4)â€Šâ€”â€ŠGenerating Certificates and Artifacts\n\nHello everyone, through this article series we will the Hyperledger Fabric integration with Spring Boot.In this article, we will look into generating certificates,genesis block and channel transaction.Also,we will deploy job on kubernetes for this certificates and artifacts generating.\nOther articles on Hyperledger Fabric integration with Spring Boot can be accessed from the linksÂ below.\nPart 1â€Šâ€”â€ŠIntroduction\nPart 2â€Šâ€”â€ŠKubernetes ClusterÂ Setup\nPart 3â€Šâ€”â€ŠFabric CAÂ Server\nPart 4â€” Generating Certificates and Artifacts\nPart 5â€Šâ€”â€ŠKafka\nPart 6â€Šâ€”â€ŠOrderer\nFirst, letâ€™s explain some concepts that are often repeated in theÂ article.\nAnchor Peer\nUsed by gossip to make sure peers in different organizations know about eachÂ other.\nWhen a configuration block that contains an update to the anchor peers is committed, peers reach out to the anchor peers and learn from them about all of the peers known to the anchor peer(s). Once at least one peer from each organization has contacted an anchor peer, the anchor peer learns about every peer in theÂ channel.\nACL\nAn ACL, or Access Control List, associates access to specific peer resources (such as system chaincode APIs or event services) to a Policy (which specifies how many and what types of organizations or roles are required).\nThe ACL is part of a channelâ€™s configuration.\nA set of default ACLs is provided in the configtx.yaml file which is used by configtxgen to build channel configurations.\nBlock\n\nA block contains an ordered set of transactions.\nIt is cryptographically linked to the preceding block, and in turn it is linked to be subsequent blocks.\nThe first block in such a chain of blocks is called the genesisÂ block.\nBlocks are created by the ordering service, and then validated and committed byÂ peers.\nChannel\n\nA channel is a private blockchain overlay which allows for data isolation and confidentiality.\nA channel-specific ledger is shared across the peers in the channel, and transacting parties must be authenticated to a channel in order to interact withÂ it.\nConfiguration Settings\nLetâ€™s open the project we downloaded from this link and go to the directory where the k8s isÂ located.\n$ cd deploy/k8s\nWe need the configtx.yaml file to create generate genesis block and channel transaction.The yaml file is in deploy/k8s/fabricfiles/configtx/configtx.yaml.\nhttps://medium.com/media/695e23290dcec3efa57a53c40d5e80e7/href\n- &OrdererOrg\n    # ID to load the MSP definition as\n    ID: OrdererMSP\nThe ID field is required to load the Orderer MSP definitions.Likewise, this parameter is mandatory in Org1,Org2 andÂ Org3.\nMSPDir: ../organizations/ordererOrganizations/example.com/msp\nMSPDir is the filesystem path which contains the MSP configuration.This parameter is mandatory in Org1,Org2 andÂ Org3.\nOrdererEndpoints:\n    - orderer:7050\nFor OrdererOrg, it is the service name and service port information of the orderer on kubernetes.\nAnchorPeers:\n    # AnchorPeers defines the location of peers which can be used\n    # for cross org gossip communication.  Note, this value is only\n    # encoded in the genesis block in the Application section context\n    - Host: peer0-org1\n      Port: 7051\nAnchor Peer defines the location of peers which can be used.It must be defined for each organization.it is the service name and service port definition of the peer to be used as the anchor peer for Org1 on kubernetes.The anchor peer must be defined in Org2 andÂ Org3.\nOrderer: &OrdererDefaults\n    # Orderer Type: The orderer implementation to start\n    OrdererType: kafka\nWith this configuration,Hyperledger Fabric ordering service nodes (OSNs) use your Kafka cluster and provide an ordering service to your blockchain network.\nAddresses:\n    - orderer:7050\n    - orderer2:7050\n    - orderer3:7050\n    - orderer4:7050\n    - orderer5:7050\nThe service name and service port of orderers on kubernetes are defined. 5 orderers are running on blockchain network.\nKafka:\n    # Brokers: A list of Kafka brokers to which the orderer connects\n    # NOTE: Use IP:port notation\n    Brokers:\n        - broker-0.broker:9092\n        - broker-1.broker:9092\nThe service name and service port of the kafka brokers on kubernetes to which the orderer will be connected are defined.We will set up Kafka cluster in this project. Kafka will run 2 instances.\nTwoOrgsOrdererGenesis:\n    ...\n            Organizations:\n                - *Org1\n                - *Org2\n                - *Org3\nThe TwoOrgsOrdererGenesis profile needs to be defined to create a genesis block.3 organizations are defined in this configuration.\nTwoOrgsChannel:\n    ...\n        Organizations:\n            - *Org1\n            - *Org2\n            - *Org3\nThe TwoOrgsChannel profile needs to be defined to create the channel transaction.3 organizations are defined in this configuration.\nThe generating certificate scripts defined in the project for the Orderer is as follows.The script file is in the deploy/k8s/fabricfiles/scripts/orderer-certs.shÂ .\nhttps://medium.com/media/f341500919f04e7f641fd98534490d46/href\n.Certificate Authorities are used to generate the identities assigned to admins, nodes, and users (client applications).This script creates tls certificate, msp for each of the orderers and register and enroll identities withÂ CA.\nThe generating certificate scripts defined in the project for the Org1,Org2 and Org3 are as follows.The scripts files are in the deploy/k8s/fabricfiles/scripts folder.\nhttps://medium.com/media/dc3a5f7fcd52849a0b95a067bb9fd876/href\nThis scripts create tls certificate, msp for each of the organization and register and enroll identities withÂ CA.\nThe generating artifact scripts defined in the project are as follows.The scripts files are in the deploy/k8s/fabricfiles/scripts folder.\nhttps://medium.com/media/57b25494ed8dd23a88d3376b60ed5d86/href\nconfigtxgen -profile TwoOrgsOrdererGenesis\nThe TwoOrgsOrdererGenesis profile must be defined in the configtx.yaml file. It is required to create a GenesisÂ block.\n-channelID system-channel\nTo create a Genesis block, the channel id must be system-channel.\nconfigtxgen -profile TwoOrgsChannel\nThe TwoOrgsChannel profile must be defined in the configtx.yaml file. It is required to create channel transaction.\nCHANNEL_NAME:=\"mychannel\"\nChannel is a private â€œsubnetâ€ of communication between two or more specific network members, for the purpose of conducting private and confidential transactions.Channel id information is required to create a channel transaction.The channel id is assigned â€œmychannelâ€.\nfor orgmsp in Org1MSP Org2MSP Org3MSP; do\nset -x\nconfigtxgen -profile TwoOrgsChannel -outputAnchorPeersUpdate ./channel-artifacts/${orgmsp}anchors.tx -channelID $CHANNEL_NAME -asOrg ${orgmsp}\ndone\nGenerate anchor peer update transaction for each organization.\nLets create an certificate creation job, create-certs.yamlÂ .the yaml file is in the deploy/k8s/job folder.\nhttps://medium.com/media/74694427ddbd7822ba8a6d87c17cece6/href\n- |\n  ./scripts/orderer-certs.sh &&\n  ./scripts/org1-certs.sh &&\n  ./scripts/org2-certs.sh &&\n  ./scripts/org3-certs.sh\nScripts to run in job are defined to create certificates.\nvolumeMounts:\n  - name: fabricfiles\n    mountPath: /organizations \n    subPath: organizations \n  - name: fabricfiles\n    mountPath: /scripts\n    subPath: scripts \nRequired to access scripts folder and organizations folder on nfs server.Certificates are created in the organizations folder.\nvolumes:\n  - name: fabricfiles\n    persistentVolumeClaim:\n      claimName: fabricfiles-pvc\nLets create an artifact creation job, create-certs.yamlÂ .the yaml file is in the deploy/k8s/job folder.\nhttps://medium.com/media/216e5a46deced1fb1eefa08659529323/href\nIt allows us to mount the fabric files on the nfs server to the container.\n- |\n  ./scripts/createGenesis.sh &&\n  ./scripts/createChannel.sh\nScripts to run in job are defined to create artifacts.\nvolumeMounts:\n  - name: fabricfiles\n    mountPath: /organizations\n    subPath: organizations \n  - name: fabricfiles\n    mountPath: /configtx \n    subPath: configtx \n  - name: fabricfiles\n    mountPath: /system-genesis-block\n    subPath: system-genesis-block \n  - name: fabricfiles\n    mountPath: /channel-artifacts\n    subPath: channel-artifacts \n  - name: fabricfiles\n    mountPath: /scripts\n    subPath: scripts\nRequired to access scripts folder,configtx folder and organizations folder on nfs server.System Genesis Block is created in the system-genesis-block folder.Channel artifacts are created in the channel-artifacts folder.\nInstallation of Jobs on Kubernetes\n\nLetâ€™s connect to the kubernetes master node virtual machine with the vagrant sshÂ command.\n$ vagrant ssh k8smaster\nLetâ€™s go to the directory where the kubernetes installation scripts are located.This directory is the same as the deploy/k8s folder in the project. With Vagrant, this directory is synchronized to the virtualÂ machine.\n$ cd /vagrant/k8s\nDeploying the certificate creating job for peers and orderers.\n$ kubectl apply -f job/create-certs.yaml\n\nCertificate creation job pending completion\n$ kubectl wait --for=condition=complete --timeout=300s job create-certs\n\nAfter the job is completed, ordererOrganizations and peerOrganizations were created under the organizations folder on the nfs server.Letâ€™s check if these folders areÂ created.\nLetâ€™s open a new terminal and connect to the nfs server virtualÂ machine.\n$ vagrant ssh nfsserver\n\nLetâ€™s list the folders under the organizations folder.\n$ ls /srv/kubedata/fabricfiles/organizations\n\nAs you can see, the ordererOrganizations and peerOrganizations folders have been created.Certificate creation completed successfully.\nLets come back to kubernetes master node terminal.\nDeploying the certificate artifacts job.\n$ kubectl apply -f job/create-artifacts.yaml\n\nArtifact creation job pending completion.\n$ kubectl wait --for=condition=complete --timeout=300s job create-artifacts\n\nAfter the job is completed, system-genesis-block and channel-artifacts were created under the fabricfiles folder on the nfs server.Letâ€™s check if these folders areÂ created.\nLets come back to nfs server virtual machine terminal.\nLetâ€™s list the folders under the fabricfiles folder.\n$ ls /srv/kubedata/fabricfiles\n\nAs you can see, the system-genesis-block and channel-artifacts folders have been created.Artifacts creation completed successfully.\nFinally, letâ€™s check the conditions of the jobs we run from the lensÂ ide.\n\nThe jobs we run have been completed.\nMy article ends here. In general,I explained generating certificates for peers,orderer and generating genesis block and generating channel transactions on Kubernetes.\nSee you in the next articles.\nProject Links\nSpring Boot Hlf Starter Project details and installation can be accessed via thisÂ link.\nAsset Transfer Project details and installation can be accessed via thisÂ link",
      "dc:creator": "Åuayb ÅimÅŸek",
      "guid": "https://medium.com/p/d6583a57153c",
      "categories": ["blockchain", "kubernetes", "hyperledger", "hyperledger-fabric", "spring-boot"],
      "isoDate": "2021-11-27T13:47:38.000Z"
    },
    {
      "creator": "Åuayb ÅimÅŸek",
      "title": "Spring Boot Fullstack Blockchain Application With Hyperledger Fabric running on Kubernetes (Part 3)",
      "link": "https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-3-7eb2fc75ba60?source=rss-bda589f2335a------2",
      "pubDate": "Sun, 21 Nov 2021 14:33:12 GMT",
      "content:encoded": "<h3>Spring Boot Fullstack Blockchain Application With Hyperledger Fabric running on Kubernetes (Part 3)â€Šâ€”â€ŠFabric CAÂ Server</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*S9G4q0odpEbDjhcIowUDQw.png\" /></figure><p>Hello everyone, through this article series we will the Hyperledger Fabric integration with Spring Boot.In this article, we will look into Fabric CA Server and installation of Fabric CA Server on Kubernetes.</p><p>Other articles on Hyperledger Fabric integration with Spring Boot can be accessed from the linksÂ below.</p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-1-c8f7c87d60eb\">Part 1â€Šâ€”â€ŠIntroduction</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-ddce8d25858d\">Part 2â€” Kubernetes ClusterÂ Setup</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-3-7eb2fc75ba60\">Part 3â€” Fabric CAÂ Server</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-4-d6583a57153c\">Part 4â€Šâ€”â€ŠGenerating Certificates and Artifacts</a></p><p><a href=\"https://suaybsimsek58.medium.com/1723502770e7\">Part 5â€Šâ€”â€ŠKafka</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-6-b82662ada0a4\">Part 6â€Šâ€”â€ŠOrderer</a></p><h3>What is Fabric CAÂ Server?</h3><p>Fabric CA is a Certificate Authority (CA) for Hyperledger Fabric.</p><p>It provides features suchÂ as:</p><ul><li>registration of identities, or connects to LDAP as the userÂ registry</li><li>issuance of Enrollment Certificates (ECerts)</li><li>certificate renewal and revocation</li></ul><p>The diagram below illustrates how the Hyperledger Fabric CA server fits into the overall Hyperledger Fabric architecture.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/884/1*EUg8Gmew5iO6kiqgYjEdbQ.png\" /></figure><p>There are two ways of interacting with a Hyperledger Fabric CA server: via the Hyperledger Fabric CA client or through one of the Fabric SDKs. All communication to the Hyperledger Fabric CA server is via RESTÂ APIs.</p><p>The Hyperledger Fabric CA client or SDK may connect to a server in a cluster of Hyperledger Fabric CA servers. This is illustrated in the top right section of the diagram. The client routes to an HA Proxy endpoint which load balances traffic to one of the fabric-ca-server clusterÂ members.</p><p>A server may contain multiple CAs. Each CA is either a root CA or an intermediate CA. Each intermediate CA has a parent CA which is either a root CA or another intermediate CA.</p><h3><strong>Configuration</strong> Fabric CAÂ Server</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*fKBNd-R9xUgwkybAqRo1jw.png\" /></figure><p>Letâ€™s open the project we downloaded from <a href=\"https://github.com/susimsek/spring-boot-hlf-k8s-fullstack-blockchain-app\">this link</a> and go to the directory where the ca isÂ located.</p><pre>$ cd deploy/k8s</pre><p><strong>Configuration Settings</strong></p><p>The Fabric CA provides 3 ways to configure settings on the Fabric CA server and client. The precedence orderÂ is:</p><p>1. CLIÂ flags</p><p>2. Environment variables</p><p>3. Configuration file</p><p>We will configure the fabric ca server with the configuration file.</p><p>We will set up 4 Fabric CAservers for orderer, org1, org2 and org3.It is necessary to define a configuration file for each Fabric CAÂ server.</p><p>The Fabric CA configuration file defined in the project for the Orderer is as follows.the configuration file is in the deploy/k8s/fabricfiles/organizations/fabric-ca/ordererOrg folder.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/29e4118b63e1c508812e25622f641b59/href\">https://medium.com/media/29e4118b63e1c508812e25622f641b59/href</a></iframe><pre>csr:<br>   ...<br>   hosts:<br>     - localhost<br>     - example.com<br>     - ca-orderer</pre><p>Added localhost,example.com,ca-orderer as csr host to the configÂ file.</p><p>ca-orderer is the service name of fabric ca server in kubernetes.</p><pre>ca:<br>  <em># Name of this CA<br>  </em>name: OrdererCA</pre><p>OrdererCA is the CAÂ name.</p><pre>csr:<br>   cn: ca-org1<br>   names:<br>      - C: US<br>        ST: &quot;New York&quot;<br>        L: &quot;New York&quot;<br>        O: ca-org1<br>        OU: ca-org1</pre><p>All of the fields above pertain to the X.509 signing key and certificate which is generated by the fabric-ca-server init. This corresponds to the ca.certfile and ca.keyfile files in the serverâ€™s configuration file. The fields are asÂ follows:</p><ul><li><strong>cn</strong> is the CommonÂ Name</li><li><strong>O</strong> is the organization name</li><li><strong>OU</strong> is the organizational unit</li><li><strong>L</strong> is the location orÂ city</li><li><strong>ST</strong> is theÂ state</li><li><strong>C</strong> is theÂ country</li></ul><pre>registry:<br>  ...<em><br>  </em>identities:<br>     - name: admin<br>       pass: adminpw<br>       type: client<br>       affiliation: &quot;&quot;</pre><p>it must be configured with at least one pre-registered bootstrap identity to enable you to register and enroll other identities.The -b option specifies the name and password for a bootstrap identity.</p><pre>db:<br>  type: sqlite3<br>  datasource: fabric-ca-server.db</pre><p>The default database is SQLite and the default database file is fabric-ca-server.db in the Fabric CA serverâ€™s home directory. Fabric CA server can also connect to PostgreSQL or MySQL databases.</p><p>Fabric CA supports the following database versions in a clusterÂ setup:</p><ul><li>PostgreSQL: 9.5.5 orÂ later</li><li>MySQL: 5.7 orÂ later</li></ul><pre>affiliations:<br>   org1:<br>      - department1<br>      - department2<br>   org2:<br>      - department1</pre><p>I think of affiliations as hierarchical tags. Each identity can be tagged (affiliated) to (with) one affiliation in the hierarchy. When an identity is associated with an affiliation, it is affiliated with that and all the child affiliations.Affiliations are currently used during registration and revocation.</p><p>The Fabric CA configuration file defined in the project for the Org1 is as follows.The configuration file is in the deploy/k8s/fabricfiles/organizations/fabric-ca/org1 folder.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/295956ebcc3ae128e2627175f0876e64/href\">https://medium.com/media/295956ebcc3ae128e2627175f0876e64/href</a></iframe><pre>csr:<br>   ...<br>   hosts:<br>     - localhost<br>     - example.com<br>     - ca-org1</pre><p>Added localhost,example.com,ca-org1 as csr host to the configÂ file.</p><p>ca-org1 is the service name of fabric ca server in kubernetes.</p><pre>ca:<br>  <em># Name of this CA<br>  </em>name: Org1CA</pre><p>Org1CA is the CAÂ name.</p><pre>csr:<br>   cn: ca-org1<br>   names:<br>      - C: US<br>        ST: &quot;New York&quot;<br>        L: &quot;New York&quot;<br>        O: ca-org1<br>        OU: ca-org1</pre><p>All of the fields above pertain to the X.509 signing key and certificate which is generated by the fabric-ca-server init.</p><pre>affiliations:<br>   org1:<br>      - department1<br>      - department2<br>   org2:<br>      - department1<br>   org3:<br>      - department1</pre><p>The affiliation definitions differ from the CA Orderer configuration file.Affiliation definitions have been added in Org2 and Org3.Database and bootstrap user configurations are the same as caÂ orderer.</p><p>The Fabric CA configuration file defined in the project for the Org2 is as follows.The configuration file is in the deploy/k8s/fabricfiles/organizations/fabric-ca/org2 folder.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/214a2d3e86b8b7cded53e56907794e6b/href\">https://medium.com/media/214a2d3e86b8b7cded53e56907794e6b/href</a></iframe><p>Csr settings and CA name are different from CA Org1 configuration. Other configurations are the same as caÂ org1.</p><pre>csr:<br>   cn: ca-org2<br>   names:<br>      - C: US<br>        ST: &quot;New York&quot;<br>        L: &quot;New York&quot;<br>        O: ca-org2<br>        OU: ca-org2<br>   hosts:<br>     - localhost<br>     - example.com<br>     - ca-org2</pre><p>ca-org2 is the service name of fabric ca server in kubernetes.</p><pre>ca:<br>  <em># Name of this CA<br>  </em>name: Org2CA</pre><p>The Fabric CA configuration file defined in the project for the Org3 is as follows.the configuration file is in the deploy/k8s/fabricfiles/organizations/fabric-ca/org3 folder.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/aef09ba77b4e984307ac6bcdc4faf023/href\">https://medium.com/media/aef09ba77b4e984307ac6bcdc4faf023/href</a></iframe><p>Csr settings and CA name are different from CA Org1 configuration. Other configurations are the same as caÂ org1.</p><pre>csr:<br>   cn: ca-org3<br>   names:<br>      - C: US<br>        ST: &quot;New York&quot;<br>        L: &quot;New York&quot;<br>        O: ca-org3<br>        OU: ca-org3<br>   hosts:<br>     - localhost<br>     - example.com<br>     - ca-org3</pre><p>ca-org3 is the service name of fabric ca server in kubernetes.</p><pre>ca:<br>  <em># Name of this CA<br>  </em>name: Org3CA</pre><p>After setting Fabric CA Server configurations for Orderer,Org1,Org2 and Org3, we can deploy CA Servers to kubernetes.</p><h3>Installation of Fabric CA Server on Kubernetes</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/922/1*nojGPkEmyjulIIyPGv6k9w.png\" /></figure><p>Letâ€™s create persistence volume and persistence volume claim to access fabric files from the pod,fabricfiles-pv.yaml and fabricfiles-pvc.yaml.</p><p>The fabricfiles-pv.yaml file is in the deploy/k8s/pv folder.</p><p>The fabricfiles-pvc.yaml file is in the deploy/k8s/pvc folder.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/cedebf67a436edecb0cedf0dfc21e5d3/href\">https://medium.com/media/cedebf67a436edecb0cedf0dfc21e5d3/href</a></iframe><pre>nfs:<br>  path: /srv/kubedata/fabricfiles<br>  server: 192.168.12.9</pre><p>192.168.12.9 is the ip of the nfsÂ server.</p><p>/srv/kubedata/fabricfiles is the directory where the fabric files are located on the nfsÂ server.</p><p>lets create an Fabric Orderer CA server deployment and service, ca-orderer.yaml and ca-orderer-svc.yaml.The yaml files is in the deploy/k8s/ca folder.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/64a53d8b9a94c102168ff620ddd6df47/href\">https://medium.com/media/64a53d8b9a94c102168ff620ddd6df47/href</a></iframe><pre>kind: Service<br>metadata:<br>  name: ca-orderer</pre><p>ca-orderer is the kubernetes service name of the Orderer Fabric CA server.It must be added in the csr host configuration in the Fabric CA Configuration file.</p><pre>ports:<br>- protocol: TCP<br>  targetPort: 10054 <em><br>  </em>port: 10054 </pre><p>targetPort is the container internal port.10054</p><p>port is the service port.10054</p><pre>&quot;fabric-ca-server&quot;,<br>&quot;start&quot;, &quot;-b&quot;, &quot;admin:adminpw&quot;, &quot;--port&quot;, &quot;10054&quot;, &quot;-d&quot;</pre><p>The -b (bootstrap identity) option is required for initialization when LDAP is disabled. At least one bootstrap identity is required to start the Fabric CA server; this identity is the server administrator.</p><pre>admin:adminpw </pre><p>server admin information.Admin username is admin,Admin password isÂ adminpw.</p><pre>--port&quot;, &quot;10054&quot;</pre><p>10054 is assigned as the ca server container port.</p><pre>env:<br>  - name: FABRIC_CA_SERVER_CA_NAME<br>    value: ca-orderer<br>  - name: FABRIC_CA_SERVER_TLS_ENABLED<br>    value: &quot;true&quot;</pre><p>Tls aws active and ca name ca-orderer is assigned.</p><pre>volumeMounts:<br>  - name: data<br>    mountPath: /etc/hyperledger/fabric-ca-server<br>    subPath: organizations/fabric-ca/ordererOrg</pre><p>organizations/fabric-ca/ordererOrg: path of ca server configuration file in fabricÂ files</p><p>lets create an Fabric CA Org1 server deployment and service, ca-org1.yaml and ca-org1-svc.yaml.the yaml files are in the deploy/k8s/ca folder.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/e0bae4d673e3b457a0647e232d5c98d0/href\">https://medium.com/media/e0bae4d673e3b457a0647e232d5c98d0/href</a></iframe><pre>kind: Service<br>metadata:<br>  name: ca-org1</pre><p>ca-org1 is the kubernetes service name of the Org1 Fabric CA server.It must be added in the csr host configuration in the Fabric CA Configuration file.</p><pre>ports:<br>- protocol: TCP<br>  targetPort: 7054 <em><br>  </em>port: 7054</pre><p>targetPort is the container internal port.7054</p><p>port is the service port.7054</p><pre>--port&quot;, &quot;7054&quot;</pre><p>7054 is assigned as the ca server container port.</p><pre>- name: FABRIC_CA_SERVER_CA_NAME<br>  value: ca-org1<br>- name: FABRIC_CA_SERVER_TLS_ENABLED<br>  value: &quot;true&quot;<br>- name: FABRIC_CA_SERVER_CSR_CN<br>  value: &quot;ca-org1&quot;<br>- name: FABRIC_CA_SERVER_CSR_HOSTS<br>  value: &quot;ca-org1&quot;</pre><p>Tls was activated, ca name ca-org1 was assigned, and csr host and cn ca-org1 were assigned.</p><pre>volumeMounts:<br>  - name: data<br>    mountPath: /etc/hyperledger/fabric-ca-server<br>    subPath: organizations/fabric-ca/org1</pre><p>organizations/fabric-ca/org1: path of ca server configuration file in fabricÂ files</p><p>lets create an Fabric CA Org2 server deployment and service, ca-org2.yaml and ca-org2-svc.yaml.the yaml files are in the deploy/k8s/ca folder.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/fa8787671f4500c5f0ec8e5e957976bc/href\">https://medium.com/media/fa8787671f4500c5f0ec8e5e957976bc/href</a></iframe><pre>kind: Service<br>metadata:<br>  name: ca-org2</pre><p>ca-org2 is the kubernetes service name of the Org2 Fabric CA server.It must be added in the csr host configuration in the Fabric CA Configuration file.</p><pre>ports:<br>- protocol: TCP<br>  targetPort: 8054 <em><br>  </em>port: 8054 </pre><p>targetPort is the container internal port.8054</p><p>port is the service port.8054</p><pre>--port&quot;, &quot;8054&quot;</pre><p>8054 is assigned as the ca server container port.</p><pre>- name: FABRIC_CA_SERVER_CA_NAME<br>  value: ca-org2<br>- name: FABRIC_CA_SERVER_TLS_ENABLED<br>  value: &quot;true&quot;<br>- name: FABRIC_CA_SERVER_CSR_CN<br>  value: &quot;ca-org2&quot;<br>- name: FABRIC_CA_SERVER_CSR_HOSTS<br>  value: &quot;ca-org2&quot;</pre><p>Tls was activated, ca name ca-org2 was assigned, and csr host and cn ca-org2 were assigned.</p><pre>volumeMounts:<br>  - name: data<br>    mountPath: /etc/hyperledger/fabric-ca-server<br>    subPath: organizations/fabric-ca/org2</pre><p>organizations/fabric-ca/org2: path of ca server configuration file in fabricÂ files</p><p>lets create an Fabric CA Org3 server deployment and service, ca-org3.yaml and ca-org3-svc.yaml.the yaml files is in the deploy/k8s/ca folder.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/c6194cb554cfb69b415ee745346d6c43/href\">https://medium.com/media/c6194cb554cfb69b415ee745346d6c43/href</a></iframe><pre>kind: Service<br>metadata:<br>  name: ca-org3</pre><p>ca-org3 is the kubernetes service name of the Org3 Fabric CA server.It must be added in the csr host configuration in the Fabric CA Configuration file.</p><pre>ports:<br>- protocol: TCP<br>  targetPort: 9054 <em><br>  </em>port: 9054</pre><p>targetPort is the container internal port.9054</p><p>port is the service port.9054</p><pre>--port&quot;, &quot;9054&quot;</pre><p>9054 is assigned as the ca server container port.</p><pre>- name: FABRIC_CA_SERVER_CA_NAME<br>  value: ca-org3<br>- name: FABRIC_CA_SERVER_TLS_ENABLED<br>  value: &quot;true&quot;<br>- name: FABRIC_CA_SERVER_CSR_CN<br>  value: &quot;ca-org3&quot;<br>- name: FABRIC_CA_SERVER_CSR_HOSTS<br>  value: &quot;ca-org3&quot;</pre><p>Tls was activated, ca name ca-org3 was assigned, and csr host and cn ca-org3 were assigned.</p><pre>volumeMounts:<br>  - name: data<br>    mountPath: /etc/hyperledger/fabric-ca-server<br>    subPath: organizations/fabric-ca/org3</pre><p>organizations/fabric-ca/org3: path of ca server configuration file in fabricÂ files</p><p>Letâ€™s define a job that creates the certificates for orderer,org1,org2 andÂ org3.</p><p>Letâ€™s connect to the kubernetes master node virtual machine with the <strong>vagrant ssh</strong>Â command.</p><pre>$ vagrant ssh k8smaster</pre><p>Letâ€™s go to the directory where the kubernetes installation scripts are located.This directory is the same as the deploy/k8s folder in the project. With Vagrant, this directory is synchronized to the virtualÂ machine.</p><pre>$ <em>cd</em> /vagrant/k8s</pre><p>Deploying the persistence volume for fabricÂ files</p><pre>$ kubectl apply -f pv/fabricfiles-pv.yaml</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*ZD7joKRdD6roe6G3dSz1EQ.png\" /></figure><p>Deploying the persistence volume claim for fabricÂ files</p><pre>$ kubectl apply -f pvc/fabricfiles-pvc.yaml</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*84R17p_E8SirPgdTQ5CDQQ.png\" /></figure><p>Deploying the fabric caÂ server</p><pre>$ <em>kubectl </em>apply -f ca/</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*kM3uuCXEnB830NaihhFq7g.png\" /></figure><p>Fabric ca server creation pending completion</p><pre>$ <em>kubectl </em>wait --for condition=available --timeout=300s deployment -l &quot;app in (ca-orderer,ca-org1,ca-org2,ca-org3)&quot;</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*xLAXkMaAPyLEGGifkmnwMg.png\" /></figure><p>Finally, letâ€™s check whether our pods from the lens ide areÂ running.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*s-mL0tBGbWIv83dVeFCr4A.png\" /></figure><p>All of fabric ca server pods areÂ running.</p><p>My article ends here. In general,I explained Fabric CA Server introduction and installation of Fabric CA Server on Kubernetes.</p><p>See you in the next articles.</p><h3>Project Links</h3><p>Spring Boot Hlf Starter Project details and installation can be accessed via <a href=\"https://github.com/susimsek/spring-boot-hlf-starter\">thisÂ link</a>.</p><p>Asset Transfer Project details and installation can be accessed via <a href=\"https://github.com/susimsek/spring-boot-hlf-starter\">thisÂ link</a></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=7eb2fc75ba60\" width=\"1\" height=\"1\" alt=\"\">",
      "content:encodedSnippet": "Spring Boot Fullstack Blockchain Application With Hyperledger Fabric running on Kubernetes (Part 3)â€Šâ€”â€ŠFabric CAÂ Server\n\nHello everyone, through this article series we will the Hyperledger Fabric integration with Spring Boot.In this article, we will look into Fabric CA Server and installation of Fabric CA Server on Kubernetes.\nOther articles on Hyperledger Fabric integration with Spring Boot can be accessed from the linksÂ below.\nPart 1â€Šâ€”â€ŠIntroduction\nPart 2â€” Kubernetes ClusterÂ Setup\nPart 3â€” Fabric CAÂ Server\nPart 4â€Šâ€”â€ŠGenerating Certificates and Artifacts\nPart 5â€Šâ€”â€ŠKafka\nPart 6â€Šâ€”â€ŠOrderer\nWhat is Fabric CAÂ Server?\nFabric CA is a Certificate Authority (CA) for Hyperledger Fabric.\nIt provides features suchÂ as:\n\nregistration of identities, or connects to LDAP as the userÂ registry\nissuance of Enrollment Certificates (ECerts)\ncertificate renewal and revocation\n\nThe diagram below illustrates how the Hyperledger Fabric CA server fits into the overall Hyperledger Fabric architecture.\n\nThere are two ways of interacting with a Hyperledger Fabric CA server: via the Hyperledger Fabric CA client or through one of the Fabric SDKs. All communication to the Hyperledger Fabric CA server is via RESTÂ APIs.\nThe Hyperledger Fabric CA client or SDK may connect to a server in a cluster of Hyperledger Fabric CA servers. This is illustrated in the top right section of the diagram. The client routes to an HA Proxy endpoint which load balances traffic to one of the fabric-ca-server clusterÂ members.\nA server may contain multiple CAs. Each CA is either a root CA or an intermediate CA. Each intermediate CA has a parent CA which is either a root CA or another intermediate CA.\nConfiguration Fabric CAÂ Server\n\nLetâ€™s open the project we downloaded from this link and go to the directory where the ca isÂ located.\n$ cd deploy/k8s\nConfiguration Settings\nThe Fabric CA provides 3 ways to configure settings on the Fabric CA server and client. The precedence orderÂ is:\n1. CLIÂ flags\n2. Environment variables\n3. Configuration file\nWe will configure the fabric ca server with the configuration file.\nWe will set up 4 Fabric CAservers for orderer, org1, org2 and org3.It is necessary to define a configuration file for each Fabric CAÂ server.\nThe Fabric CA configuration file defined in the project for the Orderer is as follows.the configuration file is in the deploy/k8s/fabricfiles/organizations/fabric-ca/ordererOrg folder.\nhttps://medium.com/media/29e4118b63e1c508812e25622f641b59/href\ncsr:\n   ...\n   hosts:\n     - localhost\n     - example.com\n     - ca-orderer\nAdded localhost,example.com,ca-orderer as csr host to the configÂ file.\nca-orderer is the service name of fabric ca server in kubernetes.\nca:\n  # Name of this CA\n  name: OrdererCA\nOrdererCA is the CAÂ name.\ncsr:\n   cn: ca-org1\n   names:\n      - C: US\n        ST: \"New York\"\n        L: \"New York\"\n        O: ca-org1\n        OU: ca-org1\nAll of the fields above pertain to the X.509 signing key and certificate which is generated by the fabric-ca-server init. This corresponds to the ca.certfile and ca.keyfile files in the serverâ€™s configuration file. The fields are asÂ follows:\n\ncn is the CommonÂ Name\nO is the organization name\nOU is the organizational unit\nL is the location orÂ city\nST is theÂ state\nC is theÂ country\n\nregistry:\n  ...\n  identities:\n     - name: admin\n       pass: adminpw\n       type: client\n       affiliation: \"\"\nit must be configured with at least one pre-registered bootstrap identity to enable you to register and enroll other identities.The -b option specifies the name and password for a bootstrap identity.\ndb:\n  type: sqlite3\n  datasource: fabric-ca-server.db\nThe default database is SQLite and the default database file is fabric-ca-server.db in the Fabric CA serverâ€™s home directory. Fabric CA server can also connect to PostgreSQL or MySQL databases.\nFabric CA supports the following database versions in a clusterÂ setup:\n\nPostgreSQL: 9.5.5 orÂ later\nMySQL: 5.7 orÂ later\n\naffiliations:\n   org1:\n      - department1\n      - department2\n   org2:\n      - department1\nI think of affiliations as hierarchical tags. Each identity can be tagged (affiliated) to (with) one affiliation in the hierarchy. When an identity is associated with an affiliation, it is affiliated with that and all the child affiliations.Affiliations are currently used during registration and revocation.\nThe Fabric CA configuration file defined in the project for the Org1 is as follows.The configuration file is in the deploy/k8s/fabricfiles/organizations/fabric-ca/org1 folder.\nhttps://medium.com/media/295956ebcc3ae128e2627175f0876e64/href\ncsr:\n   ...\n   hosts:\n     - localhost\n     - example.com\n     - ca-org1\nAdded localhost,example.com,ca-org1 as csr host to the configÂ file.\nca-org1 is the service name of fabric ca server in kubernetes.\nca:\n  # Name of this CA\n  name: Org1CA\nOrg1CA is the CAÂ name.\ncsr:\n   cn: ca-org1\n   names:\n      - C: US\n        ST: \"New York\"\n        L: \"New York\"\n        O: ca-org1\n        OU: ca-org1\nAll of the fields above pertain to the X.509 signing key and certificate which is generated by the fabric-ca-server init.\naffiliations:\n   org1:\n      - department1\n      - department2\n   org2:\n      - department1\n   org3:\n      - department1\nThe affiliation definitions differ from the CA Orderer configuration file.Affiliation definitions have been added in Org2 and Org3.Database and bootstrap user configurations are the same as caÂ orderer.\nThe Fabric CA configuration file defined in the project for the Org2 is as follows.The configuration file is in the deploy/k8s/fabricfiles/organizations/fabric-ca/org2 folder.\nhttps://medium.com/media/214a2d3e86b8b7cded53e56907794e6b/href\nCsr settings and CA name are different from CA Org1 configuration. Other configurations are the same as caÂ org1.\ncsr:\n   cn: ca-org2\n   names:\n      - C: US\n        ST: \"New York\"\n        L: \"New York\"\n        O: ca-org2\n        OU: ca-org2\n   hosts:\n     - localhost\n     - example.com\n     - ca-org2\nca-org2 is the service name of fabric ca server in kubernetes.\nca:\n  # Name of this CA\n  name: Org2CA\nThe Fabric CA configuration file defined in the project for the Org3 is as follows.the configuration file is in the deploy/k8s/fabricfiles/organizations/fabric-ca/org3 folder.\nhttps://medium.com/media/aef09ba77b4e984307ac6bcdc4faf023/href\nCsr settings and CA name are different from CA Org1 configuration. Other configurations are the same as caÂ org1.\ncsr:\n   cn: ca-org3\n   names:\n      - C: US\n        ST: \"New York\"\n        L: \"New York\"\n        O: ca-org3\n        OU: ca-org3\n   hosts:\n     - localhost\n     - example.com\n     - ca-org3\nca-org3 is the service name of fabric ca server in kubernetes.\nca:\n  # Name of this CA\n  name: Org3CA\nAfter setting Fabric CA Server configurations for Orderer,Org1,Org2 and Org3, we can deploy CA Servers to kubernetes.\nInstallation of Fabric CA Server on Kubernetes\n\nLetâ€™s create persistence volume and persistence volume claim to access fabric files from the pod,fabricfiles-pv.yaml and fabricfiles-pvc.yaml.\nThe fabricfiles-pv.yaml file is in the deploy/k8s/pv folder.\nThe fabricfiles-pvc.yaml file is in the deploy/k8s/pvc folder.\nhttps://medium.com/media/cedebf67a436edecb0cedf0dfc21e5d3/href\nnfs:\n  path: /srv/kubedata/fabricfiles\n  server: 192.168.12.9\n192.168.12.9 is the ip of the nfsÂ server.\n/srv/kubedata/fabricfiles is the directory where the fabric files are located on the nfsÂ server.\nlets create an Fabric Orderer CA server deployment and service, ca-orderer.yaml and ca-orderer-svc.yaml.The yaml files is in the deploy/k8s/ca folder.\nhttps://medium.com/media/64a53d8b9a94c102168ff620ddd6df47/href\nkind: Service\nmetadata:\n  name: ca-orderer\nca-orderer is the kubernetes service name of the Orderer Fabric CA server.It must be added in the csr host configuration in the Fabric CA Configuration file.\nports:\n- protocol: TCP\n  targetPort: 10054 \n  port: 10054 \ntargetPort is the container internal port.10054\nport is the service port.10054\n\"fabric-ca-server\",\n\"start\", \"-b\", \"admin:adminpw\", \"--port\", \"10054\", \"-d\"\nThe -b (bootstrap identity) option is required for initialization when LDAP is disabled. At least one bootstrap identity is required to start the Fabric CA server; this identity is the server administrator.\nadmin:adminpw \nserver admin information.Admin username is admin,Admin password isÂ adminpw.\n--port\", \"10054\"\n10054 is assigned as the ca server container port.\nenv:\n  - name: FABRIC_CA_SERVER_CA_NAME\n    value: ca-orderer\n  - name: FABRIC_CA_SERVER_TLS_ENABLED\n    value: \"true\"\nTls aws active and ca name ca-orderer is assigned.\nvolumeMounts:\n  - name: data\n    mountPath: /etc/hyperledger/fabric-ca-server\n    subPath: organizations/fabric-ca/ordererOrg\norganizations/fabric-ca/ordererOrg: path of ca server configuration file in fabricÂ files\nlets create an Fabric CA Org1 server deployment and service, ca-org1.yaml and ca-org1-svc.yaml.the yaml files are in the deploy/k8s/ca folder.\nhttps://medium.com/media/e0bae4d673e3b457a0647e232d5c98d0/href\nkind: Service\nmetadata:\n  name: ca-org1\nca-org1 is the kubernetes service name of the Org1 Fabric CA server.It must be added in the csr host configuration in the Fabric CA Configuration file.\nports:\n- protocol: TCP\n  targetPort: 7054 \n  port: 7054\ntargetPort is the container internal port.7054\nport is the service port.7054\n--port\", \"7054\"\n7054 is assigned as the ca server container port.\n- name: FABRIC_CA_SERVER_CA_NAME\n  value: ca-org1\n- name: FABRIC_CA_SERVER_TLS_ENABLED\n  value: \"true\"\n- name: FABRIC_CA_SERVER_CSR_CN\n  value: \"ca-org1\"\n- name: FABRIC_CA_SERVER_CSR_HOSTS\n  value: \"ca-org1\"\nTls was activated, ca name ca-org1 was assigned, and csr host and cn ca-org1 were assigned.\nvolumeMounts:\n  - name: data\n    mountPath: /etc/hyperledger/fabric-ca-server\n    subPath: organizations/fabric-ca/org1\norganizations/fabric-ca/org1: path of ca server configuration file in fabricÂ files\nlets create an Fabric CA Org2 server deployment and service, ca-org2.yaml and ca-org2-svc.yaml.the yaml files are in the deploy/k8s/ca folder.\nhttps://medium.com/media/fa8787671f4500c5f0ec8e5e957976bc/href\nkind: Service\nmetadata:\n  name: ca-org2\nca-org2 is the kubernetes service name of the Org2 Fabric CA server.It must be added in the csr host configuration in the Fabric CA Configuration file.\nports:\n- protocol: TCP\n  targetPort: 8054 \n  port: 8054 \ntargetPort is the container internal port.8054\nport is the service port.8054\n--port\", \"8054\"\n8054 is assigned as the ca server container port.\n- name: FABRIC_CA_SERVER_CA_NAME\n  value: ca-org2\n- name: FABRIC_CA_SERVER_TLS_ENABLED\n  value: \"true\"\n- name: FABRIC_CA_SERVER_CSR_CN\n  value: \"ca-org2\"\n- name: FABRIC_CA_SERVER_CSR_HOSTS\n  value: \"ca-org2\"\nTls was activated, ca name ca-org2 was assigned, and csr host and cn ca-org2 were assigned.\nvolumeMounts:\n  - name: data\n    mountPath: /etc/hyperledger/fabric-ca-server\n    subPath: organizations/fabric-ca/org2\norganizations/fabric-ca/org2: path of ca server configuration file in fabricÂ files\nlets create an Fabric CA Org3 server deployment and service, ca-org3.yaml and ca-org3-svc.yaml.the yaml files is in the deploy/k8s/ca folder.\nhttps://medium.com/media/c6194cb554cfb69b415ee745346d6c43/href\nkind: Service\nmetadata:\n  name: ca-org3\nca-org3 is the kubernetes service name of the Org3 Fabric CA server.It must be added in the csr host configuration in the Fabric CA Configuration file.\nports:\n- protocol: TCP\n  targetPort: 9054 \n  port: 9054\ntargetPort is the container internal port.9054\nport is the service port.9054\n--port\", \"9054\"\n9054 is assigned as the ca server container port.\n- name: FABRIC_CA_SERVER_CA_NAME\n  value: ca-org3\n- name: FABRIC_CA_SERVER_TLS_ENABLED\n  value: \"true\"\n- name: FABRIC_CA_SERVER_CSR_CN\n  value: \"ca-org3\"\n- name: FABRIC_CA_SERVER_CSR_HOSTS\n  value: \"ca-org3\"\nTls was activated, ca name ca-org3 was assigned, and csr host and cn ca-org3 were assigned.\nvolumeMounts:\n  - name: data\n    mountPath: /etc/hyperledger/fabric-ca-server\n    subPath: organizations/fabric-ca/org3\norganizations/fabric-ca/org3: path of ca server configuration file in fabricÂ files\nLetâ€™s define a job that creates the certificates for orderer,org1,org2 andÂ org3.\nLetâ€™s connect to the kubernetes master node virtual machine with the vagrant sshÂ command.\n$ vagrant ssh k8smaster\nLetâ€™s go to the directory where the kubernetes installation scripts are located.This directory is the same as the deploy/k8s folder in the project. With Vagrant, this directory is synchronized to the virtualÂ machine.\n$ cd /vagrant/k8s\nDeploying the persistence volume for fabricÂ files\n$ kubectl apply -f pv/fabricfiles-pv.yaml\n\nDeploying the persistence volume claim for fabricÂ files\n$ kubectl apply -f pvc/fabricfiles-pvc.yaml\n\nDeploying the fabric caÂ server\n$ kubectl apply -f ca/\n\nFabric ca server creation pending completion\n$ kubectl wait --for condition=available --timeout=300s deployment -l \"app in (ca-orderer,ca-org1,ca-org2,ca-org3)\"\n\nFinally, letâ€™s check whether our pods from the lens ide areÂ running.\n\nAll of fabric ca server pods areÂ running.\nMy article ends here. In general,I explained Fabric CA Server introduction and installation of Fabric CA Server on Kubernetes.\nSee you in the next articles.\nProject Links\nSpring Boot Hlf Starter Project details and installation can be accessed via thisÂ link.\nAsset Transfer Project details and installation can be accessed via thisÂ link",
      "dc:creator": "Åuayb ÅimÅŸek",
      "guid": "https://medium.com/p/7eb2fc75ba60",
      "categories": ["spring-boot", "hyperledger-fabric", "blockchain", "fabric-ca-server", "kubernetes"],
      "isoDate": "2021-11-21T14:33:12.000Z"
    },
    {
      "creator": "Åuayb ÅimÅŸek",
      "title": "Spring Boot Fullstack Blockchain Application With Hyperledger Fabric running on Kubernetes (Part 2)",
      "link": "https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-ddce8d25858d?source=rss-bda589f2335a------2",
      "pubDate": "Sun, 31 Oct 2021 18:04:46 GMT",
      "content:encoded": "<h3>Spring Boot Fullstack Blockchain Application With Hyperledger Fabric running on Kubernetes (Part 2)â€Šâ€”â€ŠKubernetes ClusterÂ Setup</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*9wP-lyvH_YZXzgJHWQRfCw.png\" /></figure><p>Hello everyone, through this article series we will the Hyperledger Fabric integration with Spring Boot.In this article, we will look into Kubernetes Cluster<em>, Haproxy,Nfs server </em>introductions and installations usingÂ Vagrant.</p><p>Other articles on Hyperledger Fabric integration with Spring Boot can be accessed from the linksÂ below.</p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-1-c8f7c87d60eb\">Part 1â€Šâ€”â€ŠIntroduction</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-ddce8d25858d\">Part 2â€Šâ€”â€ŠKubernetes ClusterÂ Setup</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-3-7eb2fc75ba60\">Part 3â€Šâ€”â€ŠFabric CAÂ Server</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-4-d6583a57153c\">Part 4â€Šâ€”â€ŠGenerating Certificates and Artifacts</a></p><p><a href=\"https://suaybsimsek58.medium.com/1723502770e7\">Part 5â€Šâ€”â€ŠKafka</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-6-b82662ada0a4\">Part 6â€Šâ€”â€ŠOrderer</a></p><h3>What is Kubernetes?</h3><p>Kubernetes is a production-grade open-source container orchestration tool developed by Google to help you manage the containerized/dockerized applications supporting multiple deployment environments like On-premise, cloud, or virtual machines.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/263/1*T9coezg1goi_0U8TbqZmVw.png\" /></figure><p>The name Kubernetes originates from Greek, meaning helmsman or pilot. K8s as an abbreviation results from counting the eight letters between the â€œKâ€ and the â€œsâ€. Google open-sourced the Kubernetes project inÂ 2014.</p><p>Project details of Kubernetes can be accessed via <a href=\"https://github.com/kubernetes/kubernetes\">thisÂ link</a></p><h3>What isÂ Vagrant?</h3><p>Vagrant is a tool that allows us to create and manage virtual machines developed by HashiCorp. With Vagrant, we can easily configure the virtual machine and manage many machinesÂ easily.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/256/1*yiX9jOguPIJUxd3_GQ6aCQ.png\" /></figure><p>Vagrant automates the infrastructure of your virtual machines by using a single file Vagrantfile.</p><p>Project details of Vagrant can be accessed via <a href=\"https://github.com/hashicorp/vagrant\">thisÂ link</a></p><h3>Vagrant Installation</h3><p>For vagrant installation, virtualbox or hyperv must be installed on your local computer.Weâ€™ll be using the VirtualBox provider, which is the default provider for Vagrant.The Vagranfile I created supports hyperv and virtualbox providers.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/630/1*ZjDtrZjJRQREjFrsn_VaJw.png\" /></figure><p>You can download Vagrant from <a href=\"https://www.vagrantup.com/\">https://www.vagrantup.com/</a> and VirtualBox from <a href=\"https://www.virtualbox.org/wiki/Downloads\">https://www.virtualbox.org/wiki/Downloads</a> and installÂ .</p><p>After installing you can confirm the <strong>vagrant version</strong> with installation.</p><h3>Installing the Guest Addition Plugin forÂ Vagrant</h3><p>Guest Addition is essentially for being able to unleash Vagrantâ€™s full potential, meaning it is important that it is installed and kept updated. However, ensuring your Vagrant boxes are always running the latest version of Guest Additions can be a time-consuming task, stealing away crucial cycles that could be put to betterÂ use.</p><p>We can install this plugin with the following command.</p><pre>vagrant plugin install vagrant-vbguest</pre><h3>Vagrantfile</h3><p>Vagrantfile is a file in which the necessary information (ram, cpu, vm_provider) is written for configuring and orchestrating virtual machines. Ruby language is used to make this configuration.</p><h3>Virtual Machine Installations UsingÂ Vagrant</h3><p>Letâ€™s open the project we downloaded from <a href=\"https://github.com/susimsek/spring-boot-hlf-k8s-fullstack-blockchain-app\">this link</a> and go to the directory where the Vagrantfile isÂ located.</p><pre>$ cd deploy</pre><p>We will set up 3 virtual machines for nfs server, ha proxy and kubernetes cluster usingÂ vagrant.</p><p>The hardware requirements and ips of the virtual machine are shown in the tableÂ below.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/5be99320a54d3a8150d624b011e732bc/href\">https://medium.com/media/5be99320a54d3a8150d624b011e732bc/href</a></iframe><p>Vagrant distribution of the Asset Transfer application, a minimum of 10 GB of RAM and 8 core CPU are required.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/d07a6992382747b7e404d5b6937f86fa/href\">https://medium.com/media/d07a6992382747b7e404d5b6937f86fa/href</a></iframe><p>The BOX_BASE variable in the Vagrantfile specifies the operating system of the vms. For Ubuntu Server 20.04 LTS, the value of this variable is ubuntu/focal64.</p><pre>BOX_BASE = â€œubuntu/focal64â€</pre><p>The NODES variable in the Vagrantfile sets the vms list and their paramaters(ram,cpu,ip,hostname). The type parameter can be haproxy, nfs, and k8s.Kubernetes cluster is currently setting up a single node.If the Kubernetes cluster is desired to consist of multiple master and worker nodes, this NODES variable can be assigned asÂ follows.</p><p>For 3 Master,3 Worker NodeÂ Setup</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/87bf16dd4764acc71ea16c89348847ef/href\">https://medium.com/media/87bf16dd4764acc71ea16c89348847ef/href</a></iframe><p>Vagrantfile supports multiple master and worker mode installation. It also supports virtualbox and hyperv as virtual machine provider.</p><p>2. Now lets create virtual machines.</p><pre>$ vagrant up</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*1TALnl6zmDvf3wylDNu62Q.png\" /></figure><p>After the Vagrant installation is completed, that is, the virtual machines are created and the configurations we have specified in the Vagrantfile are made, we can connect to the virtual machine we have installed with the <strong>vagrant ssh</strong>Â command.</p><h3>Dynamic NFS Provisioning in Kubernetes</h3><p>Kubernetes containerized storage allows people to develop <strong>data containers on-demand</strong>. The confidentiality of the data contained is maintained by providing automatic provisioning.</p><p>The storage method is at sky-high demand sprawling out far and wide among various verticals. Numerous applications are coming up for providing contented access toÂ people.</p><p>One of the ways Kubernetes allow applications to access storage is the standard Network File Service (NFS) protocol.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1000/0*tddU2bUZzEVAQf0t.png\" /></figure><h3>NFS Server Installation UsingÂ Vagrant</h3><p>Letâ€™s connect to the nfs server virtual machine with the <strong>vagrant ssh</strong>Â command.</p><pre>$ vagrant ssh nfsserver</pre><p>Letâ€™s go to the directory where the nfs server installation scripts are located.This directory is the same as the deploy/setup/nfs-server-setup folder in the project. With Vagrant, this directory is synchronized to the virtualÂ machine.</p><pre>$ <em>cd</em> /vagrant/setup/nfs-server-setup</pre><p>Letâ€™s give the permission toÂ execute.</p><pre>$ <em>sudo</em> chmod u+x *.sh</pre><p>Letâ€™s install NfsÂ Server.</p><pre>$ <em>sudo</em> ./install_nfs.sh</pre><p>After the nfs server installation is complete, letâ€™s run the script that creates the necessary directories for fabricÂ files.</p><pre>$ <em>./create_fabric_dir.sh</em></pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*1_Wqr52F0NtVVkFPIgG3qQ.png\" /></figure><p>After the directories are created, letâ€™s copy the fabric files to these directories.</p><pre>$ <em>./copy_fabricfiles.sh</em></pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Z6y6omk328DFub1EZjLNhQ.png\" /></figure><p>The nfs server path is set to /srv/kubedata by default. If you want to change this path, you need to change the NFS_PATH variable in install_nfs.sh, NFS_DIR variable in copy_fabricfiles.sh and NFS_DIR variable in create_fabric_dir.sh.In the project, these scripts are in the deploy/setup/nfs-server-setup folder.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/b2931a092a01ca2fe6a0fd0e9e698a83/href\">https://medium.com/media/b2931a092a01ca2fe6a0fd0e9e698a83/href</a></iframe><p>Fabric dosyalarÄ± srv/kubedata/fabricfiles patihne kopyalanmÄ±ÅŸtÄ±r.AÅŸaÄŸÄ± komut ile kontrolÂ edelim.</p><pre>$ ls /srv/kubedata/fabricfiles/</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*mBqNBVkf6oXRr-owi9tuOw.png\" /></figure><p>Nfs server installation completed successfully. Letâ€™s exit the terminal of the virtualÂ machine.</p><pre>$ exit</pre><h3>External Load Balancer in Kubernetes</h3><p>HAProxy is a free, very fast and reliable solution offering high availability, load balancing, and proxying for TCP and HTTP-based applications. It is particularly suited for very high traffic web sites and powers quite a number of the worldâ€™s most visited ones. Over the years it has become the de-facto standard opensource load balancer, is now shipped with most mainstream Linux distributions.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/350/1*aRJP7GH---H3jf957JxzIg.png\" /></figure><p>We will use HAProxy as a Kubernetes external loadbalancer. Requests to haproxy from port 80 will be balanced to the nginx ingress controller on the worker nodes.In addition, requests to haproxy from port 6443 will be balanced between kubernetes masterÂ nodes.</p><h3>HAProxy Installation UsingÂ Vagrant</h3><p>Letâ€™s connect to the haproxy virtual machine with the <strong>vagrant ssh</strong>Â command.</p><pre>$ vagrant ssh haproxy</pre><p>Letâ€™s go to the directory where the haproxy installation scripts are located.This directory is the same as the deploy/setup/haproxy-setup folder in the project. With Vagrant, this directory is synchronized to the virtualÂ machine.</p><pre>$ <em>cd</em> /vagrant/setup/haproxy-setup</pre><p>Letâ€™s give the permission toÂ execute.</p><pre>$ <em>sudo</em> chmod u+x *.sh</pre><p>Letâ€™s installÂ haproxy</p><pre>$ <em>sudo</em> ./install_haproxy.sh</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*zRODx-jdZd-yOyYEWr4Rvg.png\" /></figure><p>Haproxy installation completed successfully. Letâ€™s exit the terminal of the virtualÂ machine.</p><pre>$ exit</pre><h3>Kubernetes Installation UsingÂ Vagrant</h3><p>We will install a bare metal kubernetes cluster using Kubespray.Kubespray is a composition of <a href=\"https://docs.ansible.com/\">Ansible</a> playbooks, <a href=\"https://github.com/kubernetes-sigs/kubespray/blob/master/docs/ansible.md\">inventory</a>, provisioning tools, and domain knowledge for generic OS/Kubernetes clusters configuration management tasks.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/551/1*07j4DZQ_8Y8DcQHUDOBGdQ.png\" /></figure><p>Letâ€™s connect to the kubernetes master node virtual machine with the <strong>vagrant ssh</strong>Â command.</p><pre>$ vagrant ssh k8smaster</pre><p>Letâ€™s go to the directory where the kubernetes installation scripts are located.This directory is the same as the deploy/setup/kubernetes-setup folder in the project. With Vagrant, this directory is synchronized to the virtualÂ machine.</p><pre>$ <em>cd</em> /vagrant/setup/kubernetes-setup</pre><p>Letâ€™s give the permission toÂ execute.</p><pre>$ <em>sudo</em> chmod u+x *.sh</pre><p>By default, kubernetes is installed with metallb, metric server,helm,weavenet,nfs provisioner,cert manager option.If you want to change this option, you need to change the variables(METALLB_ENABLED,METRIC_SERVER_ENABLED,etc) in install_kubespray.sh and the variables(CERT_MANAGER_ENABLED,ISTIO_ENABLED,etc) in install-prereqs.sh.In the project, these scripts are in the deploy/setup/kubernetes-setup folder.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/93c595019998c4845f73568279d40418/href\">https://medium.com/media/93c595019998c4845f73568279d40418/href</a></iframe><h3>What isÂ MetalLB?</h3><p>MetalLB is a load-balancer implementation for bare metal <a href=\"https://kubernetes.io/\">Kubernetes</a> clusters, using standard routing protocols.It is installed byÂ default.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/165/1*uGHr9r2Lk_rnvQQyuL9B4Q.png\" /></figure><h3>What isÂ Helm?</h3><p>Helm is a tool for managing Charts. Charts are packages of pre-configured Kubernetes resources.It is installed byÂ default.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/165/1*Nkod4KY-P42hQ4QJAMOwSg.png\" /></figure><h3>What is Nginx Ingress Controller?</h3><p>Ingress Nginx is an Ingress controller for Kubernetes using <a href=\"https://www.nginx.org/\">NGINX</a> as a reverse proxy and load balancer.It is installed byÂ default.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/165/1*FwKurDFszP01i85PmvHinQ.png\" /></figure><h3>What isÂ Istio?</h3><p>Istio, the most popular service mesh implementation, was developed on top of Kubernetes and has a different niche in the cloud native application ecosystem than Kubernetes.It is not installed by default.If you want to change this option, you need to change the ISTIO_ENABLED variable in install-prereqs.sh.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/165/1*6A9IYK5xuvkLszlNejR4nw.png\" /></figure><p>Letâ€™s install kubernetes</p><pre>$ <em>./install-prereqs.sh</em></pre><p>Kubernetes installation completed successfully. Finally, letâ€™s add the kubernetes cluster to LensÂ IDE.</p><h3>What isÂ Lens?</h3><p>Lensâ€Šâ€”â€ŠThe Kubernetes IDE (â€œLens IDEâ€) is a distribution of the OpenLens repository with Team Lens specific customizations released under a traditional <a href=\"https://k8slens.dev/licenses/eula\">EULA</a>.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/225/1*JAGCKJ7DtT6Jp1HwfdNqFA.png\" /></figure><p>Lens IDE provides the full situational awareness for everything that runs in Kubernetes. Itâ€™s lowering the barrier of entry for people just getting started and radically improving productivity for people with more experience.</p><p>Lens IDE a standalone application for MacOS, Windows and Linux operating systems. You can download it free of charge for Windows, MacOS, and Linux from <a href=\"https://k8slens.dev/\">Lens IDEÂ website</a>.</p><h3>Add Kubernetes Cluster to LensÂ IDE</h3><p>Letâ€™s copy the contents of the kubectl configuration file.The default kubectl configuration file is located at <strong>~/.</strong> <strong>kube/config</strong> and is referred to as the kubeconfig file.</p><pre>$ cat ${HOME}/.kube/config</pre><p>Letâ€™s open the Lens IDE, letâ€™s go to Clusters from the catalog. Letâ€™s choose Add from kubeconfig.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*p9fi6IoBQ7Ejkst7dT6l6A.png\" /></figure><p>Letâ€™s paste the contents of the kubectl configuration file into the window that opens.Letâ€™s replace https://elb.kub:6443 with <a href=\"https://192.168.12.10:6443.\">https://192.168.12.10:6443.</a> Finally, click AddÂ Cluster.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*66Z_BzpNP9xSmcU79O89ww.png\" /></figure><p>Kubernetes Cluster has been added successfully.letâ€™s go to Clusters from the catalog. Letâ€™s select kubernetes-admin@cluster.local and clickÂ connect.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*KOCWzb4mQgxYwIxU-hLIAg.png\" /></figure><p>Kubernetes successfully connected to the cluster. When we select Storage Class under Storage, there is a storage class called nfs-client.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*NDajHMjten2vXHPtmFZdHg.png\" /></figure><p>My article ends here. In general,I explained introduction and installation of kubernetes cluster and kubernetes tools, nfs server and haproxy usingÂ vagrant.</p><p>See you in the next articles.</p><h3>Project Links</h3><p>Spring Boot Hlf Starter Project details and installation can be accessed via <a href=\"https://github.com/susimsek/spring-boot-hlf-starter\">thisÂ link</a>.</p><p>Asset Transfer Project details and installation can be accessed via <a href=\"https://github.com/susimsek/spring-boot-hlf-starter\">thisÂ link</a></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=ddce8d25858d\" width=\"1\" height=\"1\" alt=\"\">",
      "content:encodedSnippet": "Spring Boot Fullstack Blockchain Application With Hyperledger Fabric running on Kubernetes (Part 2)â€Šâ€”â€ŠKubernetes ClusterÂ Setup\n\nHello everyone, through this article series we will the Hyperledger Fabric integration with Spring Boot.In this article, we will look into Kubernetes Cluster, Haproxy,Nfs server introductions and installations usingÂ Vagrant.\nOther articles on Hyperledger Fabric integration with Spring Boot can be accessed from the linksÂ below.\nPart 1â€Šâ€”â€ŠIntroduction\nPart 2â€Šâ€”â€ŠKubernetes ClusterÂ Setup\nPart 3â€Šâ€”â€ŠFabric CAÂ Server\nPart 4â€Šâ€”â€ŠGenerating Certificates and Artifacts\nPart 5â€Šâ€”â€ŠKafka\nPart 6â€Šâ€”â€ŠOrderer\nWhat is Kubernetes?\nKubernetes is a production-grade open-source container orchestration tool developed by Google to help you manage the containerized/dockerized applications supporting multiple deployment environments like On-premise, cloud, or virtual machines.\n\nThe name Kubernetes originates from Greek, meaning helmsman or pilot. K8s as an abbreviation results from counting the eight letters between the â€œKâ€ and the â€œsâ€. Google open-sourced the Kubernetes project inÂ 2014.\nProject details of Kubernetes can be accessed via thisÂ link\nWhat isÂ Vagrant?\nVagrant is a tool that allows us to create and manage virtual machines developed by HashiCorp. With Vagrant, we can easily configure the virtual machine and manage many machinesÂ easily.\n\nVagrant automates the infrastructure of your virtual machines by using a single file Vagrantfile.\nProject details of Vagrant can be accessed via thisÂ link\nVagrant Installation\nFor vagrant installation, virtualbox or hyperv must be installed on your local computer.Weâ€™ll be using the VirtualBox provider, which is the default provider for Vagrant.The Vagranfile I created supports hyperv and virtualbox providers.\n\nYou can download Vagrant from https://www.vagrantup.com/ and VirtualBox from https://www.virtualbox.org/wiki/Downloads and installÂ .\nAfter installing you can confirm the vagrant version with installation.\nInstalling the Guest Addition Plugin forÂ Vagrant\nGuest Addition is essentially for being able to unleash Vagrantâ€™s full potential, meaning it is important that it is installed and kept updated. However, ensuring your Vagrant boxes are always running the latest version of Guest Additions can be a time-consuming task, stealing away crucial cycles that could be put to betterÂ use.\nWe can install this plugin with the following command.\nvagrant plugin install vagrant-vbguest\nVagrantfile\nVagrantfile is a file in which the necessary information (ram, cpu, vm_provider) is written for configuring and orchestrating virtual machines. Ruby language is used to make this configuration.\nVirtual Machine Installations UsingÂ Vagrant\nLetâ€™s open the project we downloaded from this link and go to the directory where the Vagrantfile isÂ located.\n$ cd deploy\nWe will set up 3 virtual machines for nfs server, ha proxy and kubernetes cluster usingÂ vagrant.\nThe hardware requirements and ips of the virtual machine are shown in the tableÂ below.\nhttps://medium.com/media/5be99320a54d3a8150d624b011e732bc/href\nVagrant distribution of the Asset Transfer application, a minimum of 10 GB of RAM and 8 core CPU are required.\nhttps://medium.com/media/d07a6992382747b7e404d5b6937f86fa/href\nThe BOX_BASE variable in the Vagrantfile specifies the operating system of the vms. For Ubuntu Server 20.04 LTS, the value of this variable is ubuntu/focal64.\nBOX_BASE = â€œubuntu/focal64â€\nThe NODES variable in the Vagrantfile sets the vms list and their paramaters(ram,cpu,ip,hostname). The type parameter can be haproxy, nfs, and k8s.Kubernetes cluster is currently setting up a single node.If the Kubernetes cluster is desired to consist of multiple master and worker nodes, this NODES variable can be assigned asÂ follows.\nFor 3 Master,3 Worker NodeÂ Setup\nhttps://medium.com/media/87bf16dd4764acc71ea16c89348847ef/href\nVagrantfile supports multiple master and worker mode installation. It also supports virtualbox and hyperv as virtual machine provider.\n2. Now lets create virtual machines.\n$ vagrant up\n\nAfter the Vagrant installation is completed, that is, the virtual machines are created and the configurations we have specified in the Vagrantfile are made, we can connect to the virtual machine we have installed with the vagrant sshÂ command.\nDynamic NFS Provisioning in Kubernetes\nKubernetes containerized storage allows people to develop data containers on-demand. The confidentiality of the data contained is maintained by providing automatic provisioning.\nThe storage method is at sky-high demand sprawling out far and wide among various verticals. Numerous applications are coming up for providing contented access toÂ people.\nOne of the ways Kubernetes allow applications to access storage is the standard Network File Service (NFS) protocol.\n\nNFS Server Installation UsingÂ Vagrant\nLetâ€™s connect to the nfs server virtual machine with the vagrant sshÂ command.\n$ vagrant ssh nfsserver\nLetâ€™s go to the directory where the nfs server installation scripts are located.This directory is the same as the deploy/setup/nfs-server-setup folder in the project. With Vagrant, this directory is synchronized to the virtualÂ machine.\n$ cd /vagrant/setup/nfs-server-setup\nLetâ€™s give the permission toÂ execute.\n$ sudo chmod u+x *.sh\nLetâ€™s install NfsÂ Server.\n$ sudo ./install_nfs.sh\nAfter the nfs server installation is complete, letâ€™s run the script that creates the necessary directories for fabricÂ files.\n$ ./create_fabric_dir.sh\n\nAfter the directories are created, letâ€™s copy the fabric files to these directories.\n$ ./copy_fabricfiles.sh\n\nThe nfs server path is set to /srv/kubedata by default. If you want to change this path, you need to change the NFS_PATH variable in install_nfs.sh, NFS_DIR variable in copy_fabricfiles.sh and NFS_DIR variable in create_fabric_dir.sh.In the project, these scripts are in the deploy/setup/nfs-server-setup folder.\nhttps://medium.com/media/b2931a092a01ca2fe6a0fd0e9e698a83/href\nFabric dosyalarÄ± srv/kubedata/fabricfiles patihne kopyalanmÄ±ÅŸtÄ±r.AÅŸaÄŸÄ± komut ile kontrolÂ edelim.\n$ ls /srv/kubedata/fabricfiles/\n\nNfs server installation completed successfully. Letâ€™s exit the terminal of the virtualÂ machine.\n$ exit\nExternal Load Balancer in Kubernetes\nHAProxy is a free, very fast and reliable solution offering high availability, load balancing, and proxying for TCP and HTTP-based applications. It is particularly suited for very high traffic web sites and powers quite a number of the worldâ€™s most visited ones. Over the years it has become the de-facto standard opensource load balancer, is now shipped with most mainstream Linux distributions.\n\nWe will use HAProxy as a Kubernetes external loadbalancer. Requests to haproxy from port 80 will be balanced to the nginx ingress controller on the worker nodes.In addition, requests to haproxy from port 6443 will be balanced between kubernetes masterÂ nodes.\nHAProxy Installation UsingÂ Vagrant\nLetâ€™s connect to the haproxy virtual machine with the vagrant sshÂ command.\n$ vagrant ssh haproxy\nLetâ€™s go to the directory where the haproxy installation scripts are located.This directory is the same as the deploy/setup/haproxy-setup folder in the project. With Vagrant, this directory is synchronized to the virtualÂ machine.\n$ cd /vagrant/setup/haproxy-setup\nLetâ€™s give the permission toÂ execute.\n$ sudo chmod u+x *.sh\nLetâ€™s installÂ haproxy\n$ sudo ./install_haproxy.sh\n\nHaproxy installation completed successfully. Letâ€™s exit the terminal of the virtualÂ machine.\n$ exit\nKubernetes Installation UsingÂ Vagrant\nWe will install a bare metal kubernetes cluster using Kubespray.Kubespray is a composition of Ansible playbooks, inventory, provisioning tools, and domain knowledge for generic OS/Kubernetes clusters configuration management tasks.\n\nLetâ€™s connect to the kubernetes master node virtual machine with the vagrant sshÂ command.\n$ vagrant ssh k8smaster\nLetâ€™s go to the directory where the kubernetes installation scripts are located.This directory is the same as the deploy/setup/kubernetes-setup folder in the project. With Vagrant, this directory is synchronized to the virtualÂ machine.\n$ cd /vagrant/setup/kubernetes-setup\nLetâ€™s give the permission toÂ execute.\n$ sudo chmod u+x *.sh\nBy default, kubernetes is installed with metallb, metric server,helm,weavenet,nfs provisioner,cert manager option.If you want to change this option, you need to change the variables(METALLB_ENABLED,METRIC_SERVER_ENABLED,etc) in install_kubespray.sh and the variables(CERT_MANAGER_ENABLED,ISTIO_ENABLED,etc) in install-prereqs.sh.In the project, these scripts are in the deploy/setup/kubernetes-setup folder.\nhttps://medium.com/media/93c595019998c4845f73568279d40418/href\nWhat isÂ MetalLB?\nMetalLB is a load-balancer implementation for bare metal Kubernetes clusters, using standard routing protocols.It is installed byÂ default.\n\nWhat isÂ Helm?\nHelm is a tool for managing Charts. Charts are packages of pre-configured Kubernetes resources.It is installed byÂ default.\n\nWhat is Nginx Ingress Controller?\nIngress Nginx is an Ingress controller for Kubernetes using NGINX as a reverse proxy and load balancer.It is installed byÂ default.\n\nWhat isÂ Istio?\nIstio, the most popular service mesh implementation, was developed on top of Kubernetes and has a different niche in the cloud native application ecosystem than Kubernetes.It is not installed by default.If you want to change this option, you need to change the ISTIO_ENABLED variable in install-prereqs.sh.\n\nLetâ€™s install kubernetes\n$ ./install-prereqs.sh\nKubernetes installation completed successfully. Finally, letâ€™s add the kubernetes cluster to LensÂ IDE.\nWhat isÂ Lens?\nLensâ€Šâ€”â€ŠThe Kubernetes IDE (â€œLens IDEâ€) is a distribution of the OpenLens repository with Team Lens specific customizations released under a traditional EULA.\n\nLens IDE provides the full situational awareness for everything that runs in Kubernetes. Itâ€™s lowering the barrier of entry for people just getting started and radically improving productivity for people with more experience.\nLens IDE a standalone application for MacOS, Windows and Linux operating systems. You can download it free of charge for Windows, MacOS, and Linux from Lens IDEÂ website.\nAdd Kubernetes Cluster to LensÂ IDE\nLetâ€™s copy the contents of the kubectl configuration file.The default kubectl configuration file is located at ~/. kube/config and is referred to as the kubeconfig file.\n$ cat ${HOME}/.kube/config\nLetâ€™s open the Lens IDE, letâ€™s go to Clusters from the catalog. Letâ€™s choose Add from kubeconfig.\n\nLetâ€™s paste the contents of the kubectl configuration file into the window that opens.Letâ€™s replace https://elb.kub:6443 with https://192.168.12.10:6443. Finally, click AddÂ Cluster.\n\nKubernetes Cluster has been added successfully.letâ€™s go to Clusters from the catalog. Letâ€™s select kubernetes-admin@cluster.local and clickÂ connect.\n\nKubernetes successfully connected to the cluster. When we select Storage Class under Storage, there is a storage class called nfs-client.\n\nMy article ends here. In general,I explained introduction and installation of kubernetes cluster and kubernetes tools, nfs server and haproxy usingÂ vagrant.\nSee you in the next articles.\nProject Links\nSpring Boot Hlf Starter Project details and installation can be accessed via thisÂ link.\nAsset Transfer Project details and installation can be accessed via thisÂ link",
      "dc:creator": "Åuayb ÅimÅŸek",
      "guid": "https://medium.com/p/ddce8d25858d",
      "categories": ["blockchain", "kubernetes", "spring-boot", "vagrant", "hyperledger-fabric"],
      "isoDate": "2021-10-31T18:04:46.000Z"
    },
    {
      "creator": "Åuayb ÅimÅŸek",
      "title": "Spring Boot Fullstack Blockchain Application With Hyperledger Fabric running on Kubernetes (Part 1)",
      "link": "https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-1-c8f7c87d60eb?source=rss-bda589f2335a------2",
      "pubDate": "Sun, 17 Oct 2021 17:59:32 GMT",
      "content:encoded": "<h3>Spring Boot Fullstack Blockchain Application With Hyperledger Fabric running on Kubernetes (Part 1)â€Šâ€”â€ŠIntroduction</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*R9q34HnrsGeQiExwr54NYA.png\" /></figure><p>Hello everyone, through this article series we will the Hyperledger Fabric integration with Spring Boot.In this article, we will look into <em>Spring Boot Hlf Starter i</em>ntroduction<em>Â , Asset Transfer Application i</em>ntroduction<em>,Blockchain and Hyperledger Fabric concept</em>.In other articles we will implement Asset Transfer Application step by step and deploy this app on Kubernetes.In the next articles,I will also explain Hyperledger explorer, grafana integration on Kubernetes.</p><p>Other articles on Hyperledger Fabric integration with Spring Boot can be accessed from the linksÂ below.</p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-1-c8f7c87d60eb\">Part 1â€Šâ€”â€ŠIntroduction</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-ddce8d25858d\">Part 2â€Šâ€”â€ŠKubernetes ClusterÂ Setup</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-3-7eb2fc75ba60\">Part 3â€Šâ€”â€ŠFabric CAÂ Server</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-4-d6583a57153c\">Part 4â€Šâ€”â€ŠGenerating Certificates and Artifacts</a></p><p><a href=\"https://suaybsimsek58.medium.com/1723502770e7\">Part 5â€Šâ€”â€ŠKafka</a></p><p><a href=\"https://suaybsimsek58.medium.com/spring-boot-fullstack-blockchain-application-with-hyperledger-fabric-running-on-kubernetes-part-6-b82662ada0a4\">Part 6â€Šâ€”â€ŠOrderer</a></p><h3>What is Asset TransferÂ App?</h3><p>Asset Transfer Application is a blockchain based fullstack application that allows you to create and transfer an asset by putting data on the ledger and retrieving it.The chaincode part of the application is written in Go, the backend part is written in Spring boot, and the frontend part is written inÂ Angular.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*XI1aWY2GwvhnYSXyb2Oipg.png\" /></figure><p>Project details and installation of Asset Transfer Application can be accessed via<a href=\"https://github.com/susimsek/spring-boot-hlf-k8s-fullstack-blockchain-app\"> thisÂ link</a></p><h3>What is Spring Boot HlfÂ Starter?</h3><p>Spring Boot Hlf Starter library is a spring boot starter library I wrote.This library provides an easy way to get your Spring boot application using Hyperledger Fabric Gateway SDK v2.2 up and runningÂ quickly.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1000/0*E_7xA9se2QPU_EgU.png\" /></figure><p>Spring Native provides support for compiling Spring applications to native executables using the <a href=\"https://www.graalvm.org/\">GraalVM</a> <a href=\"https://www.graalvm.org/reference-manual/native-image/\">native-image</a> compiler.Iâ€™m thinking of adding spring native support in the next versions of theÂ project.</p><p>Project details and installation can be accessed via <a href=\"https://github.com/susimsek/spring-boot-hlf-starter\">thisÂ link</a></p><h3>What is Blockchain?</h3><p>It was made with the work done by Stuart Haber and Scott Stornetta, who were in the cryptographic development phase of blockchain technology in the 1990s. Starting from the concept of the â€œhash treeâ€, which was classified in a way in the 1970s, two experts have managed to approach its definition with the modern blockchain, in full terms. Not Satoshi Nakamoto, but Haber and Stornetta.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Zmpq5aDS3HceMvlgxJep7Q.png\" /></figure><p>The first Blockchain in its current sense is as old as Bitcoin, the first cryptocurrency in history. When the calendars showed October 31, 2008, the white paper document called â€œBitcoin: Peer-to-Peer Electronic Cash Systemâ€ published by a person or group named Satoshi Nakamoto stated that the BTC infrastructure was completely based on blockchain technology.</p><h3>How Blockchain Works?</h3><p>Blockchain, which has a very wide working area, is used. Blockchain can be made from authorized and unofficial use cases, from usage areas, from contracts to notary transactions.</p><p>The most important features underlying the working principle of blockchain technology; it is anonymous, distributed, decentralized and public, yet unbreakable and unhackable. The fact that an information record chain is both accessible and unbreakable to everyone causes everyone, especially computer scientists, to approach this technology with admiration. There is a simple logic behind the fact that the records committed to the blocks are unbreakable and unchangeable: All blocks in this registry, which has billions of copies, must be changed in order to corrupt the blocks. It is almost impossible to make such an intervention.</p><h3>What is Ethereum?</h3><p>Ethereum is an open source distributed public blockchain network. It allows decentralized apps to be built on it with the help of Smart Contract functionality.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/700/0*t_TrUui-wwvBOrlx.png\" /></figure><p>Vitalik Buterin developed Ethereum as an extension to the original core blockchain concept. He improvised Bitcoinâ€™s protocols to support applications beyond currency issuance. Its major breakthrough is the ability to easily write and deploy Smart Contracts. These are actually bits of code that are executed on the network. Hence, this platform could help developers to write programs for building decentralized organisations.</p><h3>What is Hyperledger Fabric?</h3><p>Hyperledger Fabric is a framework for developing Blockchain-based solutions for the enterprise. It is open-source and under the umbrella of the Linux Foundation.It is a private chain, thus super-helpful for enterprises since you donâ€™t want to put your transactions for public display.The framework has a very sophisticated module architecture which allows thedeveloper to design the blockchain network with security, scalability, confidentiality and with high performance.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*z2REkO4RIp73p4wESu_0IA.png\" /></figure><h3>Hyperledger Fabric vsÂ Ethereum</h3><p>The subject of this article is not etherium. so I wonâ€™t explain ethereum in detail. The differences between Ethereum and Hyperledger Fabric are explained in the tableÂ below.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/938/1*1MxxMQNC8E8LMTRA6URwKw.png\" /></figure><h3>What isÂ DLT?</h3><p>Any Blockchain works on top of a DLT, or Distributed Ledger Technology. DLTs are protocols to store all transactions that take place in a network. In a Blockchain protocol, the network is <strong>decentralized</strong><em>, </em>which means that the transaction history is replicated across all the participating nodes.</p><h3>The Architecture ofÂ Fabric</h3><p>Fabric is a permissioned Blockchain, it has certain protocols you might not have heard of before, itâ€™s time to take a look atÂ them.</p><h3>Ledger</h3><p>In Hyperledger fabric ledger is a transaction history log that contains the state of each transaction committed to the ledger. The ledger consists of two parts World State, Blockchain.</p><p><strong>World State</strong>â€Šâ€”â€ŠItâ€™s a database that contains the updated records for a transaction. The World state gets more comfortable for the programmer to check the record is stored accurately into the ledger or not.Hyperledger fabric supports LevelDB and CouchDB as the state database.I used CouchDB in Asset Transfer Application.</p><p><strong>Blockchain</strong>â€Šâ€”â€Šis a set of sequential blocks containing the sequence of transactions carried out in the world state. Hyperledger Fabric uses an ordering service for transaction sequencing within theÂ blocks.</p><h3>Transaction</h3><p>The role of a transaction is to store the state of anÂ object.</p><h3>Blocks</h3><p>Blocks In Blockchain the first block is called the genesis block. This initial block doesnâ€™t contain any user transaction but stores the channel configuration in theÂ network.</p><h3>Peers</h3><p>Peers are a fundamental unit of any distributed model. Peers are essentially nodes, but in public blockchain networks like Bitcoin or Ethereum, all peers are equal. In a private Blockchain, all peers are <strong>notÂ </strong>equal.</p><p>In Asset transfer application, each organization has only 1Â peer.</p><h3>The Consensus Mechanism</h3><p>The point of Consensus is validating the correctness of a Block, and that it adheres to the policies set by the Chaincode. There are 2 types of Consensus Mechanisms in Fabric: <strong>Voting </strong>and <strong>Lottery</strong>. Voting is the more accepted method in enterprise right now. You, as a developer, must develop your Fabric-based enterprise application on the idea that there is partial trust in a network. I hope Voting and Lottery are clear from their nomenclature as to what they do, if not, refer to the officialÂ docs.</p><p>Consensus in Fabric is broken down in threeÂ phases:</p><ol><li><strong>Endorsement</strong>: The participants will have to endorse a transaction.</li><li><strong>Ordering</strong>: This phase will agree to the order of commitment to theÂ ledger.</li><li><strong>Validation</strong>: Validates structure andÂ order.</li></ol><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*qRymZnyZF7HqFeqd.jpeg\" /></figure><h3>Chaincode</h3><p>Chaincode is the software that defines the asset. Assets are basically a key-value pair, where Asset definitions enable the exchange of almost anything with a monetary value over the network. Chaincode is a program that runs in a secured Docker container. Chaincode is the Smart Contract of Fabric in the sense it runs the business logic of the network. You can write ChainCodes in Go, NodeJS, or Java.The chaincode part of Asset transfer application is written inÂ Go.</p><p>Asset transfer application has a single chaincode calledÂ basic.</p><h3>Organization</h3><p>This is a set of groups of members under a single MSP. An organization can be related to a big multi corporation group or small coffee shop. The naming convention for organization MSP can be derived as Org1.MSP, here organization is â€œOrg1â€. And also an organization can have many MSPs based on its requirements.</p><p>Asset transfer application has 3 organizations.Org1,Org2,Org3.</p><h3>Membership Service ProviderÂ (MSP)</h3><p>For clients to participate in a private network, they need credentials to be authentic. MSPs are an abstract component that provides credentials to the clients. Clients use these credentials to authenticate their transactions, and peers use these credentials to authenticate transaction processing results (endorsements).</p><h3>Channels</h3><p>Hyperledger Fabric has a technique that allows the organization to join multiple blockchain networks through a channel. So, multiple organizations can communicate or participate in many networks implementing various channels.Asset transfer application has a single channel called mychannel.</p><h3>What is Hyperledger Explorer?</h3><p>Hyperledger Explorer is a simple, powerful, easy-to-use, well maintained, open source utility to browse activity on the underlying blockchain network. Users have the ability to configure and build Hyperledger Explorer on MacOS andÂ Ubuntu.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*xtnKvzLqttYwzadI.png\" /></figure><p>In the next articles, I will explain the integration of Hyperledger Fabric with Hyperledger Explorer.</p><p>My article ends here. In general, I tried to give theoretical information about Hyperledger Fabric,Hyperledger Explorer and Blockchain.</p><p>See you in the next articles.</p><h3>Project Links</h3><p>Spring Boot Hlf Starter Project details and installation can be accessed via <a href=\"https://github.com/susimsek/spring-boot-hlf-starter\">thisÂ link</a>.</p><p>Asset Transfer Project details and installation can be accessed via <a href=\"https://github.com/susimsek/spring-boot-hlf-starter\">thisÂ link</a></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=c8f7c87d60eb\" width=\"1\" height=\"1\" alt=\"\">",
      "content:encodedSnippet": "Spring Boot Fullstack Blockchain Application With Hyperledger Fabric running on Kubernetes (Part 1)â€Šâ€”â€ŠIntroduction\n\nHello everyone, through this article series we will the Hyperledger Fabric integration with Spring Boot.In this article, we will look into Spring Boot Hlf Starter introductionÂ , Asset Transfer Application introduction,Blockchain and Hyperledger Fabric concept.In other articles we will implement Asset Transfer Application step by step and deploy this app on Kubernetes.In the next articles,I will also explain Hyperledger explorer, grafana integration on Kubernetes.\nOther articles on Hyperledger Fabric integration with Spring Boot can be accessed from the linksÂ below.\nPart 1â€Šâ€”â€ŠIntroduction\nPart 2â€Šâ€”â€ŠKubernetes ClusterÂ Setup\nPart 3â€Šâ€”â€ŠFabric CAÂ Server\nPart 4â€Šâ€”â€ŠGenerating Certificates and Artifacts\nPart 5â€Šâ€”â€ŠKafka\nPart 6â€Šâ€”â€ŠOrderer\nWhat is Asset TransferÂ App?\nAsset Transfer Application is a blockchain based fullstack application that allows you to create and transfer an asset by putting data on the ledger and retrieving it.The chaincode part of the application is written in Go, the backend part is written in Spring boot, and the frontend part is written inÂ Angular.\n\nProject details and installation of Asset Transfer Application can be accessed via thisÂ link\nWhat is Spring Boot HlfÂ Starter?\nSpring Boot Hlf Starter library is a spring boot starter library I wrote.This library provides an easy way to get your Spring boot application using Hyperledger Fabric Gateway SDK v2.2 up and runningÂ quickly.\n\nSpring Native provides support for compiling Spring applications to native executables using the GraalVM native-image compiler.Iâ€™m thinking of adding spring native support in the next versions of theÂ project.\nProject details and installation can be accessed via thisÂ link\nWhat is Blockchain?\nIt was made with the work done by Stuart Haber and Scott Stornetta, who were in the cryptographic development phase of blockchain technology in the 1990s. Starting from the concept of the â€œhash treeâ€, which was classified in a way in the 1970s, two experts have managed to approach its definition with the modern blockchain, in full terms. Not Satoshi Nakamoto, but Haber and Stornetta.\n\nThe first Blockchain in its current sense is as old as Bitcoin, the first cryptocurrency in history. When the calendars showed October 31, 2008, the white paper document called â€œBitcoin: Peer-to-Peer Electronic Cash Systemâ€ published by a person or group named Satoshi Nakamoto stated that the BTC infrastructure was completely based on blockchain technology.\nHow Blockchain Works?\nBlockchain, which has a very wide working area, is used. Blockchain can be made from authorized and unofficial use cases, from usage areas, from contracts to notary transactions.\nThe most important features underlying the working principle of blockchain technology; it is anonymous, distributed, decentralized and public, yet unbreakable and unhackable. The fact that an information record chain is both accessible and unbreakable to everyone causes everyone, especially computer scientists, to approach this technology with admiration. There is a simple logic behind the fact that the records committed to the blocks are unbreakable and unchangeable: All blocks in this registry, which has billions of copies, must be changed in order to corrupt the blocks. It is almost impossible to make such an intervention.\nWhat is Ethereum?\nEthereum is an open source distributed public blockchain network. It allows decentralized apps to be built on it with the help of Smart Contract functionality.\n\nVitalik Buterin developed Ethereum as an extension to the original core blockchain concept. He improvised Bitcoinâ€™s protocols to support applications beyond currency issuance. Its major breakthrough is the ability to easily write and deploy Smart Contracts. These are actually bits of code that are executed on the network. Hence, this platform could help developers to write programs for building decentralized organisations.\nWhat is Hyperledger Fabric?\nHyperledger Fabric is a framework for developing Blockchain-based solutions for the enterprise. It is open-source and under the umbrella of the Linux Foundation.It is a private chain, thus super-helpful for enterprises since you donâ€™t want to put your transactions for public display.The framework has a very sophisticated module architecture which allows thedeveloper to design the blockchain network with security, scalability, confidentiality and with high performance.\n\nHyperledger Fabric vsÂ Ethereum\nThe subject of this article is not etherium. so I wonâ€™t explain ethereum in detail. The differences between Ethereum and Hyperledger Fabric are explained in the tableÂ below.\n\nWhat isÂ DLT?\nAny Blockchain works on top of a DLT, or Distributed Ledger Technology. DLTs are protocols to store all transactions that take place in a network. In a Blockchain protocol, the network is decentralized, which means that the transaction history is replicated across all the participating nodes.\nThe Architecture ofÂ Fabric\nFabric is a permissioned Blockchain, it has certain protocols you might not have heard of before, itâ€™s time to take a look atÂ them.\nLedger\nIn Hyperledger fabric ledger is a transaction history log that contains the state of each transaction committed to the ledger. The ledger consists of two parts World State, Blockchain.\nWorld Stateâ€Šâ€”â€ŠItâ€™s a database that contains the updated records for a transaction. The World state gets more comfortable for the programmer to check the record is stored accurately into the ledger or not.Hyperledger fabric supports LevelDB and CouchDB as the state database.I used CouchDB in Asset Transfer Application.\nBlockchainâ€Šâ€”â€Šis a set of sequential blocks containing the sequence of transactions carried out in the world state. Hyperledger Fabric uses an ordering service for transaction sequencing within theÂ blocks.\nTransaction\nThe role of a transaction is to store the state of anÂ object.\nBlocks\nBlocks In Blockchain the first block is called the genesis block. This initial block doesnâ€™t contain any user transaction but stores the channel configuration in theÂ network.\nPeers\nPeers are a fundamental unit of any distributed model. Peers are essentially nodes, but in public blockchain networks like Bitcoin or Ethereum, all peers are equal. In a private Blockchain, all peers are notÂ equal.\nIn Asset transfer application, each organization has only 1Â peer.\nThe Consensus Mechanism\nThe point of Consensus is validating the correctness of a Block, and that it adheres to the policies set by the Chaincode. There are 2 types of Consensus Mechanisms in Fabric: Voting and Lottery. Voting is the more accepted method in enterprise right now. You, as a developer, must develop your Fabric-based enterprise application on the idea that there is partial trust in a network. I hope Voting and Lottery are clear from their nomenclature as to what they do, if not, refer to the officialÂ docs.\nConsensus in Fabric is broken down in threeÂ phases:\n\nEndorsement: The participants will have to endorse a transaction.\nOrdering: This phase will agree to the order of commitment to theÂ ledger.\nValidation: Validates structure andÂ order.\n\nChaincode\nChaincode is the software that defines the asset. Assets are basically a key-value pair, where Asset definitions enable the exchange of almost anything with a monetary value over the network. Chaincode is a program that runs in a secured Docker container. Chaincode is the Smart Contract of Fabric in the sense it runs the business logic of the network. You can write ChainCodes in Go, NodeJS, or Java.The chaincode part of Asset transfer application is written inÂ Go.\nAsset transfer application has a single chaincode calledÂ basic.\nOrganization\nThis is a set of groups of members under a single MSP. An organization can be related to a big multi corporation group or small coffee shop. The naming convention for organization MSP can be derived as Org1.MSP, here organization is â€œOrg1â€. And also an organization can have many MSPs based on its requirements.\nAsset transfer application has 3 organizations.Org1,Org2,Org3.\nMembership Service ProviderÂ (MSP)\nFor clients to participate in a private network, they need credentials to be authentic. MSPs are an abstract component that provides credentials to the clients. Clients use these credentials to authenticate their transactions, and peers use these credentials to authenticate transaction processing results (endorsements).\nChannels\nHyperledger Fabric has a technique that allows the organization to join multiple blockchain networks through a channel. So, multiple organizations can communicate or participate in many networks implementing various channels.Asset transfer application has a single channel called mychannel.\nWhat is Hyperledger Explorer?\nHyperledger Explorer is a simple, powerful, easy-to-use, well maintained, open source utility to browse activity on the underlying blockchain network. Users have the ability to configure and build Hyperledger Explorer on MacOS andÂ Ubuntu.\n\nIn the next articles, I will explain the integration of Hyperledger Fabric with Hyperledger Explorer.\nMy article ends here. In general, I tried to give theoretical information about Hyperledger Fabric,Hyperledger Explorer and Blockchain.\nSee you in the next articles.\nProject Links\nSpring Boot Hlf Starter Project details and installation can be accessed via thisÂ link.\nAsset Transfer Project details and installation can be accessed via thisÂ link",
      "dc:creator": "Åuayb ÅimÅŸek",
      "guid": "https://medium.com/p/c8f7c87d60eb",
      "categories": ["hyperledger-fabric", "blockchain", "spring", "spring-boot", "hyperledger-explorer"],
      "isoDate": "2021-10-17T17:59:32.000Z"
    }
  ],
  "feedUrl": "https://medium.com/@suaybsimsek58/feed",
  "image": {
    "link": "https://medium.com/@suaybsimsek58?source=rss-bda589f2335a------2",
    "url": "https://cdn-images-1.medium.com/fit/c/150/150/1*lX-xeCmafVAFHZQp-zXGVQ.png",
    "title": "Stories by Åuayb ÅimÅŸek on Medium"
  },
  "paginationLinks": {
    "self": "https://medium.com/@suaybsimsek58/feed"
  },
  "title": "Stories by Åuayb ÅimÅŸek on Medium",
  "description": "Stories by Åuayb ÅimÅŸek on Medium",
  "webMaster": "yourfriends@medium.com",
  "generator": "Medium",
  "link": "https://medium.com/@suaybsimsek58?source=rss-bda589f2335a------2",
  "lastBuildDate": "Fri, 05 Dec 2025 00:06:09 GMT"
}
